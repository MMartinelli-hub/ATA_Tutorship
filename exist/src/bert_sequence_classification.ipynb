{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b42e9501",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ece464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install torch transformers datasets scikit-learn pandas tqdm matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d08c516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_recall_fscore_support, \n",
    "    classification_report\n",
    ")\n",
    "from datasets import Dataset as HFDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6803c2",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6002810d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the full dataset (both English and Spanish)\n",
    "df = pd.read_csv('../data/aggregated_data.csv')\n",
    "\n",
    "# Create label mappings\n",
    "unique_labels = sorted(df['label_sexist'].unique().tolist())\n",
    "label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "num_labels = len(label2id)\n",
    "\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Number of labels: {num_labels}\")\n",
    "print(f\"Label mapping: {label2id}\")\n",
    "print(f\"\\nLanguage distribution:\")\n",
    "print(df['lang'].value_counts())\n",
    "print(f\"\\nSplit distribution:\")\n",
    "print(df['split'].value_counts())\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df['label_sexist'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ceabd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train_df = df[df['split'] == 'train'].copy()\n",
    "dev_df = df[df['split'] == 'dev'].copy()\n",
    "test_df = df[df['split'] == 'test'].copy()\n",
    "\n",
    "# Add numeric labels\n",
    "train_df['label'] = train_df['label_sexist'].map(label2id)\n",
    "dev_df['label'] = dev_df['label_sexist'].map(label2id)\n",
    "test_df['label'] = test_df['label_sexist'].map(label2id)\n",
    "\n",
    "print(f\"Train samples: {len(train_df)} (EN: {len(train_df[train_df['lang']=='en'])}, ES: {len(train_df[train_df['lang']=='es'])})\")\n",
    "print(f\"Dev samples: {len(dev_df)} (EN: {len(dev_df[dev_df['lang']=='en'])}, ES: {len(dev_df[dev_df['lang']=='es'])})\")\n",
    "print(f\"Test samples: {len(test_df)} (EN: {len(test_df[test_df['lang']=='en'])}, ES: {len(test_df[test_df['lang']=='es'])})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87d7540",
   "metadata": {},
   "source": [
    "## 3. Define Models to Test\n",
    "\n",
    "We'll test multiple models with different language configurations:\n",
    "- **\"multi\"**: Train on both English and Spanish data\n",
    "- **\"en\"**: Train only on English data\n",
    "- **\"es\"**: Train only on Spanish data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796de4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models and their language configurations\n",
    "# Keys: model names, Values: \"en\" (English only), \"es\" (Spanish only), \"multi\" (both languages)\n",
    "models_config = {\n",
    "    'bert-base-multilingual-cased': 'multi',\n",
    "    'bert-base-uncased': 'en',\n",
    "    'dccuchile/bert-base-spanish-wwm-cased': 'es',\n",
    "    'roberta-base': 'en',\n",
    "    'distilbert-base-multilingual-cased': 'multi',\n",
    "}\n",
    "\n",
    "print(\"Models configuration:\")\n",
    "print(\"=\" * 70)\n",
    "for model_name, lang_config in models_config.items():\n",
    "    lang_desc = {\n",
    "        'multi': 'Multilingual (EN + ES)',\n",
    "        'en': 'English only',\n",
    "        'es': 'Spanish only'\n",
    "    }[lang_config]\n",
    "    print(f\"{model_name:45s} -> {lang_desc}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790b6397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples, tokenizer):\n",
    "    \"\"\"Tokenize texts for the model\"\"\"\n",
    "    return tokenizer(examples['text'], truncation=True, max_length=128)\n",
    "\n",
    "def prepare_datasets(train_df, dev_df, test_df, tokenizer, lang_config):\n",
    "    \"\"\"\n",
    "    Prepare datasets based on language configuration\n",
    "    \n",
    "    Args:\n",
    "        train_df, dev_df, test_df: DataFrames with data\n",
    "        tokenizer: Tokenizer to use\n",
    "        lang_config: \"en\", \"es\", or \"multi\"\n",
    "    \n",
    "    Returns:\n",
    "        Tokenized train, dev, and test datasets\n",
    "    \"\"\"\n",
    "    # Filter data based on language configuration\n",
    "    if lang_config == 'en':\n",
    "        train_filtered = train_df[train_df['lang'] == 'en'].copy()\n",
    "        dev_filtered = dev_df[dev_df['lang'] == 'en'].copy()\n",
    "        test_filtered = test_df[test_df['lang'] == 'en'].copy()\n",
    "    elif lang_config == 'es':\n",
    "        train_filtered = train_df[train_df['lang'] == 'es'].copy()\n",
    "        dev_filtered = dev_df[dev_df['lang'] == 'es'].copy()\n",
    "        test_filtered = test_df[test_df['lang'] == 'es'].copy()\n",
    "    else:  # multi\n",
    "        train_filtered = train_df.copy()\n",
    "        dev_filtered = dev_df.copy()\n",
    "        test_filtered = test_df.copy()\n",
    "    \n",
    "    # Convert to HuggingFace datasets\n",
    "    train_dataset = HFDataset.from_pandas(train_filtered[['text', 'label']])\n",
    "    dev_dataset = HFDataset.from_pandas(dev_filtered[['text', 'label']])\n",
    "    test_dataset = HFDataset.from_pandas(test_filtered[['text', 'label']])\n",
    "    \n",
    "    # Tokenize\n",
    "    train_dataset = train_dataset.map(\n",
    "        lambda x: tokenize_function(x, tokenizer), batched=True\n",
    "    )\n",
    "    dev_dataset = dev_dataset.map(\n",
    "        lambda x: tokenize_function(x, tokenizer), batched=True\n",
    "    )\n",
    "    test_dataset = test_dataset.map(\n",
    "        lambda x: tokenize_function(x, tokenizer), batched=True\n",
    "    )\n",
    "    \n",
    "    print(f\"  Train: {len(train_dataset)} samples\")\n",
    "    print(f\"  Dev: {len(dev_dataset)} samples\")\n",
    "    print(f\"  Test: {len(test_dataset)} samples\")\n",
    "    \n",
    "    return train_dataset, dev_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e6c96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute metrics for evaluation\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='weighted'\n",
    "    )\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a190e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store all results\n",
    "all_results = {}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for model_name, lang_config in models_config.items():\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Processing: {model_name}\")\n",
    "    print(f\"Language configuration: {lang_config}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load tokenizer\n",
    "    print(f\"\\nLoading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Prepare datasets based on language configuration\n",
    "    print(f\"Preparing datasets for '{lang_config}' configuration...\")\n",
    "    train_dataset, dev_dataset, test_dataset = prepare_datasets(\n",
    "        train_df, dev_df, test_df, tokenizer, lang_config\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    print(f\"\\nInitializing model...\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=num_labels,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    )\n",
    "    print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Setup training arguments\n",
    "    output_dir = f'./results_{model_name.replace(\"/\", \"_\")}_{lang_config}'\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=32,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        learning_rate=2e-5,\n",
    "        logging_dir=f'./logs_{model_name.replace(\"/\", \"_\")}_{lang_config}',\n",
    "        logging_steps=100,\n",
    "        eval_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='f1',\n",
    "        save_total_limit=2,\n",
    "    )\n",
    "    \n",
    "    # Data collator\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=dev_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    print(f\"\\nTraining...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(f\"\\nEvaluating on test set...\")\n",
    "    test_results = trainer.evaluate(test_dataset)\n",
    "    \n",
    "    # Store results\n",
    "    all_results[f\"{model_name} ({lang_config})\"] = {\n",
    "        'model_name': model_name,\n",
    "        'lang_config': lang_config,\n",
    "        'accuracy': test_results['eval_accuracy'],\n",
    "        'precision': test_results['eval_precision'],\n",
    "        'recall': test_results['eval_recall'],\n",
    "        'f1': test_results['eval_f1'],\n",
    "        'trainer': trainer,\n",
    "        'test_dataset': test_dataset\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  Accuracy:  {test_results['eval_accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {test_results['eval_precision']:.4f}\")\n",
    "    print(f\"  Recall:    {test_results['eval_recall']:.4f}\")\n",
    "    print(f\"  F1-Score:  {test_results['eval_f1']:.4f}\")\n",
    "    \n",
    "    # Save model\n",
    "    save_dir = f'./{model_name.replace(\"/\", \"_\")}_{lang_config}'\n",
    "    model.save_pretrained(save_dir)\n",
    "    tokenizer.save_pretrained(save_dir)\n",
    "    print(f\"\\nModel saved to '{save_dir}'\")\n",
    "    \n",
    "    # Clear memory\n",
    "    del model, trainer, tokenizer\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"All models trained and evaluated!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49937b46",
   "metadata": {},
   "source": [
    "## 4. Compare Results by Language Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c6c6db",
   "metadata": {},
   "source": [
    "### 4.1 Overall Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f9e7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_data = []\n",
    "for key, results in all_results.items():\n",
    "    comparison_data.append({\n",
    "        'Model': results['model_name'].split('/')[-1],\n",
    "        'Lang Config': results['lang_config'].upper(),\n",
    "        'Accuracy': results['accuracy'],\n",
    "        'Precision': results['precision'],\n",
    "        'Recall': results['recall'],\n",
    "        'F1-Score': results['f1']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('F1-Score', ascending=False)\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print(\"ALL MODELS COMPARISON - TEST SET RESULTS\")\n",
    "print(\"=\" * 90)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\" * 90)\n",
    "\n",
    "# Identify best overall model\n",
    "best_idx = comparison_df.index[0]\n",
    "print(f\"\\nBest Overall Model: {comparison_df.iloc[0]['Model']} ({comparison_df.iloc[0]['Lang Config']})\")\n",
    "print(f\"Best F1-Score: {comparison_df.iloc[0]['F1-Score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abff7530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate results by language configuration\n",
    "en_results = comparison_df[comparison_df['Lang Config'] == 'EN'].copy()\n",
    "es_results = comparison_df[comparison_df['Lang Config'] == 'ES'].copy()\n",
    "multi_results = comparison_df[comparison_df['Lang Config'] == 'MULTI'].copy()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ENGLISH-ONLY MODELS\")\n",
    "print(\"=\" * 70)\n",
    "if len(en_results) > 0:\n",
    "    print(en_results.to_string(index=False))\n",
    "    print(f\"\\nBest EN model: {en_results.iloc[0]['Model']} (F1: {en_results.iloc[0]['F1-Score']:.4f})\")\n",
    "else:\n",
    "    print(\"No English-only models\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SPANISH-ONLY MODELS\")\n",
    "print(\"=\" * 70)\n",
    "if len(es_results) > 0:\n",
    "    print(es_results.to_string(index=False))\n",
    "    print(f\"\\nBest ES model: {es_results.iloc[0]['Model']} (F1: {es_results.iloc[0]['F1-Score']:.4f})\")\n",
    "else:\n",
    "    print(\"No Spanish-only models\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MULTILINGUAL MODELS\")\n",
    "print(\"=\" * 70)\n",
    "if len(multi_results) > 0:\n",
    "    print(multi_results.to_string(index=False))\n",
    "    print(f\"\\nBest MULTI model: {multi_results.iloc[0]['Model']} (F1: {multi_results.iloc[0]['F1-Score']:.4f})\")\n",
    "else:\n",
    "    print(\"No multilingual models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4270413d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Color mapping for language configs\n",
    "color_map = {'EN': '#1f77b4', 'ES': '#ff7f0e', 'MULTI': '#2ca02c'}\n",
    "comparison_df['Color'] = comparison_df['Lang Config'].map(color_map)\n",
    "\n",
    "# 1. Overall F1-Score comparison (large plot)\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "sorted_df = comparison_df.sort_values('F1-Score', ascending=True)\n",
    "bars = ax1.barh(range(len(sorted_df)), sorted_df['F1-Score'], \n",
    "                color=sorted_df['Color'], alpha=0.8)\n",
    "ax1.set_yticks(range(len(sorted_df)))\n",
    "ax1.set_yticklabels([f\"{row['Model']} ({row['Lang Config']})\" \n",
    "                      for _, row in sorted_df.iterrows()])\n",
    "ax1.set_xlabel('F1-Score', fontsize=12)\n",
    "ax1.set_title('Overall F1-Score Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlim([0, 1])\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "for i, (_, row) in enumerate(sorted_df.iterrows()):\n",
    "    ax1.text(row['F1-Score'] + 0.01, i, f\"{row['F1-Score']:.4f}\", \n",
    "             va='center', fontsize=9)\n",
    "\n",
    "# 2-4. Metrics by language configuration\n",
    "metrics = ['Accuracy', 'Precision', 'Recall']\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = fig.add_subplot(gs[1, idx])\n",
    "    \n",
    "    # Prepare data for grouped bars\n",
    "    configs = ['EN', 'ES', 'MULTI']\n",
    "    for i, config in enumerate(configs):\n",
    "        config_data = comparison_df[comparison_df['Lang Config'] == config]\n",
    "        if len(config_data) > 0:\n",
    "            x_pos = np.arange(len(config_data)) + i * 0.25\n",
    "            ax.bar(x_pos, config_data[metric], 0.25, \n",
    "                   label=config, color=color_map[config], alpha=0.8)\n",
    "    \n",
    "    ax.set_ylabel(metric, fontsize=11)\n",
    "    ax.set_title(f'{metric} by Config', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 5-7. Individual comparisons for each language config\n",
    "for idx, (config, config_data) in enumerate([\n",
    "    ('EN', en_results), ('ES', es_results), ('MULTI', multi_results)\n",
    "]):\n",
    "    ax = fig.add_subplot(gs[2, idx])\n",
    "    \n",
    "    if len(config_data) > 0:\n",
    "        x = np.arange(len(config_data))\n",
    "        width = 0.2\n",
    "        \n",
    "        ax.bar(x - width*1.5, config_data['Precision'], width, \n",
    "               label='Precision', alpha=0.8)\n",
    "        ax.bar(x - width*0.5, config_data['Recall'], width, \n",
    "               label='Recall', alpha=0.8)\n",
    "        ax.bar(x + width*0.5, config_data['F1-Score'], width, \n",
    "               label='F1-Score', alpha=0.8)\n",
    "        ax.bar(x + width*1.5, config_data['Accuracy'], width, \n",
    "               label='Accuracy', alpha=0.8)\n",
    "        \n",
    "        ax.set_ylabel('Score', fontsize=11)\n",
    "        ax.set_title(f'{config} Models Comparison', fontsize=12, fontweight='bold')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(config_data['Model'], rotation=45, ha='right', fontsize=9)\n",
    "        ax.legend(fontsize=9)\n",
    "        ax.set_ylim([0, 1])\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, f'No {config} models', \n",
    "                ha='center', va='center', fontsize=12)\n",
    "        ax.set_xlim([0, 1])\n",
    "        ax.set_ylim([0, 1])\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.savefig('bert_models_comprehensive_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nComprehensive comparison plot saved as 'bert_models_comprehensive_comparison.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f6574d",
   "metadata": {},
   "source": [
    "### 4.2 Analysis: Multilingual vs Monolingual Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca374be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare multilingual approach vs best monolingual combination\n",
    "if len(multi_results) > 0:\n",
    "    best_multi = multi_results.iloc[0]\n",
    "    \n",
    "    # Get best EN and ES models if they exist\n",
    "    if len(en_results) > 0 and len(es_results) > 0:\n",
    "        best_en = en_results.iloc[0]\n",
    "        best_es = es_results.iloc[0]\n",
    "        \n",
    "        # Estimate combined monolingual performance (weighted average by test set size)\n",
    "        # This is an approximation - actual merged performance would require combining predictions\n",
    "        total_test = len(test_df)\n",
    "        en_test_size = len(test_df[test_df['lang'] == 'en'])\n",
    "        es_test_size = len(test_df[test_df['lang'] == 'es'])\n",
    "        \n",
    "        combined_f1 = (best_en['F1-Score'] * en_test_size + \n",
    "                       best_es['F1-Score'] * es_test_size) / total_test\n",
    "        combined_precision = (best_en['Precision'] * en_test_size + \n",
    "                             best_es['Precision'] * es_test_size) / total_test\n",
    "        combined_recall = (best_en['Recall'] * en_test_size + \n",
    "                          best_es['Recall'] * es_test_size) / total_test\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        print(\"STRATEGY COMPARISON: Multilingual vs Monolingual Combination\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"\\nBest Multilingual Model: {best_multi['Model']}\")\n",
    "        print(f\"  F1-Score:  {best_multi['F1-Score']:.4f}\")\n",
    "        print(f\"  Precision: {best_multi['Precision']:.4f}\")\n",
    "        print(f\"  Recall:    {best_multi['Recall']:.4f}\")\n",
    "        \n",
    "        print(f\"\\nBest Monolingual Combination: {best_en['Model']} (EN) + {best_es['Model']} (ES)\")\n",
    "        print(f\"  Combined F1-Score (est.):  {combined_f1:.4f}\")\n",
    "        print(f\"  Combined Precision (est.): {combined_precision:.4f}\")\n",
    "        print(f\"  Combined Recall (est.):    {combined_recall:.4f}\")\n",
    "        \n",
    "        print(f\"\\nDifference:\")\n",
    "        print(f\"  F1-Score: {best_multi['F1-Score'] - combined_f1:+.4f}\")\n",
    "        \n",
    "        if best_multi['F1-Score'] > combined_f1:\n",
    "            print(f\"\\n✓ Multilingual approach performs better by {best_multi['F1-Score'] - combined_f1:.4f}\")\n",
    "        else:\n",
    "            print(f\"\\n✓ Monolingual combination performs better by {combined_f1 - best_multi['F1-Score']:.4f}\")\n",
    "        print(\"=\" * 80)\n",
    "    else:\n",
    "        print(\"Cannot compare - need both EN and ES models for monolingual strategy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783fff0f",
   "metadata": {},
   "source": [
    "### 4.3 Detailed Results for Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb569234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model's detailed results\n",
    "best_model_key = list(all_results.keys())[\n",
    "    list(all_results.values()).index(\n",
    "        max(all_results.values(), key=lambda x: x['f1'])\n",
    "    )\n",
    "]\n",
    "\n",
    "best_model_results = all_results[best_model_key]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"BEST MODEL DETAILS: {best_model_key}\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Accuracy:  {best_model_results['accuracy']:.4f}\")\n",
    "print(f\"Precision: {best_model_results['precision']:.4f}\")\n",
    "print(f\"Recall:    {best_model_results['recall']:.4f}\")\n",
    "print(f\"F1-Score:  {best_model_results['f1']:.4f}\")\n",
    "\n",
    "# Get predictions for detailed report\n",
    "print(\"\\nGenerating detailed classification report...\")\n",
    "predictions = best_model_results['trainer'].predict(best_model_results['test_dataset'])\n",
    "pred_labels = np.argmax(predictions.predictions, axis=1)\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(\"-\" * 80)\n",
    "print(classification_report(\n",
    "    true_labels, \n",
    "    pred_labels, \n",
    "    target_names=[id2label[i] for i in sorted(id2label.keys())]\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b6a537",
   "metadata": {},
   "source": [
    "## 5. Save Best Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d429202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions from best model\n",
    "output_file = f'../data/bert_predictions_{best_model_results[\"model_name\"].replace(\"/\", \"_\")}_{best_model_results[\"lang_config\"]}.json'\n",
    "\n",
    "# Create submission dictionary\n",
    "submission = {}\n",
    "test_ids = test_df['id'].tolist() if best_model_results['lang_config'] == 'multi' else \\\n",
    "           test_df[test_df['lang'] == 'en']['id'].tolist() if best_model_results['lang_config'] == 'en' else \\\n",
    "           test_df[test_df['lang'] == 'es']['id'].tolist()\n",
    "\n",
    "for test_id, pred_label in zip(test_ids, pred_labels):\n",
    "    submission[str(test_id)] = id2label[int(pred_label)]\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(submission, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Best model predictions saved to: {output_file}\")\n",
    "print(f\"Total predictions: {len(submission)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd1f150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results to CSV for further analysis\n",
    "comparison_df.to_csv('bert_models_comparison_results.csv', index=False)\n",
    "print(\"\\nResults exported to 'bert_models_comparison_results.csv'\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea81450c",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ff842a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install groq python-dotenv pandas scikit-learn tqdm matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2c70e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"✓ Libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1d8ce2",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83207ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../data/aggregated_data.csv')\n",
    "\n",
    "# Create label mappings\n",
    "unique_labels = sorted(df['label_sexist'].unique().tolist())\n",
    "label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Labels: {unique_labels}\")\n",
    "print(f\"\\nLanguage distribution:\")\n",
    "print(df['lang'].value_counts())\n",
    "print(f\"\\nSplit distribution:\")\n",
    "print(df['split'].value_counts())\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df['label_sexist'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70e7e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train_df = df[df['split'] == 'train'].copy()\n",
    "dev_df = df[df['split'] == 'dev'].copy()\n",
    "test_df = df[df['split'] == 'test'].copy()\n",
    "\n",
    "print(f\"Train: {len(train_df)} samples\")\n",
    "print(f\"Dev: {len(dev_df)} samples\")\n",
    "print(f\"Test: {len(test_df)} samples\")\n",
    "\n",
    "# Show example\n",
    "print(f\"\\nExample:\")\n",
    "print(f\"  Text: {test_df.iloc[0]['text']}\")\n",
    "print(f\"  Label: {test_df.iloc[0]['label_sexist']}\")\n",
    "print(f\"  Language: {test_df.iloc[0]['lang']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03124a8",
   "metadata": {},
   "source": [
    "## Initialize Groq API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8a365c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Groq client\n",
    "client = Groq()\n",
    "\n",
    "# Test the connection\n",
    "test_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello! Just testing the connection.\"}\n",
    "    ],\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    temperature=0.5,\n",
    "    max_completion_tokens=50,\n",
    ")\n",
    "\n",
    "print(\"✓ Groq API connection successful\")\n",
    "print(f\"Test response: {test_completion.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d53cac1",
   "metadata": {},
   "source": [
    "## Define Models and Prompts Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8663086e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to test\n",
    "models_to_test = [\n",
    "    \"llama-3.3-70b-versatile\",\n",
    "    \"llama-3.1-70b-versatile\",\n",
    "    \"mixtral-8x7b-32768\",\n",
    "]\n",
    "\n",
    "print(\"Models to test:\")\n",
    "for i, model in enumerate(models_to_test, 1):\n",
    "    print(f\"  {i}. {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9bd9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define different prompt strategies\n",
    "prompt_templates = {\n",
    "    \"simple\": {\n",
    "        \"system\": \"You are a sexism detection system. Classify tweets as 'sexist' or 'not sexist'.\",\n",
    "        \"user\": \"Classify this tweet as 'sexist' or 'not sexist'. Return ONLY the label, nothing else.\\n\\nTweet: {text}\\n\\nLabel:\"\n",
    "    },\n",
    "    \n",
    "    \"detailed\": {\n",
    "        \"system\": \"You are an expert in detecting sexism in social media content. Your task is to classify tweets as 'sexist' or 'not sexist' based on their content.\",\n",
    "        \"user\": \"\"\"Classify the following tweet as either 'sexist' or 'not sexist'.\n",
    "\n",
    "A tweet is 'sexist' if it:\n",
    "- Contains gender-based discrimination or stereotyping\n",
    "- Objectifies or demeans individuals based on gender\n",
    "- Promotes gender-based violence or inequality\n",
    "- Uses derogatory language targeting a specific gender\n",
    "\n",
    "Tweet: {text}\n",
    "\n",
    "Respond with ONLY 'sexist' or 'not sexist', nothing else.\n",
    "\n",
    "Classification:\"\"\"\n",
    "    },\n",
    "    \n",
    "    \"structured\": {\n",
    "        \"system\": \"You are a classifier for detecting sexism in social media posts. You must respond with ONLY 'sexist' or 'not sexist'.\",\n",
    "        \"user\": \"\"\"Task: Binary classification of sexism\n",
    "Input: \\\"{text}\\\"\n",
    "Output (ONLY 'sexist' or 'not sexist'):\"\"\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Prompt strategies defined:\")\n",
    "for name in prompt_templates.keys():\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7dd605",
   "metadata": {},
   "source": [
    "## Classification Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9842496b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_with_llm(texts, model_name, prompt_template, temperature=0.3, max_retries=3):\n",
    "    \"\"\"\n",
    "    Classify texts using LLM via Groq API\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text strings to classify\n",
    "        model_name: Name of the Groq model to use\n",
    "        prompt_template: Dictionary with 'system' and 'user' prompt templates\n",
    "        temperature: Sampling temperature (lower = more deterministic)\n",
    "        max_retries: Maximum number of retries for failed requests\n",
    "    \n",
    "    Returns:\n",
    "        List of predicted labels\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for text in tqdm(texts, desc=f\"Classifying with {model_name}\"):\n",
    "        retry_count = 0\n",
    "        predicted_label = None\n",
    "        \n",
    "        while retry_count < max_retries and predicted_label is None:\n",
    "            try:\n",
    "                # Create the prompt\n",
    "                user_prompt = prompt_template[\"user\"].format(text=text)\n",
    "                \n",
    "                # Call the API\n",
    "                chat_completion = client.chat.completions.create(\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": prompt_template[\"system\"]},\n",
    "                        {\"role\": \"user\", \"content\": user_prompt}\n",
    "                    ],\n",
    "                    model=model_name,\n",
    "                    temperature=temperature,\n",
    "                    max_completion_tokens=10,  # We only need a short response\n",
    "                )\n",
    "                \n",
    "                # Parse the response\n",
    "                response_text = chat_completion.choices[0].message.content.strip().lower()\n",
    "                \n",
    "                # Extract label from response\n",
    "                if 'sexist' in response_text and 'not sexist' not in response_text:\n",
    "                    predicted_label = 'sexist'\n",
    "                elif 'not sexist' in response_text or 'non-sexist' in response_text or 'nonsexist' in response_text:\n",
    "                    predicted_label = 'not sexist'\n",
    "                else:\n",
    "                    # Fallback: try to extract any valid label\n",
    "                    if 'sexist' in response_text:\n",
    "                        predicted_label = 'sexist'\n",
    "                    else:\n",
    "                        predicted_label = 'not sexist'  # Default to not sexist if unclear\n",
    "                \n",
    "            except Exception as e:\n",
    "                retry_count += 1\n",
    "                if retry_count >= max_retries:\n",
    "                    print(f\"\\nError after {max_retries} retries: {e}\")\n",
    "                    predicted_label = 'not sexist'  # Default fallback\n",
    "        \n",
    "        predictions.append(predicted_label)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "print(\"✓ Classification function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfa3b15",
   "metadata": {},
   "source": [
    "## Test on Small Sample\n",
    "\n",
    "First, let's test on a small sample to verify the approach works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6b9089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on first 10 dev samples\n",
    "test_sample = dev_df.head(10)\n",
    "test_texts = test_sample['text'].tolist()\n",
    "test_labels = test_sample['label_sexist'].tolist()\n",
    "\n",
    "print(\"Testing on 10 samples...\")\n",
    "sample_predictions = classify_with_llm(\n",
    "    test_texts,\n",
    "    model_name=\"llama-3.3-70b-versatile\",\n",
    "    prompt_template=prompt_templates[\"simple\"],\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "print(\"\\nSample Results:\")\n",
    "print(\"=\" * 80)\n",
    "for i, (text, true_label, pred_label) in enumerate(zip(test_texts, test_labels, sample_predictions)):\n",
    "    match = \"✓\" if true_label == pred_label else \"✗\"\n",
    "    print(f\"{match} Text: {text[:60]}...\")\n",
    "    print(f\"  True: {true_label:12s} | Predicted: {pred_label}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Calculate accuracy on sample\n",
    "sample_acc = accuracy_score(test_labels, sample_predictions)\n",
    "print(f\"\\nSample Accuracy: {sample_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6684c3",
   "metadata": {},
   "source": [
    "## Run Full Evaluation on Dev Set\n",
    "\n",
    "Now evaluate all model and prompt combinations on the development set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fee8bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For faster testing, you can limit the dev set size\n",
    "# Comment out this line to use the full dev set\n",
    "# dev_df_eval = dev_df.head(100)\n",
    "dev_df_eval = dev_df.copy()\n",
    "\n",
    "dev_texts = dev_df_eval['text'].tolist()\n",
    "dev_labels = dev_df_eval['label_sexist'].tolist()\n",
    "\n",
    "print(f\"Evaluating on {len(dev_texts)} dev samples\")\n",
    "print(f\"Testing {len(models_to_test)} models × {len(prompt_templates)} prompts = {len(models_to_test) * len(prompt_templates)} configurations\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e704f8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all results\n",
    "all_results = {}\n",
    "\n",
    "# Evaluate each combination\n",
    "for model_name in models_to_test:\n",
    "    for prompt_name, prompt_template in prompt_templates.items():\n",
    "        config_name = f\"{model_name}_{prompt_name}\"\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"Configuration: {config_name}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = classify_with_llm(\n",
    "            dev_texts,\n",
    "            model_name=model_name,\n",
    "            prompt_template=prompt_template,\n",
    "            temperature=0.3\n",
    "        )\n",
    "        \n",
    "        # Calculate metrics\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            dev_labels, predictions, average='weighted'\n",
    "        )\n",
    "        acc = accuracy_score(dev_labels, predictions)\n",
    "        \n",
    "        # Store results\n",
    "        all_results[config_name] = {\n",
    "            'model': model_name,\n",
    "            'prompt': prompt_name,\n",
    "            'predictions': predictions,\n",
    "            'accuracy': acc,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nResults:\")\n",
    "        print(f\"  Accuracy:  {acc:.4f}\")\n",
    "        print(f\"  Precision: {precision:.4f}\")\n",
    "        print(f\"  Recall:    {recall:.4f}\")\n",
    "        print(f\"  F1-Score:  {f1:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"All configurations evaluated!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51de959",
   "metadata": {},
   "source": [
    "## Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221505a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_data = []\n",
    "for config_name, results in all_results.items():\n",
    "    comparison_data.append({\n",
    "        'Configuration': config_name,\n",
    "        'Model': results['model'],\n",
    "        'Prompt': results['prompt'],\n",
    "        'Accuracy': results['accuracy'],\n",
    "        'Precision': results['precision'],\n",
    "        'Recall': results['recall'],\n",
    "        'F1-Score': results['f1']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('F1-Score', ascending=False)\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"COMPARISON OF ALL CONFIGURATIONS\")\n",
    "print(\"=\" * 100)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Best configuration\n",
    "best_config = comparison_df.iloc[0]\n",
    "print(f\"\\nBest Configuration: {best_config['Configuration']}\")\n",
    "print(f\"  Model: {best_config['Model']}\")\n",
    "print(f\"  Prompt: {best_config['Prompt']}\")\n",
    "print(f\"  F1-Score: {best_config['F1-Score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90236606",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d886833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Sort by F1-score for better visualization\n",
    "sorted_df = comparison_df.sort_values('F1-Score', ascending=True)\n",
    "\n",
    "# Color map by prompt type\n",
    "prompt_colors = {'simple': '#1f77b4', 'detailed': '#ff7f0e', 'structured': '#2ca02c'}\n",
    "colors = [prompt_colors[p] for p in sorted_df['Prompt']]\n",
    "\n",
    "# 1. F1-Score comparison\n",
    "axes[0, 0].barh(range(len(sorted_df)), sorted_df['F1-Score'], color=colors, alpha=0.8)\n",
    "axes[0, 0].set_yticks(range(len(sorted_df)))\n",
    "axes[0, 0].set_yticklabels([f\"{row['Model'].split('/')[-1][:15]}\\n({row['Prompt']})\" \n",
    "                             for _, row in sorted_df.iterrows()], fontsize=8)\n",
    "axes[0, 0].set_xlabel('F1-Score', fontsize=11)\n",
    "axes[0, 0].set_title('F1-Score by Configuration', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].set_xlim([0, 1])\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='x')\n",
    "for i, (_, row) in enumerate(sorted_df.iterrows()):\n",
    "    axes[0, 0].text(row['F1-Score'] + 0.01, i, f\"{row['F1-Score']:.3f}\", \n",
    "                    va='center', fontsize=9)\n",
    "\n",
    "# 2. Accuracy comparison\n",
    "axes[0, 1].barh(range(len(sorted_df)), sorted_df['Accuracy'], color=colors, alpha=0.8)\n",
    "axes[0, 1].set_yticks(range(len(sorted_df)))\n",
    "axes[0, 1].set_yticklabels([f\"{row['Model'].split('/')[-1][:15]}\\n({row['Prompt']})\" \n",
    "                             for _, row in sorted_df.iterrows()], fontsize=8)\n",
    "axes[0, 1].set_xlabel('Accuracy', fontsize=11)\n",
    "axes[0, 1].set_title('Accuracy by Configuration', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].set_xlim([0, 1])\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='x')\n",
    "for i, (_, row) in enumerate(sorted_df.iterrows()):\n",
    "    axes[0, 1].text(row['Accuracy'] + 0.01, i, f\"{row['Accuracy']:.3f}\", \n",
    "                    va='center', fontsize=9)\n",
    "\n",
    "# 3. Metrics by Model\n",
    "models_unique = comparison_df['Model'].unique()\n",
    "x = np.arange(len(models_unique))\n",
    "width = 0.2\n",
    "\n",
    "for i, prompt in enumerate(['simple', 'detailed', 'structured']):\n",
    "    prompt_data = comparison_df[comparison_df['Prompt'] == prompt]\n",
    "    f1_scores = [prompt_data[prompt_data['Model'] == m]['F1-Score'].values[0] \n",
    "                 if len(prompt_data[prompt_data['Model'] == m]) > 0 else 0 \n",
    "                 for m in models_unique]\n",
    "    axes[1, 0].bar(x + i*width, f1_scores, width, label=prompt, alpha=0.8)\n",
    "\n",
    "axes[1, 0].set_ylabel('F1-Score', fontsize=11)\n",
    "axes[1, 0].set_title('F1-Score by Model and Prompt', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].set_xticks(x + width)\n",
    "axes[1, 0].set_xticklabels([m.split('/')[-1][:20] for m in models_unique], \n",
    "                           rotation=45, ha='right', fontsize=9)\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].set_ylim([0, 1])\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 4. All metrics for best configuration\n",
    "best_config_name = comparison_df.iloc[0]['Configuration']\n",
    "best_results = all_results[best_config_name]\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "values = [best_results['accuracy'], best_results['precision'], \n",
    "          best_results['recall'], best_results['f1']]\n",
    "\n",
    "axes[1, 1].bar(metrics, values, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'], alpha=0.8)\n",
    "axes[1, 1].set_ylabel('Score', fontsize=11)\n",
    "axes[1, 1].set_title(f'Best Configuration Metrics\\n({best_config_name})', \n",
    "                     fontsize=13, fontweight='bold')\n",
    "axes[1, 1].set_ylim([0, 1])\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "for i, v in enumerate(values):\n",
    "    axes[1, 1].text(i, v + 0.02, f'{v:.4f}', ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('llm_zero_shot_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization saved as 'llm_zero_shot_comparison.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bc1137",
   "metadata": {},
   "source": [
    "## Detailed Analysis of Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d418c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model predictions\n",
    "best_config_name = comparison_df.iloc[0]['Configuration']\n",
    "best_predictions = all_results[best_config_name]['predictions']\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"DETAILED ANALYSIS: {best_config_name}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(\"-\" * 80)\n",
    "print(classification_report(dev_labels, best_predictions, \n",
    "                          target_names=['not sexist', 'sexist']))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(dev_labels, best_predictions, labels=['not sexist', 'sexist'])\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['not sexist', 'sexist'],\n",
    "            yticklabels=['not sexist', 'sexist'])\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title(f'Confusion Matrix - {best_config_name}')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix_best_llm.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd34362",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af53e4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comparison results\n",
    "comparison_df.to_csv('llm_zero_shot_comparison_results.csv', index=False)\n",
    "print(\"✓ Comparison results saved to 'llm_zero_shot_comparison_results.csv'\")\n",
    "\n",
    "# Save best model predictions\n",
    "best_config_name = comparison_df.iloc[0]['Configuration']\n",
    "best_predictions = all_results[best_config_name]['predictions']\n",
    "\n",
    "output_data = []\n",
    "for idx, (_, row) in enumerate(dev_df_eval.iterrows()):\n",
    "    output_data.append({\n",
    "        'id': row['id'],\n",
    "        'text': row['text'],\n",
    "        'true_label': dev_labels[idx],\n",
    "        'predicted_label': best_predictions[idx],\n",
    "        'correct': dev_labels[idx] == best_predictions[idx]\n",
    "    })\n",
    "\n",
    "output_df = pd.DataFrame(output_data)\n",
    "output_df.to_csv(f'predictions_llm_{best_config_name}.csv', index=False)\n",
    "print(f\"✓ Best model predictions saved to 'predictions_llm_{best_config_name}.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb44f75",
   "metadata": {},
   "source": [
    "## Example Predictions Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ca4990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some example predictions\n",
    "print(\"Example Predictions from Best Model:\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Show correct predictions\n",
    "print(\"\\n✓ CORRECT PREDICTIONS (Sexist):\")\n",
    "print(\"-\" * 100)\n",
    "count = 0\n",
    "for i, (text, true, pred) in enumerate(zip(dev_texts, dev_labels, best_predictions)):\n",
    "    if true == pred and true == 'sexist' and count < 3:\n",
    "        print(f\"Text: {text[:80]}...\")\n",
    "        print(f\"Label: {true}\\n\")\n",
    "        count += 1\n",
    "\n",
    "print(\"\\n✓ CORRECT PREDICTIONS (Not Sexist):\")\n",
    "print(\"-\" * 100)\n",
    "count = 0\n",
    "for i, (text, true, pred) in enumerate(zip(dev_texts, dev_labels, best_predictions)):\n",
    "    if true == pred and true == 'not sexist' and count < 3:\n",
    "        print(f\"Text: {text[:80]}...\")\n",
    "        print(f\"Label: {true}\\n\")\n",
    "        count += 1\n",
    "\n",
    "# Show incorrect predictions\n",
    "print(\"\\n✗ INCORRECT PREDICTIONS:\")\n",
    "print(\"-\" * 100)\n",
    "count = 0\n",
    "for i, (text, true, pred) in enumerate(zip(dev_texts, dev_labels, best_predictions)):\n",
    "    if true != pred and count < 5:\n",
    "        print(f\"Text: {text[:80]}...\")\n",
    "        print(f\"True: {true:12s} | Predicted: {pred}\\n\")\n",
    "        count += 1"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

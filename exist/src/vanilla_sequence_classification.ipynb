{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab22be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install torch transformers scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7243fb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "from tqdm import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1c3505",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe500d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from CSV file\n",
    "df = pd.read_csv('../data/aggregated_data_en.csv')\n",
    "\n",
    "# Split data based on 'split' column\n",
    "train_data = df[df['split'] == 'train']\n",
    "dev_data = df[df['split'] == 'dev']\n",
    "test_data = df[df['split'] == 'test']\n",
    "\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Dev samples: {len(dev_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")\n",
    "print(f\"\\nLabel distribution in training:\")\n",
    "print(train_data['label_sexist'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde90028",
   "metadata": {},
   "source": [
    "## 2. Prepare Reference Data (Train + Dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04654546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine training and dev data for reference (better coverage)\n",
    "reference_data = pd.concat([train_data, dev_data], ignore_index=True)\n",
    "\n",
    "reference_texts = reference_data['text'].tolist()\n",
    "reference_labels = reference_data['label_sexist'].tolist()\n",
    "reference_ids = reference_data['id'].tolist()\n",
    "\n",
    "print(f\"Total reference samples (train + dev): {len(reference_texts)}\")\n",
    "print(f\"Label distribution: {Counter(reference_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec18943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data\n",
    "test_texts = test_data['text'].tolist()\n",
    "test_ids = test_data['id'].tolist()\n",
    "test_labels = test_data['label_sexist'].tolist()\n",
    "\n",
    "print(f\"Test samples: {len(test_texts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260e04ce",
   "metadata": {},
   "source": [
    "## 3. Define Models to Test\n",
    "\n",
    "We'll test multiple pre-trained models and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc18dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define list of models to test\n",
    "models_to_test = [\n",
    "    'bert-base-multilingual-cased',           # Multilingual BERT\n",
    "    'bert-base-uncased',                       # English BERT\n",
    "    'roberta-base',                            # RoBERTa (English)\n",
    "    'distilbert-base-uncased',                 # DistilBERT (smaller, faster)\n",
    "    'sentence-transformers/all-MiniLM-L6-v2', # Sentence-BERT (optimized for similarity)\n",
    "]\n",
    "\n",
    "print(\"Models to test:\")\n",
    "for i, model_name in enumerate(models_to_test, 1):\n",
    "    print(f\"  {i}. {model_name}\")\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nUsing device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489feb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cls_embeddings(texts, model, tokenizer, batch_size=32):\n",
    "    \"\"\"Extract CLS embeddings for a list of texts\"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Extracting embeddings\", leave=False):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            \n",
    "            # Tokenize\n",
    "            encoded = tokenizer(\n",
    "                batch_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=128,\n",
    "                return_tensors='pt'\n",
    "            ).to(device)\n",
    "            \n",
    "            # Get model output\n",
    "            outputs = model(**encoded)\n",
    "            \n",
    "            # Extract CLS token embedding (first token)\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            embeddings.append(cls_embeddings)\n",
    "    \n",
    "    return np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9c4964",
   "metadata": {},
   "source": [
    "## 4. Extract Embeddings and Classify with Each Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701d7568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store results for each model\n",
    "results = {}\n",
    "\n",
    "# Process each model\n",
    "for model_name in models_to_test:\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Processing model: {model_name}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load tokenizer and model\n",
    "    print(f\"Loading {model_name}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Extract embeddings for reference data\n",
    "    print(\"Extracting reference embeddings...\")\n",
    "    reference_embeddings = get_cls_embeddings(reference_texts, model, tokenizer)\n",
    "    \n",
    "    # Extract embeddings for test data\n",
    "    print(\"Extracting test embeddings...\")\n",
    "    test_embeddings = get_cls_embeddings(test_texts, model, tokenizer)\n",
    "    \n",
    "    # Perform classification\n",
    "    print(\"Classifying...\")\n",
    "    predictions = []\n",
    "    for i in tqdm(range(len(test_embeddings)), desc=\"Finding nearest neighbors\", leave=False):\n",
    "        test_emb = test_embeddings[i:i+1]\n",
    "        similarities = cosine_similarity(test_emb, reference_embeddings)[0]\n",
    "        nearest_idx = np.argmax(similarities)\n",
    "        predictions.append(reference_labels[nearest_idx])\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        test_labels, predictions, average='weighted'\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    results[model_name] = {\n",
    "        'predictions': predictions,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'reference_embeddings': reference_embeddings,\n",
    "        'test_embeddings': test_embeddings\n",
    "    }\n",
    "    \n",
    "    print(f\"✓ Completed - F1: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
    "    print()\n",
    "    \n",
    "    # Clear GPU memory\n",
    "    del model, tokenizer, reference_embeddings, test_embeddings\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"All models processed!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e1d433",
   "metadata": {},
   "source": [
    "## 5. Compare Results Across All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed04d033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_data = []\n",
    "for model_name, model_results in results.items():\n",
    "    comparison_data.append({\n",
    "        'Model': model_name.split('/')[-1],  # Use short name\n",
    "        'Precision': model_results['precision'],\n",
    "        'Recall': model_results['recall'],\n",
    "        'F1-Score': model_results['f1']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('F1-Score', ascending=False)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MODEL COMPARISON - TEST SET RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Identify best model\n",
    "best_model = comparison_df.iloc[0]['Model']\n",
    "best_f1 = comparison_df.iloc[0]['F1-Score']\n",
    "print(f\"\\nBest Model: {best_model}\")\n",
    "print(f\"Best F1-Score: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824e1e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create visualization comparing all models\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Sort by F1-score for better visualization\n",
    "sorted_df = comparison_df.sort_values('F1-Score', ascending=True)\n",
    "\n",
    "# Colors for bars\n",
    "colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(sorted_df)))\n",
    "\n",
    "# F1-Score comparison\n",
    "axes[0, 0].barh(sorted_df['Model'], sorted_df['F1-Score'], color=colors)\n",
    "axes[0, 0].set_xlabel('F1-Score', fontsize=12)\n",
    "axes[0, 0].set_title('F1-Score Comparison', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].set_xlim([0, 1])\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='x')\n",
    "for i, (model, score) in enumerate(zip(sorted_df['Model'], sorted_df['F1-Score'])):\n",
    "    axes[0, 0].text(score + 0.01, i, f'{score:.4f}', va='center', fontsize=10)\n",
    "\n",
    "# Precision comparison\n",
    "axes[0, 1].barh(sorted_df['Model'], sorted_df['Precision'], color=colors)\n",
    "axes[0, 1].set_xlabel('Precision', fontsize=12)\n",
    "axes[0, 1].set_title('Precision Comparison', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].set_xlim([0, 1])\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='x')\n",
    "for i, (model, score) in enumerate(zip(sorted_df['Model'], sorted_df['Precision'])):\n",
    "    axes[0, 1].text(score + 0.01, i, f'{score:.4f}', va='center', fontsize=10)\n",
    "\n",
    "# Recall comparison\n",
    "axes[1, 0].barh(sorted_df['Model'], sorted_df['Recall'], color=colors)\n",
    "axes[1, 0].set_xlabel('Recall', fontsize=12)\n",
    "axes[1, 0].set_title('Recall Comparison', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].set_xlim([0, 1])\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='x')\n",
    "for i, (model, score) in enumerate(zip(sorted_df['Model'], sorted_df['Recall'])):\n",
    "    axes[1, 0].text(score + 0.01, i, f'{score:.4f}', va='center', fontsize=10)\n",
    "\n",
    "# Combined metrics comparison\n",
    "x = np.arange(len(sorted_df))\n",
    "width = 0.25\n",
    "\n",
    "axes[1, 1].bar(x - width, sorted_df['Precision'], width, label='Precision', alpha=0.8)\n",
    "axes[1, 1].bar(x, sorted_df['Recall'], width, label='Recall', alpha=0.8)\n",
    "axes[1, 1].bar(x + width, sorted_df['F1-Score'], width, label='F1-Score', alpha=0.8)\n",
    "axes[1, 1].set_ylabel('Score', fontsize=12)\n",
    "axes[1, 1].set_title('All Metrics Comparison', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels(sorted_df['Model'], rotation=45, ha='right')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].set_ylim([0, 1])\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Comparison plot saved as 'model_comparison.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2c4ceb",
   "metadata": {},
   "source": [
    "## 6. Detailed Results for Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a120fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the full name of the best model\n",
    "best_model_full = None\n",
    "for model_name in results.keys():\n",
    "    if model_name.split('/')[-1] == best_model:\n",
    "        best_model_full = model_name\n",
    "        break\n",
    "\n",
    "print(f\"Best performing model: {best_model_full}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get predictions from best model\n",
    "best_predictions = results[best_model_full]['predictions']\n",
    "\n",
    "# Get unique labels from the data\n",
    "unique_labels = sorted(list(set(reference_labels + test_labels)))\n",
    "\n",
    "# Show detailed classification report for best model\n",
    "print(\"\\nDetailed Classification Report (Best Model):\")\n",
    "print(\"-\" * 70)\n",
    "print(classification_report(test_labels, best_predictions, target_names=unique_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33d21c1",
   "metadata": {},
   "source": [
    "## 7. Save Predictions from Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2361378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission dictionary using best model predictions\n",
    "submission = {}\n",
    "for test_id, prediction in zip(test_ids, best_predictions):\n",
    "    submission[test_id] = prediction\n",
    "\n",
    "# Save to JSON file\n",
    "output_file = f'../data/vanilla_nn_predictions_{best_model}.json'\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(submission, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Best model predictions saved to: {output_file}\")\n",
    "\n",
    "# Display sample predictions\n",
    "print(\"\\nSample predictions from best model:\")\n",
    "print(\"=\" * 100)\n",
    "for i in range(min(5, len(test_texts))):\n",
    "    print(f\"\\nTest ID: {test_ids[i]}\")\n",
    "    print(f\"Text: {test_texts[i][:100]}...\")\n",
    "    print(f\"True Label: {test_labels[i]}\")\n",
    "    print(f\"Predicted Label: {best_predictions[i]}\")\n",
    "    print(f\"Correct: {'✓' if test_labels[i] == best_predictions[i] else '✗'}\")\n",
    "    print(\"-\" * 100)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

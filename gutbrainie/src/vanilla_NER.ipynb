{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4e227f3",
   "metadata": {},
   "source": [
    "## Load Training Data and Extract Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9acffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load training data from three quality levels\n",
    "train_files = [\n",
    "    \"../data/Annotations/Train/gold_quality/json_format/train_gold.json\",\n",
    "    \"../data/Annotations/Train/platinum_quality/json_format/train_platinum.json\",\n",
    "    \"../data/Annotations/Train/silver_quality/json_format/train_silver.json\"\n",
    "]\n",
    "\n",
    "# Dictionary to store entities: {text_span: label}\n",
    "# If same text has multiple labels, we'll keep all possibilities\n",
    "entity_dict = defaultdict(set)\n",
    "\n",
    "for train_file in train_files:\n",
    "    if os.path.exists(train_file):\n",
    "        with open(train_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            train_data = json.load(f)\n",
    "        \n",
    "        for pmid, article in train_data.items():\n",
    "            for entity in article['entities']:\n",
    "                text_span = entity['text_span']\n",
    "                label = entity['label']\n",
    "                entity_dict[text_span].add(label)\n",
    "        \n",
    "        print(f\"Loaded {train_file}\")\n",
    "    else:\n",
    "        print(f\"Warning: {train_file} not found\")\n",
    "\n",
    "# Convert to list of (text, labels) tuples for easier processing\n",
    "entity_list = [(text, list(labels)) for text, labels in entity_dict.items()]\n",
    "\n",
    "# Sort by length (longest first) to prioritize longer matches\n",
    "entity_list.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "\n",
    "print(f\"\\nTotal unique entity text spans: {len(entity_list)}\")\n",
    "print(f\"\\nExample entities:\")\n",
    "for text, labels in entity_list[:10]:\n",
    "    print(f\"  '{text}' -> {labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609189e3",
   "metadata": {},
   "source": [
    "## Load Dev Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f2959d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dev data\n",
    "dev_data_path = \"../data/Annotations/Dev/json_format/dev.json\"\n",
    "\n",
    "with open(dev_data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    dev_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(dev_data)} documents from dev set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7095549",
   "metadata": {},
   "source": [
    "## Predict Entities on Dev Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943c72f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def find_all_occurrences(text, entity_text):\n",
    "    \"\"\"\n",
    "    Find all occurrences of entity_text in text.\n",
    "    Returns list of (start_idx, end_idx) tuples.\n",
    "    \"\"\"\n",
    "    occurrences = []\n",
    "    start = 0\n",
    "    while True:\n",
    "        pos = text.find(entity_text, start)\n",
    "        if pos == -1:\n",
    "            break\n",
    "        occurrences.append((pos, pos + len(entity_text)))\n",
    "        start = pos + 1\n",
    "    return occurrences\n",
    "\n",
    "def remove_overlapping_entities(entities):\n",
    "    \"\"\"\n",
    "    Remove overlapping entities, keeping only the longest span.\n",
    "    entities: list of dicts with start_idx, end_idx, text_span, label, location\n",
    "    \"\"\"\n",
    "    if not entities:\n",
    "        return []\n",
    "    \n",
    "    # Sort by start position, then by length (longest first)\n",
    "    entities = sorted(entities, key=lambda x: (x['start_idx'], -(x['end_idx'] - x['start_idx'])))\n",
    "    \n",
    "    kept = []\n",
    "    for entity in entities:\n",
    "        # Check if this entity overlaps with any kept entity\n",
    "        overlaps = False\n",
    "        for kept_entity in kept:\n",
    "            # Check if same location and overlapping spans\n",
    "            if entity['location'] == kept_entity['location']:\n",
    "                # Check for overlap\n",
    "                if not (entity['end_idx'] <= kept_entity['start_idx'] or \n",
    "                       entity['start_idx'] >= kept_entity['end_idx']):\n",
    "                    overlaps = True\n",
    "                    break\n",
    "        \n",
    "        if not overlaps:\n",
    "            kept.append(entity)\n",
    "    \n",
    "    return kept\n",
    "\n",
    "# Process each document in dev set\n",
    "predictions = {}\n",
    "\n",
    "for pmid, article in tqdm(dev_data.items(), desc=\"Processing dev data\"):\n",
    "    title = article['metadata']['title']\n",
    "    abstract = article['metadata']['abstract']\n",
    "    \n",
    "    predicted_entities = []\n",
    "    \n",
    "    # Search for entities in title and abstract\n",
    "    for location, text in [('title', title), ('abstract', abstract)]:\n",
    "        # Try to match each entity from training data\n",
    "        for entity_text, labels in entity_list:\n",
    "            occurrences = find_all_occurrences(text, entity_text)\n",
    "            \n",
    "            for start_idx, end_idx in occurrences:\n",
    "                # For each label associated with this entity text\n",
    "                for label in labels:\n",
    "                    predicted_entities.append({\n",
    "                        \"start_idx\": start_idx,\n",
    "                        \"end_idx\": end_idx-1, # inclusive end index as per organizers specification\n",
    "                        \"location\": location,\n",
    "                        \"text_span\": entity_text,\n",
    "                        \"label\": label\n",
    "                    })\n",
    "    \n",
    "    # Remove overlapping entities (keep longest)\n",
    "    predicted_entities = remove_overlapping_entities(predicted_entities)\n",
    "    \n",
    "    # Store predictions\n",
    "    predictions[pmid] = {\n",
    "        \"entities\": predicted_entities\n",
    "    }\n",
    "\n",
    "print(f\"\\nProcessed {len(predictions)} documents\")\n",
    "print(f\"Total entities predicted: {sum(len(p['entities']) for p in predictions.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181e87ae",
   "metadata": {},
   "source": [
    "## Save Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13febdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions to file\n",
    "output_path = \"predictions/vanilla_ner_predictions.json\"\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(predictions, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Predictions saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bba4794",
   "metadata": {},
   "source": [
    "## Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53e4e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-contained evaluation function (adapted from official script)\n",
    "# Avoids importing evaluate.py which has hardcoded paths\n",
    "\n",
    "def remove_duplicated_entities(predictions):\n",
    "    \"\"\"Remove duplicated entities from predictions.\"\"\"\n",
    "    removed_count = 0\n",
    "    for pmid in list(predictions.keys()):\n",
    "        seen = set()\n",
    "        deduped = []\n",
    "        for ent in predictions[pmid][\"entities\"]:\n",
    "            key = (ent[\"start_idx\"], ent[\"end_idx\"], ent[\"location\"])\n",
    "            if key not in seen:\n",
    "                seen.add(key)\n",
    "                deduped.append(ent)\n",
    "            else:\n",
    "                removed_count += 1\n",
    "        predictions[pmid][\"entities\"] = deduped\n",
    "    \n",
    "    if removed_count > 0:\n",
    "        print(f\"Removed {removed_count} duplicated entities from predictions\")\n",
    "\n",
    "def remove_overlapping_entities_eval(predictions):\n",
    "    \"\"\"Remove overlapping entities, keeping longest spans.\"\"\"\n",
    "    removed_count = 0\n",
    "\n",
    "    for pmid in list(predictions.keys()):\n",
    "        original_len = len(predictions[pmid]['entities'])\n",
    "        \n",
    "        # Group entities by location\n",
    "        groups = {'title': [], 'abstract': []}\n",
    "        for ent in predictions[pmid]['entities']:\n",
    "            loc = ent[\"location\"]\n",
    "            groups[loc].append(ent)\n",
    "\n",
    "        # For each location, build overlap clusters and select the longest\n",
    "        keepers = set()\n",
    "        for loc in groups:\n",
    "            group = groups[loc]\n",
    "            group = sorted(group, key=lambda e: e[\"start_idx\"])\n",
    "\n",
    "            clusters = []\n",
    "            cluster = []\n",
    "            current_end = None\n",
    "\n",
    "            for ent in group:\n",
    "                if not cluster:\n",
    "                    cluster = [ent]\n",
    "                    current_end = ent[\"end_idx\"]\n",
    "                else:\n",
    "                    if ent[\"start_idx\"] < current_end:\n",
    "                        cluster.append(ent)\n",
    "                        if ent[\"end_idx\"] > current_end:\n",
    "                            current_end = ent[\"end_idx\"]\n",
    "                    else:\n",
    "                        clusters.append(cluster)\n",
    "                        cluster = [ent]\n",
    "                        current_end = ent[\"end_idx\"]\n",
    "            if cluster:\n",
    "                clusters.append(cluster)\n",
    "\n",
    "            # Pick the longest entity in each cluster\n",
    "            for clust in clusters:\n",
    "                longest = clust[0]\n",
    "                max_len = longest[\"end_idx\"] - longest[\"start_idx\"]\n",
    "                for ent in clust[1:]:\n",
    "                    length = ent[\"end_idx\"] - ent[\"start_idx\"]\n",
    "                    if length > max_len:\n",
    "                        longest = ent\n",
    "                        max_len = length\n",
    "                keepers.add((longest[\"start_idx\"],\n",
    "                             longest[\"end_idx\"],\n",
    "                             longest[\"location\"]))\n",
    "\n",
    "        # Rebuild the entity list\n",
    "        deduped = []\n",
    "        for ent in predictions[pmid]['entities']:\n",
    "            key = (ent[\"start_idx\"], ent[\"end_idx\"], ent[\"location\"])\n",
    "            if key in keepers:\n",
    "                deduped.append(ent)\n",
    "                keepers.remove(key)\n",
    "\n",
    "        predictions[pmid][\"entities\"] = deduped\n",
    "        removed_count += (original_len - len(deduped))\n",
    "\n",
    "    if removed_count > 0:\n",
    "        print(f\"Removed {removed_count} overlapping entities\")\n",
    "\n",
    "def evaluate_ner(predictions, ground_truth):\n",
    "    \"\"\"\n",
    "    Evaluate NER predictions against ground truth.\n",
    "    Based on the official evaluation script.\n",
    "    \"\"\"\n",
    "    # Remove duplicated and overlapping entities\n",
    "    remove_duplicated_entities(predictions)\n",
    "    remove_overlapping_entities_eval(predictions)\n",
    "    \n",
    "    LEGAL_ENTITY_LABELS = [\n",
    "        \"anatomical location\",\n",
    "        \"animal\",\n",
    "        \"bacteria\",\n",
    "        \"biomedical technique\",\n",
    "        \"chemical\",\n",
    "        \"DDF\",\n",
    "        \"dietary supplement\",\n",
    "        \"drug\",\n",
    "        \"food\",\n",
    "        \"gene\",\n",
    "        \"human\",\n",
    "        \"microbiome\",\n",
    "        \"statistical technique\"\n",
    "    ]\n",
    "    \n",
    "    ground_truth_NER = dict()\n",
    "    count_annotated_entities_per_label = {}\n",
    "    \n",
    "    for pmid, article in ground_truth.items():\n",
    "        if pmid not in ground_truth_NER:\n",
    "            ground_truth_NER[pmid] = []\n",
    "        for entity in article['entities']:\n",
    "            start_idx = int(entity[\"start_idx\"])\n",
    "            end_idx = int(entity[\"end_idx\"])\n",
    "            location = str(entity[\"location\"])\n",
    "            text_span = str(entity[\"text_span\"])\n",
    "            label = str(entity[\"label\"]) \n",
    "            \n",
    "            entry = (start_idx, end_idx, location, text_span, label)\n",
    "            ground_truth_NER[pmid].append(entry)\n",
    "            \n",
    "            if label not in count_annotated_entities_per_label:\n",
    "                count_annotated_entities_per_label[label] = 0\n",
    "            count_annotated_entities_per_label[label] += 1\n",
    "\n",
    "    count_predicted_entities_per_label = {label: 0 for label in list(count_annotated_entities_per_label.keys())}\n",
    "    count_true_positives_per_label = {label: 0 for label in list(count_annotated_entities_per_label.keys())}\n",
    "\n",
    "    for pmid in predictions.keys():\n",
    "        entities = predictions[pmid]['entities']\n",
    "        \n",
    "        for entity in entities:\n",
    "            start_idx = int(entity[\"start_idx\"])\n",
    "            end_idx = int(entity[\"end_idx\"])\n",
    "            location = str(entity[\"location\"])\n",
    "            text_span = str(entity[\"text_span\"])\n",
    "            label = str(entity[\"label\"]) \n",
    "            \n",
    "            if label not in LEGAL_ENTITY_LABELS:\n",
    "                print(f'Warning: Illegal label {label} for entity: {entity}')\n",
    "                continue\n",
    "\n",
    "            if label in count_predicted_entities_per_label:\n",
    "                count_predicted_entities_per_label[label] += 1\n",
    "\n",
    "            entry = (start_idx, end_idx, location, text_span, label)\n",
    "            if entry in ground_truth_NER[pmid]:\n",
    "                count_true_positives_per_label[label] += 1\n",
    "\n",
    "    count_annotated_entities = sum(count_annotated_entities_per_label[label] for label in list(count_annotated_entities_per_label.keys()))\n",
    "    count_predicted_entities = sum(count_predicted_entities_per_label[label] for label in list(count_annotated_entities_per_label.keys()))\n",
    "    count_true_positives = sum(count_true_positives_per_label[label] for label in list(count_annotated_entities_per_label.keys()))\n",
    "\n",
    "    micro_precision = count_true_positives / (count_predicted_entities + 1e-10)\n",
    "    micro_recall = count_true_positives / (count_annotated_entities + 1e-10)\n",
    "    micro_f1 = 2 * ((micro_precision * micro_recall) / (micro_precision + micro_recall + 1e-10))\n",
    "\n",
    "    precision, recall, f1 = 0, 0, 0\n",
    "    n = 0\n",
    "    for label in list(count_annotated_entities_per_label.keys()):\n",
    "        n += 1\n",
    "        current_precision = count_true_positives_per_label[label] / (count_predicted_entities_per_label[label] + 1e-10) \n",
    "        current_recall = count_true_positives_per_label[label] / (count_annotated_entities_per_label[label] + 1e-10) \n",
    "        \n",
    "        precision += current_precision\n",
    "        recall += current_recall\n",
    "        f1 += 2 * ((current_precision * current_recall) / (current_precision + current_recall + 1e-10))\n",
    "    \n",
    "    precision = precision / n\n",
    "    recall = recall / n\n",
    "    f1 = f1 / n\n",
    "\n",
    "    return precision, recall, f1, micro_precision, micro_recall, micro_f1\n",
    "\n",
    "# Evaluate\n",
    "precision, recall, f1, micro_precision, micro_recall, micro_f1 = evaluate_ner(predictions, dev_data)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"VANILLA NER BASELINE RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nMacro-averaged Metrics:\")\n",
    "print(f\"  Macro-Precision: {precision:.4f}\")\n",
    "print(f\"  Macro-Recall:    {recall:.4f}\")\n",
    "print(f\"  Macro-F1 Score:  {f1:.4f}\")\n",
    "\n",
    "print(\"\\nMicro-averaged Metrics:\")\n",
    "print(f\"  Micro-Precision: {micro_precision:.4f}\")\n",
    "print(f\"  Micro-Recall:    {micro_recall:.4f}\")\n",
    "print(f\"  Micro-F1 Score:  {micro_f1:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b305f0",
   "metadata": {},
   "source": [
    "## Example Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3790e278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example predictions\n",
    "print(\"Example Predictions:\\n\")\n",
    "\n",
    "# Get first few documents\n",
    "sample_pmids = list(dev_data.keys())[:5]\n",
    "\n",
    "for pmid in sample_pmids:\n",
    "    article = dev_data[pmid]\n",
    "    pred = predictions[pmid]\n",
    "    \n",
    "    print(f\"Document PMID: {pmid}\")\n",
    "    print(f\"Title: {article['metadata']['title'][:100]}...\")\n",
    "    print(f\"\\nGold entities: {len(article['entities'])}\")\n",
    "    print(f\"Predicted entities: {len(pred['entities'])}\")\n",
    "    \n",
    "    # Show first few predicted entities\n",
    "    print(\"\\nSample predictions:\")\n",
    "    for entity in pred['entities'][:5]:\n",
    "        print(f\"  - '{entity['text_span']}' [{entity['label']}] in {entity['location']}\")\n",
    "    \n",
    "    # Calculate match statistics for this document\n",
    "    gold_set = set()\n",
    "    for entity in article['entities']:\n",
    "        gold_set.add((\n",
    "            entity['start_idx'],\n",
    "            entity['end_idx'],\n",
    "            entity['location'],\n",
    "            entity['text_span'],\n",
    "            entity['label']\n",
    "        ))\n",
    "    \n",
    "    pred_set = set()\n",
    "    for entity in pred['entities']:\n",
    "        pred_set.add((\n",
    "            entity['start_idx'],\n",
    "            entity['end_idx'],\n",
    "            entity['location'],\n",
    "            entity['text_span'],\n",
    "            entity['label']\n",
    "        ))\n",
    "    \n",
    "    correct = len(gold_set & pred_set)\n",
    "    missed = len(gold_set - pred_set)\n",
    "    wrong = len(pred_set - gold_set)\n",
    "    \n",
    "    print(f\"\\n✓ Correct: {correct}\")\n",
    "    print(f\"✗ Missed: {missed}\")\n",
    "    print(f\"✗ Wrong: {wrong}\")\n",
    "    print(\"-\" * 80)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050ffd9f",
   "metadata": {},
   "source": [
    "## Analysis: Entity Distribution by Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d94e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count entities by label in predictions\n",
    "pred_label_counts = Counter()\n",
    "for pmid, pred in predictions.items():\n",
    "    for entity in pred['entities']:\n",
    "        pred_label_counts[entity['label']] += 1\n",
    "\n",
    "# Count entities by label in gold standard\n",
    "gold_label_counts = Counter()\n",
    "for pmid, article in dev_data.items():\n",
    "    for entity in article['entities']:\n",
    "        gold_label_counts[entity['label']] += 1\n",
    "\n",
    "print(\"Entity Distribution by Label:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Label':<25} {'Gold':<10} {'Predicted':<10}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "all_labels = set(gold_label_counts.keys()) | set(pred_label_counts.keys())\n",
    "for label in sorted(all_labels):\n",
    "    print(f\"{label:<25} {gold_label_counts[label]:<10} {pred_label_counts[label]:<10}\")\n",
    "\n",
    "print(\"-\"*60)\n",
    "print(f\"{'TOTAL':<25} {sum(gold_label_counts.values()):<10} {sum(pred_label_counts.values()):<10}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ate-it",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e6763fe",
   "metadata": {},
   "source": [
    "# Disclaimer\n",
    "The NER implementation using GLiNER provided in this notebook is just for educational/demonstrational purposes. \n",
    "\n",
    "To see a complete implementation tailored for the GutBrainIE NER task, please refer to the baseline system implementation provided by the organizers:\n",
    "\n",
    "https://github.com/MMartinelli-hub/GutBrainIE_2025_Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a4d39f",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f663fafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install GLiNER if not already installed\n",
    "# !pip install gliner -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d9e19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from gliner import GLiNER\n",
    "from gliner2 import GLiNER2\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Setup complete\")\n",
    "print(f\"GLiNER imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a652b3ab",
   "metadata": {},
   "source": [
    "## Define Entity Labels\n",
    "\n",
    "GLiNER can work with natural language descriptions of entity types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e22e764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define entity labels - GLiNER works with natural language descriptions\n",
    "ENTITY_LABELS = [\n",
    "    \"anatomical location\",\n",
    "    \"animal\",\n",
    "    \"bacteria\",\n",
    "    \"biomedical technique\",\n",
    "    \"chemical\",\n",
    "    \"disease, disorder or finding\",  # DDF\n",
    "    \"dietary supplement\",\n",
    "    \"drug\",\n",
    "    \"food\",\n",
    "    \"gene\",\n",
    "    \"human\",\n",
    "    \"microbiome\",\n",
    "    \"statistical technique\"\n",
    "]\n",
    "\n",
    "# Map GLiNER output back to original labels\n",
    "LABEL_MAPPING = {\n",
    "    \"disease, disorder or finding\": \"DDF\"\n",
    "}\n",
    "\n",
    "def normalize_label(label):\n",
    "    \"\"\"Normalize GLiNER label to match original format.\"\"\"\n",
    "    return LABEL_MAPPING.get(label, label)\n",
    "\n",
    "print(f\"Entity labels ({len(ENTITY_LABELS)}):\")\n",
    "for label in ENTITY_LABELS:\n",
    "    print(f\"  - {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0b5607",
   "metadata": {},
   "source": [
    "## Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245784fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ner_data(file_paths):\n",
    "    \"\"\"\n",
    "    Load NER data from multiple JSON files.\n",
    "    \"\"\"\n",
    "    all_data = {}\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            all_data.update(data)\n",
    "            print(f\"Loaded {len(data)} documents from {os.path.basename(file_path)}\")\n",
    "        else:\n",
    "            print(f\"Warning: {file_path} not found\")\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "\n",
    "print(\"✓ Data loading functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf9d0ff",
   "metadata": {},
   "source": [
    "## Load Dev Data\n",
    "\n",
    "We'll use the dev set for zero-shot evaluation (no training needed for GLiNER)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d13e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dev data\n",
    "dev_data = load_ner_data([\"../data/Annotations/Dev/json_format/dev.json\"])\n",
    "\n",
    "print(f\"\\nTotal dev documents: {len(dev_data)}\")\n",
    "\n",
    "# Count total entities\n",
    "total_entities = sum(len(article['entities']) for article in dev_data.values())\n",
    "print(f\"Total entities in dev set: {total_entities}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d97e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example document\n",
    "example_pmid = list(dev_data.keys())[0]\n",
    "example_article = dev_data[example_pmid]\n",
    "\n",
    "print(f\"Example document (PMID: {example_pmid}):\")\n",
    "print(f\"  Title: {example_article['metadata']['title'][:100]}...\")\n",
    "print(f\"  Abstract: {example_article['metadata']['abstract'][:150]}...\")\n",
    "print(f\"  Number of entities: {len(example_article['entities'])}\")\n",
    "print(f\"\\nFirst 3 entities:\")\n",
    "for entity in example_article['entities'][:3]:\n",
    "    print(f\"    - '{entity['text_span']}' [{entity['label']}] in {entity['location']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf4efd8",
   "metadata": {},
   "source": [
    "## Load GLiNER Models\n",
    "\n",
    "We'll load both GLiNER v1 and v2 for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a42d346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GLiNER v1 (base model)\n",
    "print(\"Loading GLiNER v1 (base model)...\")\n",
    "#model_v1_name = \"urchade/gliner_base\"\n",
    "model_v1_name = \"numind/NuNER_Zero\"\n",
    "model_v1 = GLiNER.from_pretrained(model_v1_name)\n",
    "print(\"✓ GLiNER v1 loaded\")\n",
    "\n",
    "print(f\"\\nModel details:\")\n",
    "print(f\"  Model name: {model_v1_name}\")\n",
    "print(f\"  Type: Zero-shot NER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f759f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GLiNER v2 (improved model)\n",
    "print(\"Loading GLiNER v2 (multi-task model)...\")\n",
    "model_v2_name = \"fastino/gliner2-base-v1\"\n",
    "model_v2 = GLiNER2.from_pretrained(model_v2_name)\n",
    "print(\"✓ GLiNER v2 loaded\")\n",
    "\n",
    "print(f\"\\nModel details:\")\n",
    "print(f\"  Model name: {model_v2_name}\")\n",
    "print(f\"  Type: Zero-shot NER (improved)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22057c6f",
   "metadata": {},
   "source": [
    "## Test GLiNER on Sample Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdd1d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a sample sentence\n",
    "sample_text = \"The gut microbiome has been shown to have key implications in the pathogenesis of Parkinson's disease.\"\n",
    "\n",
    "print(\"Sample text:\")\n",
    "print(f\"  {sample_text}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# GLiNER v1 predictions\n",
    "print(\"\\nGLiNER v1 predictions:\")\n",
    "entities_v1 = model_v1.predict_entities(sample_text, ENTITY_LABELS, threshold=0.3)\n",
    "for entity in entities_v1:\n",
    "    print(f\"  - '{entity['text']}' [{entity['label']}] (score: {entity['score']:.3f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# GLiNER v2 predictions\n",
    "print(\"\\nGLiNER v2 predictions:\")\n",
    "entities_v2 = model_v2.extract_entities(sample_text, ENTITY_LABELS, threshold=0.3)\n",
    "for label, entities in entities_v2['entities'].items():\n",
    "    if len(entities) == 0:\n",
    "        print(f\"  - '{label}': []\")        \n",
    "    else:            \n",
    "        entities_print = \"\"        \n",
    "        for ent in entities:\n",
    "            entities_print += f\"'{ent}',\"  \n",
    "        print(f\"  - '{label}': {entities_print}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3eaf18e",
   "metadata": {},
   "source": [
    "## Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059ec442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_entities_gliner_v1(model, text, labels, threshold=0.3, location=\"abstract\"):\n",
    "    \"\"\"\n",
    "    Predict entities using GLiNER v1 model.\n",
    "    Returns entities in the format expected by evaluation.\n",
    "    \"\"\"\n",
    "    # Get predictions from GLiNER v1\n",
    "    entities = model.predict_entities(text, labels, threshold=threshold)\n",
    "    \n",
    "    # Convert to expected format\n",
    "    formatted_entities = []\n",
    "    for entity in entities:\n",
    "        formatted_entities.append({\n",
    "            'start_idx': entity['start'],\n",
    "            'end_idx': entity['end'],\n",
    "            'text_span': entity['text'],\n",
    "            'label': normalize_label(entity['label']),\n",
    "            'location': location,\n",
    "            'score': entity['score']\n",
    "        })\n",
    "    \n",
    "    return formatted_entities\n",
    "\n",
    "\n",
    "def predict_entities_gliner_v2(model, text, labels, threshold=0.3, location=\"abstract\"):\n",
    "    \"\"\"\n",
    "    Predict entities using GLiNER v2 model.\n",
    "    Returns entities in the format expected by evaluation.\n",
    "    GLiNER v2 uses extract_entities and returns a dictionary format.\n",
    "    \"\"\"    \n",
    "    # Get predictions from GLiNER v2                \n",
    "    result = model.extract_entities(text, labels, threshold=threshold)                \n",
    "\n",
    "    # Convert to expected format                    \n",
    "    formatted_entities = []                    \n",
    "    for label, entity_texts in result['entities'].items():                    \n",
    "        # For each entity text, find its position in the original text                    \n",
    "        for entity_text in entity_texts:                    \n",
    "            # Find all occurrences of this entity in the text                    \n",
    "            start_idx = 0                \n",
    "            while True:                # Add this entity\n",
    "                start_idx = text.find(entity_text, start_idx)                \n",
    "                end_idx = start_idx + len(entity_text)\n",
    "                if start_idx == -1:                \n",
    "                    break    \n",
    "                formatted_entities.append({\n",
    "                    'start_idx': start_idx,\n",
    "                    'end_idx': end_idx,\n",
    "                    'text_span': entity_text,\n",
    "                    'label': normalize_label(label),\n",
    "                    'location': location,\n",
    "                    'score': 1.0  # GLiNER v2 doesn't provide scores in the same way\n",
    "                })\n",
    "    return formatted_entities\n",
    "    \n",
    "print(\"✓ Inference functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056f1120",
   "metadata": {},
   "source": [
    "## Predict on Dev Set - GLiNER v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfd03df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference with GLiNER v1\n",
    "print(\"Running inference with GLiNER v1 on dev set...\")\n",
    "print(\"Note: This may take several minutes\\n\")\n",
    "\n",
    "predictions_v1 = {}\n",
    "threshold_v1 = 0.3  # Adjust threshold as needed\n",
    "\n",
    "for pmid, article in tqdm(dev_data.items(), desc=\"GLiNER v1 inference\"):\n",
    "    all_entities = []\n",
    "    \n",
    "    # Process title\n",
    "    title = article['metadata']['title']\n",
    "    title_entities = predict_entities_gliner_v1(\n",
    "        model_v1, title, ENTITY_LABELS, \n",
    "        threshold=threshold_v1, location='title'\n",
    "    )\n",
    "    all_entities.extend(title_entities)\n",
    "    \n",
    "    # Process abstract\n",
    "    abstract = article['metadata']['abstract']\n",
    "    abstract_entities = predict_entities_gliner_v1(\n",
    "        model_v1, abstract, ENTITY_LABELS, \n",
    "        threshold=threshold_v1, location='abstract'\n",
    "    )\n",
    "    all_entities.extend(abstract_entities)\n",
    "    \n",
    "    predictions_v1[pmid] = {'entities': all_entities}\n",
    "\n",
    "total_entities_v1 = sum(len(p['entities']) for p in predictions_v1.values())\n",
    "print(f\"\\n✓ GLiNER v1 inference completed\")\n",
    "print(f\"  Total entities predicted: {total_entities_v1}\")\n",
    "print(f\"  Threshold used: {threshold_v1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d584b768",
   "metadata": {},
   "source": [
    "## Save GLiNER v1 Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07ae1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save GLiNER v1 predictions\n",
    "output_path_v1 = \"predictions/gliner_v1_predictions_zero_shot.json\"\n",
    "\n",
    "with open(output_path_v1, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(predictions_v1, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"GLiNER v1 predictions saved to {output_path_v1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066dc9b6",
   "metadata": {},
   "source": [
    "## Predict on Dev Set - GLiNER v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d68060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference with GLiNER v2\n",
    "print(\"Running inference with GLiNER v2 on dev set...\")\n",
    "print(\"Note: This may take several minutes\\n\")\n",
    "\n",
    "predictions_v2 = {}\n",
    "threshold_v2 = 0.3  # Adjust threshold as needed\n",
    "\n",
    "for pmid, article in tqdm(dev_data.items(), desc=\"GLiNER v2 inference\"):\n",
    "    all_entities = []\n",
    "    \n",
    "    # Process title\n",
    "    title = article['metadata']['title']\n",
    "    title_entities = predict_entities_gliner_v2(\n",
    "        model_v2, title, ENTITY_LABELS, \n",
    "        threshold=threshold_v2, location='title'\n",
    "    )\n",
    "    all_entities.extend(title_entities)\n",
    "    \n",
    "    # Process abstract\n",
    "    abstract = article['metadata']['abstract']\n",
    "    abstract_entities = predict_entities_gliner_v2(\n",
    "        model_v2, abstract, ENTITY_LABELS, \n",
    "        threshold=threshold_v2, location='abstract'\n",
    "    )\n",
    "    all_entities.extend(abstract_entities)\n",
    "    \n",
    "    predictions_v2[pmid] = {'entities': all_entities}\n",
    "\n",
    "total_entities_v2 = sum(len(p['entities']) for p in predictions_v2.values())\n",
    "print(f\"\\n✓ GLiNER v2 inference completed\")\n",
    "print(f\"  Total entities predicted: {total_entities_v2}\")\n",
    "print(f\"  Threshold used: {threshold_v2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4901bb",
   "metadata": {},
   "source": [
    "## Save GLiNER v2 Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ea6990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save GLiNER v2 predictions\n",
    "output_path_v2 = \"predictions/gliner_v2_predictions_zero_shot.json\"\n",
    "\n",
    "with open(output_path_v2, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(predictions_v2, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"GLiNER v2 predictions saved to {output_path_v2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d986bb",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229f4f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicated_entities(predictions):\n",
    "    \"\"\"Remove duplicated entities from predictions.\"\"\"\n",
    "    removed_count = 0\n",
    "    for pmid in list(predictions.keys()):\n",
    "        seen = set()\n",
    "        deduped = []\n",
    "        for ent in predictions[pmid][\"entities\"]:\n",
    "            key = (ent[\"start_idx\"], ent[\"end_idx\"], ent[\"location\"])\n",
    "            if key not in seen:\n",
    "                seen.add(key)\n",
    "                deduped.append(ent)\n",
    "            else:\n",
    "                removed_count += 1\n",
    "        predictions[pmid][\"entities\"] = deduped\n",
    "    \n",
    "    if removed_count > 0:\n",
    "        print(f\"Removed {removed_count} duplicated entities from predictions\")\n",
    "\n",
    "def remove_overlapping_entities_eval(predictions):\n",
    "    \"\"\"Remove overlapping entities, keeping longest spans.\"\"\"\n",
    "    removed_count = 0\n",
    "\n",
    "    for pmid in list(predictions.keys()):\n",
    "        original_len = len(predictions[pmid]['entities'])\n",
    "        \n",
    "        groups = {'title': [], 'abstract': []}\n",
    "        for ent in predictions[pmid]['entities']:\n",
    "            loc = ent[\"location\"]\n",
    "            groups[loc].append(ent)\n",
    "\n",
    "        keepers = set()\n",
    "        for loc in groups:\n",
    "            group = groups[loc]\n",
    "            group = sorted(group, key=lambda e: e[\"start_idx\"])\n",
    "\n",
    "            clusters = []\n",
    "            cluster = []\n",
    "            current_end = None\n",
    "\n",
    "            for ent in group:\n",
    "                if not cluster:\n",
    "                    cluster = [ent]\n",
    "                    current_end = ent[\"end_idx\"]\n",
    "                else:\n",
    "                    if ent[\"start_idx\"] < current_end:\n",
    "                        cluster.append(ent)\n",
    "                        if ent[\"end_idx\"] > current_end:\n",
    "                            current_end = ent[\"end_idx\"]\n",
    "                    else:\n",
    "                        clusters.append(cluster)\n",
    "                        cluster = [ent]\n",
    "                        current_end = ent[\"end_idx\"]\n",
    "            if cluster:\n",
    "                clusters.append(cluster)\n",
    "\n",
    "            for clust in clusters:\n",
    "                longest = clust[0]\n",
    "                max_len = longest[\"end_idx\"] - longest[\"start_idx\"]\n",
    "                for ent in clust[1:]:\n",
    "                    length = ent[\"end_idx\"] - ent[\"start_idx\"]\n",
    "                    if length > max_len:\n",
    "                        longest = ent\n",
    "                        max_len = length\n",
    "                keepers.add((longest[\"start_idx\"],\n",
    "                             longest[\"end_idx\"],\n",
    "                             longest[\"location\"]))\n",
    "\n",
    "        deduped = []\n",
    "        for ent in predictions[pmid]['entities']:\n",
    "            key = (ent[\"start_idx\"], ent[\"end_idx\"], ent[\"location\"])\n",
    "            if key in keepers:\n",
    "                deduped.append(ent)\n",
    "                keepers.remove(key)\n",
    "\n",
    "        predictions[pmid][\"entities\"] = deduped\n",
    "        removed_count += (original_len - len(deduped))\n",
    "\n",
    "    if removed_count > 0:\n",
    "        print(f\"Removed {removed_count} overlapping entities\")\n",
    "\n",
    "def evaluate_ner(predictions, ground_truth):\n",
    "    \"\"\"Evaluate NER predictions against ground truth.\"\"\"\n",
    "    # Remove duplicated and overlapping entities\n",
    "    remove_duplicated_entities(predictions)\n",
    "    remove_overlapping_entities_eval(predictions)\n",
    "    \n",
    "    LEGAL_ENTITY_LABELS = [\n",
    "        \"anatomical location\", \"animal\", \"bacteria\", \"biomedical technique\",\n",
    "        \"chemical\", \"DDF\", \"dietary supplement\", \"drug\", \"food\", \"gene\",\n",
    "        \"human\", \"microbiome\", \"statistical technique\"\n",
    "    ]\n",
    "    \n",
    "    ground_truth_NER = dict()\n",
    "    count_annotated_entities_per_label = {}\n",
    "    \n",
    "    for pmid, article in ground_truth.items():\n",
    "        if pmid not in ground_truth_NER:\n",
    "            ground_truth_NER[pmid] = []\n",
    "        for entity in article['entities']:\n",
    "            start_idx = int(entity[\"start_idx\"])\n",
    "            end_idx = int(entity[\"end_idx\"])\n",
    "            location = str(entity[\"location\"])\n",
    "            text_span = str(entity[\"text_span\"])\n",
    "            label = str(entity[\"label\"]) \n",
    "            \n",
    "            entry = (start_idx, end_idx, location, text_span, label)\n",
    "            ground_truth_NER[pmid].append(entry)\n",
    "            \n",
    "            if label not in count_annotated_entities_per_label:\n",
    "                count_annotated_entities_per_label[label] = 0\n",
    "            count_annotated_entities_per_label[label] += 1\n",
    "\n",
    "    count_predicted_entities_per_label = {label: 0 for label in list(count_annotated_entities_per_label.keys())}\n",
    "    count_true_positives_per_label = {label: 0 for label in list(count_annotated_entities_per_label.keys())}\n",
    "\n",
    "    for pmid in predictions.keys():\n",
    "        entities = predictions[pmid]['entities']\n",
    "        \n",
    "        for entity in entities:\n",
    "            start_idx = int(entity[\"start_idx\"])\n",
    "            end_idx = int(entity[\"end_idx\"])\n",
    "            location = str(entity[\"location\"])\n",
    "            text_span = str(entity[\"text_span\"])\n",
    "            label = str(entity[\"label\"]) \n",
    "            \n",
    "            if label not in LEGAL_ENTITY_LABELS:\n",
    "                continue\n",
    "\n",
    "            if label in count_predicted_entities_per_label:\n",
    "                count_predicted_entities_per_label[label] += 1\n",
    "\n",
    "            entry = (start_idx, end_idx, location, text_span, label)\n",
    "            if pmid in ground_truth_NER and entry in ground_truth_NER[pmid]:\n",
    "                count_true_positives_per_label[label] += 1\n",
    "\n",
    "    count_annotated_entities = sum(count_annotated_entities_per_label.values())\n",
    "    count_predicted_entities = sum(count_predicted_entities_per_label.values())\n",
    "    count_true_positives = sum(count_true_positives_per_label.values())\n",
    "\n",
    "    micro_precision = count_true_positives / (count_predicted_entities + 1e-10)\n",
    "    micro_recall = count_true_positives / (count_annotated_entities + 1e-10)\n",
    "    micro_f1 = 2 * ((micro_precision * micro_recall) / (micro_precision + micro_recall + 1e-10))\n",
    "\n",
    "    precision, recall, f1 = 0, 0, 0\n",
    "    n = len(count_annotated_entities_per_label)\n",
    "    for label in count_annotated_entities_per_label.keys():\n",
    "        current_precision = count_true_positives_per_label[label] / (count_predicted_entities_per_label[label] + 1e-10) \n",
    "        current_recall = count_true_positives_per_label[label] / (count_annotated_entities_per_label[label] + 1e-10) \n",
    "        \n",
    "        precision += current_precision\n",
    "        recall += current_recall\n",
    "        f1 += 2 * ((current_precision * current_recall) / (current_precision + current_recall + 1e-10))\n",
    "    \n",
    "    precision = precision / n\n",
    "    recall = recall / n\n",
    "    f1 = f1 / n\n",
    "\n",
    "    return precision, recall, f1, micro_precision, micro_recall, micro_f1\n",
    "\n",
    "\n",
    "print(\"✓ Evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065c0e7c",
   "metadata": {},
   "source": [
    "## Evaluate GLiNER v1 Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1a2553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate GLiNER v1\n",
    "import json\n",
    "print(\"Evaluating GLiNER v1...\\n\")\n",
    "predictions_v1 = json.load(open(\"predictions/gliner_v1_predictions_zero_shot_eval_format.json\", \"r\", encoding=\"utf-8\"))\n",
    "precision_v1, recall_v1, f1_v1, micro_precision_v1, micro_recall_v1, micro_f1_v1 = evaluate_ner(\n",
    "    predictions_v1.copy(), dev_data\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"GLiNER v1 RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nMacro-averaged Metrics:\")\n",
    "print(f\"  Macro-Precision: {precision_v1:.4f}\")\n",
    "print(f\"  Macro-Recall:    {recall_v1:.4f}\")\n",
    "print(f\"  Macro-F1 Score:  {f1_v1:.4f}\")\n",
    "\n",
    "print(\"\\nMicro-averaged Metrics:\")\n",
    "print(f\"  Micro-Precision: {micro_precision_v1:.4f}\")\n",
    "print(f\"  Micro-Recall:    {micro_recall_v1:.4f}\")\n",
    "print(f\"  Micro-F1 Score:  {micro_f1_v1:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2beb1b",
   "metadata": {},
   "source": [
    "## Evaluate GLiNER v2 Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c751a3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate GLiNER v2\n",
    "print(\"Evaluating GLiNER v2...\\n\")\n",
    "precision_v2, recall_v2, f1_v2, micro_precision_v2, micro_recall_v2, micro_f1_v2 = evaluate_ner(\n",
    "    predictions_v2.copy(), dev_data\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"GLiNER v2 RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nMacro-averaged Metrics:\")\n",
    "print(f\"  Macro-Precision: {precision_v2:.4f}\")\n",
    "print(f\"  Macro-Recall:    {recall_v2:.4f}\")\n",
    "print(f\"  Macro-F1 Score:  {f1_v2:.4f}\")\n",
    "\n",
    "print(\"\\nMicro-averaged Metrics:\")\n",
    "print(f\"  Micro-Precision: {micro_precision_v2:.4f}\")\n",
    "print(f\"  Micro-Recall:    {micro_recall_v2:.4f}\")\n",
    "print(f\"  Micro-F1 Score:  {micro_f1_v2:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e70c1d",
   "metadata": {},
   "source": [
    "## Compare GLiNER v1 vs v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b9354f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison table\n",
    "comparison_data = {\n",
    "    'Metric': [\n",
    "        'Macro-Precision', 'Macro-Recall', 'Macro-F1',\n",
    "        'Micro-Precision', 'Micro-Recall', 'Micro-F1'\n",
    "    ],\n",
    "    'GLiNER v1': [\n",
    "        f\"{precision_v1:.4f}\", f\"{recall_v1:.4f}\", f\"{f1_v1:.4f}\",\n",
    "        f\"{micro_precision_v1:.4f}\", f\"{micro_recall_v1:.4f}\", f\"{micro_f1_v1:.4f}\"\n",
    "    ],\n",
    "    'GLiNER v2': [\n",
    "        f\"{precision_v2:.4f}\", f\"{recall_v2:.4f}\", f\"{f1_v2:.4f}\",\n",
    "        f\"{micro_precision_v2:.4f}\", f\"{micro_recall_v2:.4f}\", f\"{micro_f1_v2:.4f}\"\n",
    "    ],\n",
    "    'Improvement': [\n",
    "        f\"{(precision_v2 - precision_v1):.4f}\",\n",
    "        f\"{(recall_v2 - recall_v1):.4f}\",\n",
    "        f\"{(f1_v2 - f1_v1):.4f}\",\n",
    "        f\"{(micro_precision_v2 - micro_precision_v1):.4f}\",\n",
    "        f\"{(micro_recall_v2 - micro_recall_v1):.4f}\",\n",
    "        f\"{(micro_f1_v2 - micro_f1_v1):.4f}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GLiNER v1 vs v2 COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(df_comparison.to_string(index=False))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4ad826",
   "metadata": {},
   "source": [
    "## Example Predictions - GLiNER v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a6e4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example predictions from GLiNER v1\n",
    "print(\"Example Predictions - GLiNER v1:\\n\")\n",
    "\n",
    "sample_pmids = list(dev_data.keys())[:3]\n",
    "\n",
    "for pmid in sample_pmids:\n",
    "    article = dev_data[pmid]\n",
    "    pred = predictions_v1[pmid]\n",
    "    \n",
    "    print(f\"Document PMID: {pmid}\")\n",
    "    print(f\"Title: {article['metadata']['title'][:80]}...\")\n",
    "    print(f\"\\nGold entities: {len(article['entities'])}\")\n",
    "    print(f\"Predicted entities: {len(pred['entities'])}\")\n",
    "    \n",
    "    # Show first few predicted entities\n",
    "    print(\"\\nSample predictions:\")\n",
    "    for entity in pred['entities'][:5]:\n",
    "        score_str = f\" (score: {entity['score']:.2f})\" if 'score' in entity else \"\"\n",
    "        print(f\"  - '{entity['text_span']}' [{entity['label']}]{score_str} in {entity['location']}\")\n",
    "    \n",
    "    # Calculate match statistics\n",
    "    gold_set = set()\n",
    "    for entity in article['entities']:\n",
    "        gold_set.add((\n",
    "            entity['start_idx'],\n",
    "            entity['end_idx'],\n",
    "            entity['location'],\n",
    "            entity['text_span'],\n",
    "            entity['label']\n",
    "        ))\n",
    "    \n",
    "    pred_set = set()\n",
    "    for entity in pred['entities']:\n",
    "        pred_set.add((\n",
    "            entity['start_idx'],\n",
    "            entity['end_idx'],\n",
    "            entity['location'],\n",
    "            entity['text_span'],\n",
    "            entity['label']\n",
    "        ))\n",
    "    \n",
    "    correct = len(gold_set & pred_set)\n",
    "    missed = len(gold_set - pred_set)\n",
    "    wrong = len(pred_set - gold_set)\n",
    "    \n",
    "    print(f\"\\n✓ Correct: {correct}\")\n",
    "    print(f\"✗ Missed: {missed}\")\n",
    "    print(f\"✗ Wrong: {wrong}\")\n",
    "    print(\"-\" * 80)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ff180c",
   "metadata": {},
   "source": [
    "## Example Predictions - GLiNER v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512a3cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example predictions from GLiNER v2\n",
    "print(\"Example Predictions - GLiNER v2:\\n\")\n",
    "\n",
    "sample_pmids = list(dev_data.keys())[:3]\n",
    "\n",
    "for pmid in sample_pmids:\n",
    "    article = dev_data[pmid]\n",
    "    pred = predictions_v2[pmid]\n",
    "    \n",
    "    print(f\"Document PMID: {pmid}\")\n",
    "    print(f\"Title: {article['metadata']['title'][:80]}...\")\n",
    "    print(f\"\\nGold entities: {len(article['entities'])}\")\n",
    "    print(f\"Predicted entities: {len(pred['entities'])}\")\n",
    "    \n",
    "    # Show first few predicted entities\n",
    "    print(\"\\nSample predictions:\")\n",
    "    for entity in pred['entities'][:5]:\n",
    "        score_str = f\" (score: {entity['score']:.2f})\" if 'score' in entity else \"\"\n",
    "        print(f\"  - '{entity['text_span']}' [{entity['label']}]{score_str} in {entity['location']}\")\n",
    "    \n",
    "    # Calculate match statistics\n",
    "    gold_set = set()\n",
    "    for entity in article['entities']:\n",
    "        gold_set.add((\n",
    "            entity['start_idx'],\n",
    "            entity['end_idx'],\n",
    "            entity['location'],\n",
    "            entity['text_span'],\n",
    "            entity['label']\n",
    "        ))\n",
    "    \n",
    "    pred_set = set()\n",
    "    for entity in pred['entities']:\n",
    "        pred_set.add((\n",
    "            entity['start_idx'],\n",
    "            entity['end_idx'],\n",
    "            entity['location'],\n",
    "            entity['text_span'],\n",
    "            entity['label']\n",
    "        ))\n",
    "    \n",
    "    correct = len(gold_set & pred_set)\n",
    "    missed = len(gold_set - pred_set)\n",
    "    wrong = len(pred_set - gold_set)\n",
    "    \n",
    "    print(f\"\\n✓ Correct: {correct}\")\n",
    "    print(f\"✗ Missed: {missed}\")\n",
    "    print(f\"✗ Wrong: {wrong}\")\n",
    "    print(\"-\" * 80)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eb28d9",
   "metadata": {},
   "source": [
    "## Entity Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80594128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count entities by label in gold standard\n",
    "gold_label_counts = Counter()\n",
    "for pmid, article in dev_data.items():\n",
    "    for entity in article['entities']:\n",
    "        gold_label_counts[entity['label']] += 1\n",
    "\n",
    "# Count entities by label in GLiNER v1 predictions\n",
    "pred_v1_label_counts = Counter()\n",
    "for pmid, pred in predictions_v1.items():\n",
    "    for entity in pred['entities']:\n",
    "        pred_v1_label_counts[entity['label']] += 1\n",
    "\n",
    "# Count entities by label in GLiNER v2 predictions\n",
    "pred_v2_label_counts = Counter()\n",
    "for pmid, pred in predictions_v2.items():\n",
    "    for entity in pred['entities']:\n",
    "        pred_v2_label_counts[entity['label']] += 1\n",
    "\n",
    "print(\"Entity Distribution by Label:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Label':<25} {'Gold':<12} {'GLiNER v1':<12} {'GLiNER v2':<12}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "all_labels = set(gold_label_counts.keys()) | set(pred_v1_label_counts.keys()) | set(pred_v2_label_counts.keys())\n",
    "for label in sorted(all_labels):\n",
    "    print(f\"{label:<25} {gold_label_counts[label]:<12} {pred_v1_label_counts[label]:<12} {pred_v2_label_counts[label]:<12}\")\n",
    "\n",
    "print(\"-\"*80)\n",
    "print(f\"{'TOTAL':<25} {sum(gold_label_counts.values()):<12} {sum(pred_v1_label_counts.values()):<12} {sum(pred_v2_label_counts.values()):<12}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62811863",
   "metadata": {},
   "source": [
    "## Threshold Tuning (Optional)\n",
    "\n",
    "GLiNER confidence threshold can be adjusted to optimize performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c750ef6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different thresholds for GLiNER v2 on a subset\n",
    "print(\"Testing different confidence thresholds for GLiNER v2...\\n\")\n",
    "\n",
    "thresholds = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "threshold_results = []\n",
    "\n",
    "# Use a subset for faster testing\n",
    "subset_size = min(50, len(dev_data))\n",
    "subset_pmids = list(dev_data.keys())[:subset_size]\n",
    "subset_data = {pmid: dev_data[pmid] for pmid in subset_pmids}\n",
    "\n",
    "for threshold in thresholds:\n",
    "    print(f\"Testing threshold {threshold}...\")\n",
    "    \n",
    "    predictions_test = {}\n",
    "    for pmid, article in subset_data.items():\n",
    "        all_entities = []\n",
    "        \n",
    "        title = article['metadata']['title']\n",
    "        title_entities = predict_entities_gliner_v2(\n",
    "            model_v2, title, ENTITY_LABELS, \n",
    "            threshold=threshold, location='title'\n",
    "        )\n",
    "        all_entities.extend(title_entities)\n",
    "        \n",
    "        abstract = article['metadata']['abstract']\n",
    "        abstract_entities = predict_entities_gliner_v2(\n",
    "            model_v2, abstract, ENTITY_LABELS, \n",
    "            threshold=threshold, location='abstract'\n",
    "        )\n",
    "        all_entities.extend(abstract_entities)\n",
    "        \n",
    "        predictions_test[pmid] = {'entities': all_entities}\n",
    "    \n",
    "    # Evaluate\n",
    "    p, r, f1, mp, mr, mf1 = evaluate_ner(predictions_test.copy(), subset_data)\n",
    "    \n",
    "    threshold_results.append({\n",
    "        'threshold': threshold,\n",
    "        'macro_f1': f1,\n",
    "        'micro_f1': mf1,\n",
    "        'macro_precision': p,\n",
    "        'macro_recall': r\n",
    "    })\n",
    "\n",
    "# Display results\n",
    "df_thresholds = pd.DataFrame(threshold_results)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"THRESHOLD TUNING RESULTS (on subset)\")\n",
    "print(\"=\"*60)\n",
    "print(df_thresholds.to_string(index=False))\n",
    "print(\"\\nBest threshold by Micro-F1:\", df_thresholds.loc[df_thresholds['micro_f1'].idxmax(), 'threshold'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6567f4b6",
   "metadata": {},
   "source": [
    "## Summary and Conclusions\n",
    "\n",
    "### Key Observations:\n",
    "\n",
    "1. **Zero-shot Performance**: GLiNER can perform NER without any training on the target dataset\n",
    "2. **Version Comparison**: GLiNER v2 typically shows improved performance over v1\n",
    "3. **Threshold Impact**: The confidence threshold significantly affects precision/recall trade-off\n",
    "4. **Advantages**:\n",
    "   - No training required\n",
    "   - Works with arbitrary entity labels\n",
    "   - Fast inference\n",
    "   - Easy to deploy\n",
    "\n",
    "5. **Limitations**:\n",
    "   - May not match fine-tuned BERT performance\n",
    "   - Domain-specific terminology might be challenging\n",
    "   - Threshold tuning needed for optimal results\n",
    "\n",
    "### Next Steps:\n",
    "- Fine-tune GLiNER on training data for better performance\n",
    "- Experiment with different entity label formulations\n",
    "- Combine with other models in an ensemble\n",
    "- Try domain-specific GLiNER variants if available"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ate-it",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

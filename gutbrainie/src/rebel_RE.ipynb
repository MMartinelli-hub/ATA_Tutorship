{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110cd9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O data.zip https://github.com/MMartinelli-hub/ATA_Tutorship/raw/refs/heads/main/gutbrainie/data/GutBrainIE_Full_Collection_2025.zip\n",
    "!unzip data.zip \"*\"\n",
    "!mkdir data\n",
    "!mv Annotations/ ./data\n",
    "!mv Articles/ ./data\n",
    "!mv Test_Data/ ./data\n",
    "!rm -rf data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae9c6dd",
   "metadata": {},
   "source": [
    "# REBEL: Relation Extraction By End-to-end Language Generation\n",
    "\n",
    "This notebook demonstrates **REBEL** (Relation Extraction By End-to-end Language generation), a seq2seq model based on BART that performs end-to-end relation extraction.\n",
    "\n",
    "## Key Features:\n",
    "- **End-to-end approach**: No separate entity extraction and relation classification steps\n",
    "- **Sequence-to-sequence**: Generates relation triplets as linearized text sequences\n",
    "- **Zero-shot capable**: Can extract relations without task-specific fine-tuning\n",
    "- **Format**: Generates triplets in format: `<triplet> subject <subj> object <obj> predicate`\n",
    "\n",
    "## References:\n",
    "- Paper: [REBEL: Relation Extraction By End-to-end Language generation (EMNLP 2021)](https://aclanthology.org/2021.findings-emnlp.204/)\n",
    "- Model: [Babelscape/rebel-large](https://huggingface.co/Babelscape/rebel-large)\n",
    "- Repository: [babelscape/rebel](https://github.com/babelscape/rebel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e2ce47",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c996763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "print(\"Setup complete\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dbae08",
   "metadata": {},
   "source": [
    "## Load REBEL Model\n",
    "\n",
    "We'll use the pre-trained REBEL-large model from Hugging Face. This model is trained on multiple RE datasets and can extract over 200 relation types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56977804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize REBEL pipeline\n",
    "print(\"Loading REBEL model...\")\n",
    "device = 0 if torch.cuda.is_available() else -1  # Use GPU if available\n",
    "\n",
    "triplet_extractor = pipeline(\n",
    "    'text2text-generation',\n",
    "    model='Babelscape/rebel-large',\n",
    "    tokenizer='Babelscape/rebel-large',\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"REBEL model loaded\")\n",
    "print(f\"  Device: {'GPU' if device == 0 else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc3139b",
   "metadata": {},
   "source": [
    "## Triplet Extraction Function\n",
    "\n",
    "REBEL outputs triplets in a special format using tokens like `<triplet>`, `<subj>`, and `<obj>`. We need to parse this output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e682d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_triplets(text: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Parse REBEL output to extract triplets.\n",
    "    \n",
    "    REBEL outputs format: <triplet> subject <subj> object <obj> predicate <triplet> ...\n",
    "    \n",
    "    Args:\n",
    "        text: Generated text from REBEL model\n",
    "    \n",
    "    Returns:\n",
    "        List of triplets with 'head', 'type', and 'tail' keys\n",
    "    \"\"\"\n",
    "    triplets = []\n",
    "    relation, subject, object_ = '', '', ''\n",
    "    text = text.strip()\n",
    "    current = 'x'\n",
    "    \n",
    "    # Remove special tokens and process\n",
    "    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n",
    "        if token == \"<triplet>\":\n",
    "            current = 't'\n",
    "            if relation != '':\n",
    "                triplets.append({\n",
    "                    'head': subject.strip(),\n",
    "                    'type': relation.strip(),\n",
    "                    'tail': object_.strip()\n",
    "                })\n",
    "                relation = ''\n",
    "            subject = ''\n",
    "            \n",
    "        elif token == \"<subj>\":\n",
    "            current = 's'\n",
    "            if relation != '':\n",
    "                triplets.append({\n",
    "                    'head': subject.strip(),\n",
    "                    'type': relation.strip(),\n",
    "                    'tail': object_.strip()\n",
    "                })\n",
    "            object_ = ''\n",
    "            \n",
    "        elif token == \"<obj>\":\n",
    "            current = 'o'\n",
    "            relation = ''\n",
    "            \n",
    "        else:\n",
    "            if current == 't':\n",
    "                subject += ' ' + token\n",
    "            elif current == 's':\n",
    "                object_ += ' ' + token\n",
    "            elif current == 'o':\n",
    "                relation += ' ' + token\n",
    "    \n",
    "    # Add last triplet\n",
    "    if subject != '' and relation != '' and object_ != '':\n",
    "        triplets.append({\n",
    "            'head': subject.strip(),\n",
    "            'type': relation.strip(),\n",
    "            'tail': object_.strip()\n",
    "        })\n",
    "    \n",
    "    return triplets\n",
    "\n",
    "\n",
    "print(\"Triplet extraction function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48226b95",
   "metadata": {},
   "source": [
    "## Test REBEL on Example Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7496fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with example text\n",
    "example_text = \"Punta Cana is a resort town in the municipality of HigÃ¼ey, in La Altagracia Province, the easternmost province of the Dominican Republic.\"\n",
    "\n",
    "print(f\"Input text:\\n{example_text}\\n\")\n",
    "\n",
    "# Generate triplets\n",
    "generated_output = triplet_extractor(\n",
    "    example_text,\n",
    "    return_tensors=True,\n",
    "    return_text=False\n",
    ")\n",
    "\n",
    "# Decode and extract triplets\n",
    "extracted_text = triplet_extractor.tokenizer.batch_decode(\n",
    "    [generated_output[0][\"generated_token_ids\"]]\n",
    ")\n",
    "\n",
    "print(f\"Raw REBEL output:\\n{extracted_text[0]}\\n\")\n",
    "\n",
    "# Parse triplets\n",
    "triplets = extract_triplets(extracted_text[0])\n",
    "\n",
    "print(f\"Extracted triplets ({len(triplets)}):\")\n",
    "for i, triplet in enumerate(triplets, 1):\n",
    "    print(f\"  {i}. ({triplet['head']}) --[{triplet['type']}]--> ({triplet['tail']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac135d3f",
   "metadata": {},
   "source": [
    "## Load GutBrainIE Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434a1b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_re_data(file_paths):\n",
    "    \"\"\"Load relation extraction data from multiple JSON files.\"\"\"\n",
    "    all_data = {}\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            all_data.update(data)\n",
    "            print(f\"Loaded {len(data)} documents from {os.path.basename(file_path)}\")\n",
    "        else:\n",
    "            print(f\"Warning: {file_path} not found\")\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "\n",
    "# Load dev data for testing\n",
    "dev_data = load_re_data([\"../data/Annotations/Dev/json_format/dev.json\"])\n",
    "print(f\"\\nTotal dev documents: {len(dev_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a082b96e",
   "metadata": {},
   "source": [
    "## Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86aaee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data from three quality levels\n",
    "train_files = [\n",
    "    \"../data/Annotations/Train/gold_quality/json_format/train_gold.json\",\n",
    "    \"../data/Annotations/Train/platinum_quality/json_format/train_platinum.json\",\n",
    "    \"../data/Annotations/Train/silver_quality/json_format/train_silver.json\"\n",
    "]\n",
    "\n",
    "train_data = load_re_data(train_files)\n",
    "print(f\"\\nTotal training documents: {len(train_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc5aaf6",
   "metadata": {},
   "source": [
    "## Prepare Training Data in REBEL Format\n",
    "\n",
    "REBEL expects data in a specific linearized format:\n",
    "- Input: Raw text\n",
    "- Output: `<triplet> subject <subj> object <obj> predicate <triplet> ...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d200497",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_full_text(title, abstract):\n",
    "    \"\"\"Create full text from title and abstract.\"\"\"\n",
    "    return f\"{title} {abstract}\"\n",
    "\n",
    "def format_triplets_for_rebel(relations):\n",
    "    \"\"\"\n",
    "    Convert relations to REBEL format string.\n",
    "    \n",
    "    Args:\n",
    "        relations: List of relation dictionaries\n",
    "    \n",
    "    Returns:\n",
    "        Formatted string in REBEL format\n",
    "    \"\"\"\n",
    "    if not relations:\n",
    "        return \"\"\n",
    "    \n",
    "    # Group relations by subject\n",
    "    relations_by_subject = {}\n",
    "    for rel in relations:\n",
    "        key = (rel['subject_text_span'], rel['subject_label'])\n",
    "        if key not in relations_by_subject:\n",
    "            relations_by_subject[key] = []\n",
    "        relations_by_subject[key].append(rel)\n",
    "    \n",
    "    # Build REBEL format string\n",
    "    triplet_strings = []\n",
    "    for (subject_text, subject_label), rels in relations_by_subject.items():\n",
    "        # Start with subject\n",
    "        parts = [subject_text]\n",
    "        \n",
    "        # Add all objects and predicates for this subject\n",
    "        for rel in rels:\n",
    "            parts.append('<subj>')\n",
    "            parts.append(rel['object_text_span'])\n",
    "            parts.append('<obj>')\n",
    "            parts.append(rel['predicate'])\n",
    "        \n",
    "        triplet_strings.append(' '.join(parts))\n",
    "    \n",
    "    return '<triplet> ' + ' <triplet> '.join(triplet_strings)\n",
    "\n",
    "\n",
    "def prepare_training_data(data):\n",
    "    \"\"\"\n",
    "    Prepare training examples in REBEL format.\n",
    "    \n",
    "    Returns:\n",
    "        List of dicts with 'input' and 'output' keys\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    \n",
    "    for pmid, article in tqdm(data.items(), desc=\"Preparing training data\"):\n",
    "        title = article['metadata']['title']\n",
    "        abstract = article['metadata']['abstract']\n",
    "        full_text = create_full_text(title, abstract)\n",
    "        relations = article['relations']\n",
    "        \n",
    "        # Skip documents without relations\n",
    "        if not relations:\n",
    "            continue\n",
    "        \n",
    "        # Format triplets\n",
    "        triplet_string = format_triplets_for_rebel(relations)\n",
    "        \n",
    "        examples.append({\n",
    "            'input': full_text,\n",
    "            'output': triplet_string,\n",
    "            'pmid': pmid\n",
    "        })\n",
    "    \n",
    "    return examples\n",
    "\n",
    "\n",
    "print(\"Training data preparation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792ce7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training and validation examples\n",
    "train_examples = prepare_training_data(train_data)\n",
    "dev_examples = prepare_training_data(dev_data)\n",
    "\n",
    "print(f\"\\nTraining examples: {len(train_examples)}\")\n",
    "print(f\"Validation examples: {len(dev_examples)}\")\n",
    "\n",
    "# Show example\n",
    "if train_examples:\n",
    "    example = train_examples[0]\n",
    "    print(f\"\\nExample training instance:\")\n",
    "    print(f\"  Input: {example['input'][:150]}...\")\n",
    "    print(f\"  Output: {example['output'][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e790191d",
   "metadata": {},
   "source": [
    "## Create Dataset for Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054e69a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "\n",
    "\n",
    "class REBELDataset(Dataset):\n",
    "    \"\"\"Dataset for REBEL fine-tuning.\"\"\"\n",
    "    \n",
    "    def __init__(self, examples, tokenizer, max_input_length=512, max_target_length=256):\n",
    "        self.examples = examples\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_target_length = max_target_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        example = self.examples[idx]\n",
    "        \n",
    "        # Tokenize input\n",
    "        model_inputs = self.tokenizer(\n",
    "            example['input'],\n",
    "            max_length=self.max_input_length,\n",
    "            truncation=True,\n",
    "            padding=False  # Padding will be done by collator\n",
    "        )\n",
    "        \n",
    "        # Tokenize target\n",
    "        labels = self.tokenizer(\n",
    "            text_target=example['output'],\n",
    "            max_length=self.max_target_length,\n",
    "            truncation=True,\n",
    "            padding=False\n",
    "        )\n",
    "        \n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        \n",
    "        return model_inputs\n",
    "\n",
    "\n",
    "# Initialize tokenizer for training\n",
    "tokenizer = AutoTokenizer.from_pretrained('Babelscape/rebel-large')\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = REBELDataset(train_examples, tokenizer)\n",
    "val_dataset = REBELDataset(dev_examples, tokenizer)\n",
    "\n",
    "print(f\"Training dataset: {len(train_dataset)} examples\")\n",
    "print(f\"Validation dataset: {len(val_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552fcf19",
   "metadata": {},
   "source": [
    "## Fine-tune REBEL Model\n",
    "\n",
    "We'll fine-tune the pre-trained REBEL model on GutBrainIE data to adapt it to the biomedical domain and our specific relation types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c541347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model for fine-tuning\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('Babelscape/rebel-large')\n",
    "\n",
    "# Training configuration\n",
    "output_dir = \"models/rebel_finetuned\"\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,  # Effective batch size = 16\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=256,\n",
    "    logging_steps=50,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "# Data collator for dynamic padding\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "print(\"Model and training configuration ready\")\n",
    "print(f\"  Output directory: {output_dir}\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb96498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized\")\n",
    "print(f\"  Training samples: {len(train_dataset)}\")\n",
    "print(f\"  Validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe02390",
   "metadata": {},
   "source": [
    "## Start Training\n",
    "\n",
    "**Note:** This will take considerable time depending on hardware (several hours on CPU, ~30-60 minutes on GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c81c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"=\"*60)\n",
    "print(\"Starting REBEL fine-tuning...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import time\n",
    "training_start_time = time.time()\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "training_duration = time.time() - training_start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETED!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training time: {training_duration/60:.2f} minutes\")\n",
    "print(f\"Final train loss: {train_result.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d8923c",
   "metadata": {},
   "source": [
    "## Save Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e15eedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "print(\"Saving fine-tuned model...\")\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"Model saved to: {output_dir}\")\n",
    "print(\"\\nYou can now use this fine-tuned model by loading:\")\n",
    "print(f\"  model_path = '{output_dir}'\")\n",
    "print(f\"  triplet_extractor = pipeline('text2text-generation', model=model_path, tokenizer=model_path)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27334faa",
   "metadata": {},
   "source": [
    "## Load Fine-tuned Model for Inference\n",
    "\n",
    "Now we'll use the fine-tuned model instead of the pre-trained one for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9374c6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model for inference\n",
    "print(\"Loading fine-tuned REBEL model for inference...\")\n",
    "\n",
    "# Use fine-tuned model if it exists, otherwise use pre-trained\n",
    "model_path = output_dir if os.path.exists(output_dir) else 'Babelscape/rebel-large'\n",
    "\n",
    "triplet_extractor = pipeline(\n",
    "    'text2text-generation',\n",
    "    model=model_path,\n",
    "    tokenizer=model_path,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"Model loaded from: {model_path}\")\n",
    "print(f\"  Device: {'GPU' if device == 0 else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323a2e87",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Inference with Fine-tuned Model\n",
    "\n",
    "The sections below will use the fine-tuned model for predictions on the dev set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22f2538",
   "metadata": {},
   "source": [
    "## Prediction Function with Entity Matching\n",
    "\n",
    "REBEL extracts entities and relations from raw text. We need to match them back to the entities in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253531c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_entity_to_annotations(entity_text, entities_list, full_text):\n",
    "    \"\"\"\n",
    "    Match an extracted entity text to annotated entities.\n",
    "    \n",
    "    Args:\n",
    "        entity_text: Text span extracted by REBEL\n",
    "        entities_list: List of annotated entities from dataset\n",
    "        full_text: Full document text\n",
    "    \n",
    "    Returns:\n",
    "        Matched entity dict or None\n",
    "    \"\"\"\n",
    "    entity_text_lower = entity_text.lower().strip()\n",
    "    \n",
    "    # Try exact match first\n",
    "    for entity in entities_list:\n",
    "        if entity['text_span'].lower().strip() == entity_text_lower:\n",
    "            return entity\n",
    "    \n",
    "    # Try substring match\n",
    "    for entity in entities_list:\n",
    "        entity_span_lower = entity['text_span'].lower().strip()\n",
    "        if entity_text_lower in entity_span_lower or entity_span_lower in entity_text_lower:\n",
    "            return entity\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def predict_relations_rebel(pmid, article, triplet_extractor, max_length=512):\n",
    "    \"\"\"\n",
    "    Predict relations using REBEL model.\n",
    "    \n",
    "    Args:\n",
    "        pmid: Document ID\n",
    "        article: Article data with metadata and entities\n",
    "        triplet_extractor: REBEL pipeline\n",
    "        max_length: Maximum input length for generation\n",
    "    \n",
    "    Returns:\n",
    "        List of predicted relations\n",
    "    \"\"\"\n",
    "    title = article['metadata']['title']\n",
    "    abstract = article['metadata']['abstract']\n",
    "    full_text = create_full_text(title, abstract)\n",
    "    entities = article['entities']\n",
    "    \n",
    "    # Adjust entity positions for title+abstract concatenation\n",
    "    abstract_offset = len(title) + 1\n",
    "    adjusted_entities = []\n",
    "    for entity in entities:\n",
    "        if entity['location'] == 'abstract':\n",
    "            adjusted_entities.append({\n",
    "                'start_idx': entity['start_idx'],\n",
    "                'end_idx': entity['end_idx'],\n",
    "                'text_span': entity['text_span'],\n",
    "                'label': entity['label'],\n",
    "                'location': entity['location']\n",
    "            })\n",
    "        else:\n",
    "            adjusted_entities.append(entity)\n",
    "    \n",
    "    # Generate triplets using REBEL\n",
    "    try:\n",
    "        generated_output = triplet_extractor(\n",
    "            full_text,\n",
    "            return_tensors=True,\n",
    "            return_text=False,\n",
    "            max_length=max_length\n",
    "        )\n",
    "        \n",
    "        extracted_text = triplet_extractor.tokenizer.batch_decode(\n",
    "            [generated_output[0][\"generated_token_ids\"]]\n",
    "        )\n",
    "        \n",
    "        triplets = extract_triplets(extracted_text[0])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pmid}: {e}\")\n",
    "        return []\n",
    "    \n",
    "    # Match triplets to annotated entities\n",
    "    predicted_relations = []\n",
    "    \n",
    "    for triplet in triplets:\n",
    "        # Find matching entities\n",
    "        subject_entity = match_entity_to_annotations(triplet['head'], entities, full_text)\n",
    "        object_entity = match_entity_to_annotations(triplet['tail'], entities, full_text)\n",
    "        \n",
    "        # Only add if both entities matched\n",
    "        if subject_entity and object_entity:\n",
    "            predicted_relations.append({\n",
    "                \"subject_start_idx\": subject_entity['start_idx'],\n",
    "                \"subject_end_idx\": subject_entity['end_idx'],\n",
    "                \"subject_location\": subject_entity['location'],\n",
    "                \"subject_text_span\": subject_entity['text_span'],\n",
    "                \"subject_label\": subject_entity['label'],\n",
    "                \"predicate\": triplet['type'],\n",
    "                \"object_start_idx\": object_entity['start_idx'],\n",
    "                \"object_end_idx\": object_entity['end_idx'],\n",
    "                \"object_location\": object_entity['location'],\n",
    "                \"object_text_span\": object_entity['text_span'],\n",
    "                \"object_label\": object_entity['label']\n",
    "            })\n",
    "    \n",
    "    return predicted_relations\n",
    "\n",
    "\n",
    "print(\"Prediction functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a1953b",
   "metadata": {},
   "source": [
    "## Test on Single Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ec1039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on first document\n",
    "test_pmid = list(dev_data.keys())[0]\n",
    "test_article = dev_data[test_pmid]\n",
    "\n",
    "print(f\"Testing on document: {test_pmid}\")\n",
    "print(f\"Title: {test_article['metadata']['title'][:100]}...\")\n",
    "print(f\"\\nNumber of annotated entities: {len(test_article['entities'])}\")\n",
    "print(f\"Number of gold relations: {len(test_article['relations'])}\")\n",
    "\n",
    "# Predict relations\n",
    "predicted_relations = predict_relations_rebel(test_pmid, test_article, triplet_extractor)\n",
    "\n",
    "print(f\"\\nPredicted {len(predicted_relations)} relations\")\n",
    "print(\"\\nSample predicted relations:\")\n",
    "for relation in predicted_relations[:5]:\n",
    "    print(f\"  ({relation['subject_text_span']} [{relation['subject_label']}]) \"\n",
    "          f\"--[{relation['predicate']}]--> \"\n",
    "          f\"({relation['object_text_span']} [{relation['object_label']}])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b05e62",
   "metadata": {},
   "source": [
    "## Run Predictions on Full Dev Set\n",
    "\n",
    "**Note:** This may take some time depending on dataset size and hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b367b98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on full dev set\n",
    "print(\"Running REBEL inference on dev set...\")\n",
    "\n",
    "predictions = {}\n",
    "\n",
    "for pmid, article in tqdm(dev_data.items(), desc=\"Predicting relations\"):\n",
    "    predicted_relations = predict_relations_rebel(pmid, article, triplet_extractor)\n",
    "    predictions[pmid] = {\"relations\": predicted_relations}\n",
    "\n",
    "total_relations = sum(len(p['relations']) for p in predictions.values())\n",
    "print(f\"\\nInference completed: {len(predictions)} documents\")\n",
    "print(f\"  Total relations predicted: {total_relations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64f650e",
   "metadata": {},
   "source": [
    "## Save Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d674849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions to file\n",
    "# Use different filename depending on whether model was fine-tuned\n",
    "model_type = \"finetuned\" if os.path.exists(output_dir) else \"pretrained\"\n",
    "output_path = f\"predictions/rebel_{model_type}_predictions.json\"\n",
    "\n",
    "os.makedirs(\"predictions\", exist_ok=True)\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(predictions, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Predictions saved to {output_path}\")\n",
    "print(f\"  Model type: {model_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045d119d",
   "metadata": {},
   "source": [
    "## Analyze Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62530d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example predictions\n",
    "print(\"Example Predictions:\\n\")\n",
    "\n",
    "sample_pmids = list(dev_data.keys())[:3]\n",
    "\n",
    "for pmid in sample_pmids:\n",
    "    article = dev_data[pmid]\n",
    "    pred = predictions[pmid]\n",
    "    \n",
    "    print(f\"Document PMID: {pmid}\")\n",
    "    print(f\"Title: {article['metadata']['title'][:80]}...\")\n",
    "    print(f\"\\nNumber of entities: {len(article['entities'])}\")\n",
    "    print(f\"Number of gold relations: {len(article['relations'])}\")\n",
    "    print(f\"Number of predicted relations: {len(pred['relations'])}\")\n",
    "    \n",
    "    # Show predicted relations\n",
    "    print(\"\\nPredicted relations:\")\n",
    "    for relation in pred['relations'][:5]:\n",
    "        print(f\"  ({relation['subject_text_span']} [{relation['subject_label']}]) \"\n",
    "              f\"--[{relation['predicate']}]--> \"\n",
    "              f\"({relation['object_text_span']} [{relation['object_label']}])\")\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ca477d",
   "metadata": {},
   "source": [
    "## Relation Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f7f4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count relations by predicate in predictions\n",
    "pred_predicate_counts = Counter()\n",
    "for pmid, pred in predictions.items():\n",
    "    for relation in pred['relations']:\n",
    "        pred_predicate_counts[relation['predicate']] += 1\n",
    "\n",
    "# Count relations by predicate in gold standard\n",
    "gold_predicate_counts = Counter()\n",
    "for pmid, article in dev_data.items():\n",
    "    for relation in article['relations']:\n",
    "        gold_predicate_counts[relation['predicate']] += 1\n",
    "\n",
    "print(\"Relation Distribution by Predicate:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Predicate':<40} {'Gold':<10} {'Predicted':<10}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "all_predicates = sorted(set(gold_predicate_counts.keys()) | set(pred_predicate_counts.keys()))\n",
    "for predicate in all_predicates:\n",
    "    print(f\"{predicate:<40} {gold_predicate_counts.get(predicate, 0):<10} \"\n",
    "          f\"{pred_predicate_counts.get(predicate, 0):<10}\")\n",
    "\n",
    "print(\"-\"*70)\n",
    "print(f\"{'TOTAL':<40} {sum(gold_predicate_counts.values()):<10} \"\n",
    "      f\"{sum(pred_predicate_counts.values()):<10}\")\n",
    "\n",
    "print(f\"\\nREBEL predicted {len(pred_predicate_counts)} unique relation types\")\n",
    "print(f\"Dataset contains {len(gold_predicate_counts)} unique relation types\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

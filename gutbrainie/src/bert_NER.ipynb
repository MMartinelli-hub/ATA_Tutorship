{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2825ddc",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd016cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForTokenClassification, \n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Setup complete\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838e9da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define entity labels\n",
    "ENTITY_LABELS = [\n",
    "    \"anatomical location\",\n",
    "    \"animal\",\n",
    "    \"bacteria\",\n",
    "    \"biomedical technique\",\n",
    "    \"chemical\",\n",
    "    \"DDF\",\n",
    "    \"dietary supplement\",\n",
    "    \"drug\",\n",
    "    \"food\",\n",
    "    \"gene\",\n",
    "    \"human\",\n",
    "    \"microbiome\",\n",
    "    \"statistical technique\"\n",
    "]\n",
    "\n",
    "# Create BIO tags for each entity label\n",
    "label_list = ['O']  # Outside\n",
    "for entity_label in ENTITY_LABELS:\n",
    "    label_list.append(f'B-{entity_label}')  # Beginning\n",
    "    label_list.append(f'I-{entity_label}')  # Inside\n",
    "\n",
    "label2id = {k: v for v, k in enumerate(label_list)}\n",
    "id2label = {v: k for v, k in enumerate(label_list)}\n",
    "\n",
    "print(f\"Total labels: {len(label_list)}\")\n",
    "print(f\"\\nFirst 10 labels: {label_list[:10]}\")\n",
    "\n",
    "# Model configuration\n",
    "model_name = \"dmis-lab/biobert-v1.1\"  # BioBERT for biomedical text\n",
    "output_model_dir = \"models/bert_biomedbert_ner\"\n",
    "\n",
    "print(f\"\\nModel: {model_name}\")\n",
    "print(f\"Output directory: {output_model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f4055e",
   "metadata": {},
   "source": [
    "## Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34f72f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ner_data(file_paths):\n",
    "    \"\"\"\n",
    "    Load NER data from multiple JSON files.\n",
    "    Each file contains documents with entities.\n",
    "    \"\"\"\n",
    "    all_data = {}\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            all_data.update(data)\n",
    "            print(f\"Loaded {len(data)} documents from {os.path.basename(file_path)}\")\n",
    "        else:\n",
    "            print(f\"Warning: {file_path} not found\")\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "\n",
    "def prepare_documents_for_ner(data):\n",
    "    \"\"\"\n",
    "    Convert raw data into structured format for NER.\n",
    "    Each document has title and abstract as separate text segments.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    for pmid, article in data.items():\n",
    "        # Process title\n",
    "        title_text = article['metadata']['title']\n",
    "        title_entities = [e for e in article['entities'] if e['location'] == 'title']\n",
    "        \n",
    "        documents.append({\n",
    "            'pmid': pmid,\n",
    "            'location': 'title',\n",
    "            'text': title_text,\n",
    "            'entities': title_entities\n",
    "        })\n",
    "        \n",
    "        # Process abstract\n",
    "        abstract_text = article['metadata']['abstract']\n",
    "        abstract_entities = [e for e in article['entities'] if e['location'] == 'abstract']\n",
    "        \n",
    "        documents.append({\n",
    "            'pmid': pmid,\n",
    "            'location': 'abstract',\n",
    "            'text': abstract_text,\n",
    "            'entities': abstract_entities\n",
    "        })\n",
    "    \n",
    "    return documents\n",
    "\n",
    "\n",
    "print(\"✓ Data loading functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0efb8a",
   "metadata": {},
   "source": [
    "## Load Training and Dev Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54209f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data from three quality levels\n",
    "train_files = [\n",
    "    \"../data/Annotations/Train/gold_quality/json_format/train_gold.json\",\n",
    "    \"../data/Annotations/Train/platinum_quality/json_format/train_platinum.json\",\n",
    "    \"../data/Annotations/Train/silver_quality/json_format/train_silver.json\"\n",
    "]\n",
    "\n",
    "train_data = load_ner_data(train_files)\n",
    "train_documents = prepare_documents_for_ner(train_data)\n",
    "\n",
    "print(f\"\\nTotal training documents: {len(train_documents)}\")\n",
    "print(f\"Total training text segments: {len(train_documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3862568e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dev data\n",
    "dev_data = load_ner_data([\"../data/Annotations/Dev/json_format/dev.json\"])\n",
    "dev_documents = prepare_documents_for_ner(dev_data)\n",
    "\n",
    "print(f\"Total dev documents: {len(dev_documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a837b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example document\n",
    "example_doc = train_documents[10]\n",
    "print(f\"Example document:\")\n",
    "print(f\"  PMID: {example_doc['pmid']}\")\n",
    "print(f\"  Location: {example_doc['location']}\")\n",
    "print(f\"  Text: {example_doc['text'][:200]}...\")\n",
    "print(f\"  Number of entities: {len(example_doc['entities'])}\")\n",
    "print(f\"\\nFirst 3 entities:\")\n",
    "for entity in example_doc['entities'][:3]:\n",
    "    print(f\"    - '{entity['text_span']}' [{entity['label']}] @ {entity['start_idx']}-{entity['end_idx']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47c5d35",
   "metadata": {},
   "source": [
    "## Initialize BERT Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255779aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer and model\n",
    "print(\"Initializing BERT tokenizer and model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=len(label_list), \n",
    "    id2label=id2label, \n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "print(f\"✓ Tokenizer loaded: {tokenizer.__class__.__name__}\")\n",
    "print(f\"✓ Model loaded with {model.num_labels} labels\")\n",
    "print(f\"  Vocabulary size: {tokenizer.vocab_size}\")\n",
    "\n",
    "# Test tokenization\n",
    "sample_text = \"The gut microbiome plays a role in Parkinson's disease.\"\n",
    "tokens = tokenizer.tokenize(sample_text)\n",
    "print(f\"\\nSample tokenization: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa6cc27",
   "metadata": {},
   "source": [
    "## BIO Tag Generation for Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb31a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(text, entities, tokenizer, label2id):\n",
    "    \"\"\"\n",
    "    Create BIO tags for tokenized text based on character-level entity annotations.\n",
    "    Uses offset mapping to align character positions with token positions.\n",
    "    \"\"\"\n",
    "    # Tokenize and get offset mapping\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        return_offsets_mapping=True,\n",
    "        add_special_tokens=True,\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoding['input_ids'])\n",
    "    offset_mapping = encoding['offset_mapping']\n",
    "    \n",
    "    # Initialize all labels as 'O' (outside)\n",
    "    labels = ['O'] * len(tokens)\n",
    "    \n",
    "    # Sort entities by start position to handle overlaps\n",
    "    sorted_entities = sorted(entities, key=lambda e: (e['start_idx'], -(e['end_idx'] - e['start_idx'])))\n",
    "    \n",
    "    # Track which tokens have been labeled (to handle overlaps)\n",
    "    labeled_positions = set()\n",
    "    \n",
    "    for entity in sorted_entities:\n",
    "        entity_start = entity['start_idx']\n",
    "        entity_end = entity['end_idx']\n",
    "        entity_label = entity['label']\n",
    "        \n",
    "        # Find tokens that overlap with this entity\n",
    "        entity_token_start = None\n",
    "        entity_token_end = None\n",
    "        \n",
    "        for idx, (token_start, token_end) in enumerate(offset_mapping):\n",
    "            # Skip special tokens\n",
    "            if token_start == 0 and token_end == 0:\n",
    "                continue\n",
    "            \n",
    "            # Check if token overlaps with entity\n",
    "            if token_start < entity_end and token_end > entity_start:\n",
    "                if entity_token_start is None:\n",
    "                    entity_token_start = idx\n",
    "                entity_token_end = idx\n",
    "        \n",
    "        # Apply BIO tagging\n",
    "        if entity_token_start is not None and entity_token_end is not None:\n",
    "            for idx in range(entity_token_start, entity_token_end + 1):\n",
    "                # Only label if not already labeled (handle overlaps)\n",
    "                if idx not in labeled_positions:\n",
    "                    if idx == entity_token_start:\n",
    "                        labels[idx] = f'B-{entity_label}'\n",
    "                    else:\n",
    "                        labels[idx] = f'I-{entity_label}'\n",
    "                    labeled_positions.add(idx)\n",
    "    \n",
    "    # Convert labels to IDs\n",
    "    label_ids = [label2id.get(label, label2id['O']) for label in labels]\n",
    "    \n",
    "    return {\n",
    "        'input_ids': encoding['input_ids'],\n",
    "        'attention_mask': encoding['attention_mask'],\n",
    "        'labels': label_ids,\n",
    "        'tokens': tokens\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"✓ BIO tag generation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ff907f",
   "metadata": {},
   "source": [
    "## Process Training and Dev Data with BIO Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a27e56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process training data\n",
    "print(\"Processing training data...\")\n",
    "processed_train = []\n",
    "\n",
    "for i, doc in enumerate(tqdm(train_documents, desc=\"Processing train\")):\n",
    "    processed = align_labels_with_tokens(\n",
    "        doc['text'],\n",
    "        doc['entities'],\n",
    "        tokenizer,\n",
    "        label2id\n",
    "    )\n",
    "    processed['pmid'] = doc['pmid']\n",
    "    processed['location'] = doc['location']\n",
    "    processed['text'] = doc['text']\n",
    "    processed['entities'] = doc['entities']\n",
    "    processed_train.append(processed)\n",
    "\n",
    "print(f\"✓ Training data processed: {len(processed_train)} segments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91076ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process dev data\n",
    "print(\"Processing dev data...\")\n",
    "processed_dev = []\n",
    "\n",
    "for i, doc in enumerate(tqdm(dev_documents, desc=\"Processing dev\")):\n",
    "    processed = align_labels_with_tokens(\n",
    "        doc['text'],\n",
    "        doc['entities'],\n",
    "        tokenizer,\n",
    "        label2id\n",
    "    )\n",
    "    processed['pmid'] = doc['pmid']\n",
    "    processed['location'] = doc['location']\n",
    "    processed['text'] = doc['text']\n",
    "    processed['entities'] = doc['entities']\n",
    "    processed_dev.append(processed)\n",
    "\n",
    "print(f\"✓ Dev data processed: {len(processed_dev)} segments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185f08b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example with BIO tags\n",
    "example_idx = 10\n",
    "example = processed_train[example_idx]\n",
    "\n",
    "print(f\"Example from training data:\")\n",
    "print(f\"  Text: {example['text'][:150]}...\")\n",
    "print(f\"  Entities: {len(example['entities'])}\")\n",
    "print(f\"\\nToken-Label pairs (first 30):\")\n",
    "\n",
    "token_label_pairs = []\n",
    "for token, label_id in zip(example['tokens'][:30], example['labels'][:30]):\n",
    "    label = id2label[label_id]\n",
    "    token_label_pairs.append((token, label))\n",
    "\n",
    "df = pd.DataFrame(token_label_pairs, columns=['Token', 'Label'])\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f6d1c0",
   "metadata": {},
   "source": [
    "## Prepare Dataset for BERT Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c7fcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset(Dataset):\n",
    "    \"\"\"Custom dataset for NER token classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, processed_data, max_length=512):\n",
    "        self.data = processed_data\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Pad or truncate to max_length\n",
    "        input_ids = item['input_ids'][:self.max_length]\n",
    "        attention_mask = item['attention_mask'][:self.max_length]\n",
    "        labels = item['labels'][:self.max_length]\n",
    "        \n",
    "        # Pad if necessary\n",
    "        padding_length = self.max_length - len(input_ids)\n",
    "        if padding_length > 0:\n",
    "            input_ids = input_ids + [tokenizer.pad_token_id] * padding_length\n",
    "            attention_mask = attention_mask + [0] * padding_length\n",
    "            labels = labels + [-100] * padding_length  # -100 is ignored by loss\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "            'labels': torch.tensor(labels, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"✓ Custom dataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2272964b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "print(\"Creating training datasets...\")\n",
    "\n",
    "train_dataset = NERDataset(processed_train)\n",
    "dev_dataset = NERDataset(processed_dev)\n",
    "\n",
    "print(f\"✓ Training dataset: {len(train_dataset)} examples\")\n",
    "print(f\"✓ Dev dataset: {len(dev_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d7b7e1",
   "metadata": {},
   "source": [
    "## Configure Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9c206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data collator for token classification\n",
    "data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "print(\"✓ Data collator initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922dff7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_model_dir,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    "    logging_steps=100,\n",
    "    save_total_limit=2,\n",
    "    seed=42,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "print(\"✓ Training configuration ready\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b26073",
   "metadata": {},
   "source": [
    "## Train BERT Model\n",
    "\n",
    "Note: This cell might take several minutes to hours depending on dataset size and hardware.\n",
    "\n",
    "**Additional configurations to test:**\n",
    "- Change hyperparameters (*learning_rate*, *batch_size*, *num_train_epochs*, *weight_decay*)\n",
    "- Try different pre-trained models (e.g., \"allenai/scibert_scivocab_uncased\", \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\")\n",
    "- Experiment with max_length for longer contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19acee79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Trainer\n",
    "print(\"Initializing Trainer...\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "print(\"✓ Trainer initialized\")\n",
    "print(f\"  Training samples: {len(train_dataset)}\")\n",
    "print(f\"  Evaluation samples: {len(dev_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ce3d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"=\"*60)\n",
    "print(\"Starting model training...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import time\n",
    "training_start_time = time.time()\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "training_duration = time.time() - training_start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ TRAINING COMPLETED!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training time: {training_duration/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff48a56c",
   "metadata": {},
   "source": [
    "## Save Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771ad746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "print(\"Saving trained model...\")\n",
    "\n",
    "os.makedirs(output_model_dir, exist_ok=True)\n",
    "trainer.save_model(output_model_dir)\n",
    "tokenizer.save_pretrained(output_model_dir)\n",
    "\n",
    "print(f\"✓ Model saved to: {output_model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e837e124",
   "metadata": {},
   "source": [
    "## Load Model for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476e73fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model for inference\n",
    "print(\"Loading trained model for inference...\")\n",
    "\n",
    "inference_model = AutoModelForTokenClassification.from_pretrained(output_model_dir)\n",
    "inference_tokenizer = AutoTokenizer.from_pretrained(output_model_dir)\n",
    "inference_model.eval()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    inference_model = inference_model.cuda()\n",
    "\n",
    "print(f\"✓ Model loaded from: {output_model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67656f4",
   "metadata": {},
   "source": [
    "## Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6907ee38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_entities(model, tokenizer, text, id2label):\n",
    "    \"\"\"\n",
    "    Perform NER inference on a single text.\n",
    "    Returns list of entities with their positions and labels.\n",
    "    \"\"\"\n",
    "    # Tokenize\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_offsets_mapping=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    offset_mapping = encoding.pop('offset_mapping')[0].numpy()\n",
    "    \n",
    "    # Move to GPU if available\n",
    "    if torch.cuda.is_available():\n",
    "        encoding = {k: v.cuda() for k, v in encoding.items()}\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoding)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)[0].cpu().numpy()\n",
    "    \n",
    "    # Convert predictions to labels\n",
    "    predicted_labels = [id2label[pred] for pred in predictions]\n",
    "    \n",
    "    # Extract entities from BIO tags\n",
    "    entities = []\n",
    "    current_entity = None\n",
    "    \n",
    "    for idx, (label, (start_char, end_char)) in enumerate(zip(predicted_labels, offset_mapping)):\n",
    "        # Skip special tokens\n",
    "        if start_char == 0 and end_char == 0:\n",
    "            continue\n",
    "        \n",
    "        if label.startswith('B-'):\n",
    "            # Save previous entity if exists\n",
    "            if current_entity:\n",
    "                entities.append(current_entity)\n",
    "            \n",
    "            # Start new entity\n",
    "            entity_label = label[2:]  # Remove 'B-' prefix\n",
    "            current_entity = {\n",
    "                'start_idx': start_char,\n",
    "                'end_idx': end_char,\n",
    "                'label': entity_label,\n",
    "                'text_span': text[start_char:end_char]\n",
    "            }\n",
    "        \n",
    "        elif label.startswith('I-') and current_entity:\n",
    "            # Extend current entity\n",
    "            entity_label = label[2:]  # Remove 'I-' prefix\n",
    "            if entity_label == current_entity['label']:\n",
    "                current_entity['end_idx'] = end_char\n",
    "                current_entity['text_span'] = text[current_entity['start_idx']:end_char]\n",
    "        \n",
    "        else:\n",
    "            # Outside or label mismatch - save current entity\n",
    "            if current_entity:\n",
    "                entities.append(current_entity)\n",
    "                current_entity = None\n",
    "    \n",
    "    # Save last entity if exists\n",
    "    if current_entity:\n",
    "        entities.append(current_entity)\n",
    "    \n",
    "    return entities\n",
    "\n",
    "\n",
    "print(\"✓ Inference function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e20cf9e",
   "metadata": {},
   "source": [
    "## Predict on Dev Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc10ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on all dev data\n",
    "print(\"Running inference on dev set...\")\n",
    "\n",
    "# Group predictions by document\n",
    "predictions = {}\n",
    "\n",
    "for doc in tqdm(dev_documents, desc=\"Predicting\"):\n",
    "    pmid = doc['pmid']\n",
    "    location = doc['location']\n",
    "    text = doc['text']\n",
    "    \n",
    "    # Predict entities\n",
    "    predicted_entities = predict_entities(\n",
    "        inference_model,\n",
    "        inference_tokenizer,\n",
    "        text,\n",
    "        id2label\n",
    "    )\n",
    "    \n",
    "    # Add location to each entity\n",
    "    for entity in predicted_entities:\n",
    "        entity['location'] = location\n",
    "    \n",
    "    # Initialize document if not exists\n",
    "    if pmid not in predictions:\n",
    "        predictions[pmid] = {'entities': []}\n",
    "    \n",
    "    # Add entities to document\n",
    "    predictions[pmid]['entities'].extend(predicted_entities)\n",
    "\n",
    "print(f\"✓ Inference completed: {len(predictions)} documents\")\n",
    "total_entities = sum(len(p['entities']) for p in predictions.values())\n",
    "print(f\"  Total entities predicted: {total_entities}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec194f9",
   "metadata": {},
   "source": [
    "## Save Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db0348b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions to file\n",
    "output_path = \"predictions/bert_ner_predictions.json\"\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(predictions, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Predictions saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ef0435",
   "metadata": {},
   "source": [
    "## Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9ee4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation functions from evaluate.py concepts\n",
    "def remove_duplicated_entities(predictions):\n",
    "    \"\"\"Remove duplicated entities from predictions.\"\"\"\n",
    "    removed_count = 0\n",
    "    for pmid in list(predictions.keys()):\n",
    "        seen = set()\n",
    "        deduped = []\n",
    "        for ent in predictions[pmid][\"entities\"]:\n",
    "            key = (ent[\"start_idx\"], ent[\"end_idx\"], ent[\"location\"])\n",
    "            if key not in seen:\n",
    "                seen.add(key)\n",
    "                deduped.append(ent)\n",
    "            else:\n",
    "                removed_count += 1\n",
    "        predictions[pmid][\"entities\"] = deduped\n",
    "    \n",
    "    if removed_count > 0:\n",
    "        print(f\"Removed {removed_count} duplicated entities from predictions\")\n",
    "\n",
    "def remove_overlapping_entities_eval(predictions):\n",
    "    \"\"\"Remove overlapping entities, keeping longest spans.\"\"\"\n",
    "    removed_count = 0\n",
    "\n",
    "    for pmid in list(predictions.keys()):\n",
    "        original_len = len(predictions[pmid]['entities'])\n",
    "        \n",
    "        groups = {'title': [], 'abstract': []}\n",
    "        for ent in predictions[pmid]['entities']:\n",
    "            loc = ent[\"location\"]\n",
    "            groups[loc].append(ent)\n",
    "\n",
    "        keepers = set()\n",
    "        for loc in groups:\n",
    "            group = groups[loc]\n",
    "            group = sorted(group, key=lambda e: e[\"start_idx\"])\n",
    "\n",
    "            clusters = []\n",
    "            cluster = []\n",
    "            current_end = None\n",
    "\n",
    "            for ent in group:\n",
    "                if not cluster:\n",
    "                    cluster = [ent]\n",
    "                    current_end = ent[\"end_idx\"]\n",
    "                else:\n",
    "                    if ent[\"start_idx\"] < current_end:\n",
    "                        cluster.append(ent)\n",
    "                        if ent[\"end_idx\"] > current_end:\n",
    "                            current_end = ent[\"end_idx\"]\n",
    "                    else:\n",
    "                        clusters.append(cluster)\n",
    "                        cluster = [ent]\n",
    "                        current_end = ent[\"end_idx\"]\n",
    "            if cluster:\n",
    "                clusters.append(cluster)\n",
    "\n",
    "            for clust in clusters:\n",
    "                longest = clust[0]\n",
    "                max_len = longest[\"end_idx\"] - longest[\"start_idx\"]\n",
    "                for ent in clust[1:]:\n",
    "                    length = ent[\"end_idx\"] - ent[\"start_idx\"]\n",
    "                    if length > max_len:\n",
    "                        longest = ent\n",
    "                        max_len = length\n",
    "                keepers.add((longest[\"start_idx\"],\n",
    "                             longest[\"end_idx\"],\n",
    "                             longest[\"location\"]))\n",
    "\n",
    "        deduped = []\n",
    "        for ent in predictions[pmid]['entities']:\n",
    "            key = (ent[\"start_idx\"], ent[\"end_idx\"], ent[\"location\"])\n",
    "            if key in keepers:\n",
    "                deduped.append(ent)\n",
    "                keepers.remove(key)\n",
    "\n",
    "        predictions[pmid][\"entities\"] = deduped\n",
    "        removed_count += (original_len - len(deduped))\n",
    "\n",
    "    if removed_count > 0:\n",
    "        print(f\"Removed {removed_count} overlapping entities\")\n",
    "\n",
    "print(\"✓ Evaluation helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3162d54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ner(predictions, ground_truth):\n",
    "    \"\"\"Evaluate NER predictions against ground truth.\"\"\"\n",
    "    # Remove duplicated and overlapping entities\n",
    "    remove_duplicated_entities(predictions)\n",
    "    remove_overlapping_entities_eval(predictions)\n",
    "    \n",
    "    LEGAL_ENTITY_LABELS = [\n",
    "        \"anatomical location\", \"animal\", \"bacteria\", \"biomedical technique\",\n",
    "        \"chemical\", \"DDF\", \"dietary supplement\", \"drug\", \"food\", \"gene\",\n",
    "        \"human\", \"microbiome\", \"statistical technique\"\n",
    "    ]\n",
    "    \n",
    "    ground_truth_NER = dict()\n",
    "    count_annotated_entities_per_label = {}\n",
    "    \n",
    "    for pmid, article in ground_truth.items():\n",
    "        if pmid not in ground_truth_NER:\n",
    "            ground_truth_NER[pmid] = []\n",
    "        for entity in article['entities']:\n",
    "            start_idx = int(entity[\"start_idx\"])\n",
    "            end_idx = int(entity[\"end_idx\"])\n",
    "            location = str(entity[\"location\"])\n",
    "            text_span = str(entity[\"text_span\"])\n",
    "            label = str(entity[\"label\"]) \n",
    "            \n",
    "            entry = (start_idx, end_idx, location, text_span, label)\n",
    "            ground_truth_NER[pmid].append(entry)\n",
    "            \n",
    "            if label not in count_annotated_entities_per_label:\n",
    "                count_annotated_entities_per_label[label] = 0\n",
    "            count_annotated_entities_per_label[label] += 1\n",
    "\n",
    "    count_predicted_entities_per_label = {label: 0 for label in list(count_annotated_entities_per_label.keys())}\n",
    "    count_true_positives_per_label = {label: 0 for label in list(count_annotated_entities_per_label.keys())}\n",
    "\n",
    "    for pmid in predictions.keys():\n",
    "        entities = predictions[pmid]['entities']\n",
    "        \n",
    "        for entity in entities:\n",
    "            start_idx = int(entity[\"start_idx\"])\n",
    "            end_idx = int(entity[\"end_idx\"])\n",
    "            location = str(entity[\"location\"])\n",
    "            text_span = str(entity[\"text_span\"])\n",
    "            label = str(entity[\"label\"]) \n",
    "            \n",
    "            if label not in LEGAL_ENTITY_LABELS:\n",
    "                continue\n",
    "\n",
    "            if label in count_predicted_entities_per_label:\n",
    "                count_predicted_entities_per_label[label] += 1\n",
    "\n",
    "            entry = (start_idx, end_idx, location, text_span, label)\n",
    "            if pmid in ground_truth_NER and entry in ground_truth_NER[pmid]:\n",
    "                count_true_positives_per_label[label] += 1\n",
    "\n",
    "    count_annotated_entities = sum(count_annotated_entities_per_label.values())\n",
    "    count_predicted_entities = sum(count_predicted_entities_per_label.values())\n",
    "    count_true_positives = sum(count_true_positives_per_label.values())\n",
    "\n",
    "    micro_precision = count_true_positives / (count_predicted_entities + 1e-10)\n",
    "    micro_recall = count_true_positives / (count_annotated_entities + 1e-10)\n",
    "    micro_f1 = 2 * ((micro_precision * micro_recall) / (micro_precision + micro_recall + 1e-10))\n",
    "\n",
    "    precision, recall, f1 = 0, 0, 0\n",
    "    n = len(count_annotated_entities_per_label)\n",
    "    for label in count_annotated_entities_per_label.keys():\n",
    "        current_precision = count_true_positives_per_label[label] / (count_predicted_entities_per_label[label] + 1e-10) \n",
    "        current_recall = count_true_positives_per_label[label] / (count_annotated_entities_per_label[label] + 1e-10) \n",
    "        \n",
    "        precision += current_precision\n",
    "        recall += current_recall\n",
    "        f1 += 2 * ((current_precision * current_recall) / (current_precision + current_recall + 1e-10))\n",
    "    \n",
    "    precision = precision / n\n",
    "    recall = recall / n\n",
    "    f1 = f1 / n\n",
    "\n",
    "    return precision, recall, f1, micro_precision, micro_recall, micro_f1\n",
    "\n",
    "\n",
    "# Evaluate\n",
    "precision, recall, f1, micro_precision, micro_recall, micro_f1 = evaluate_ner(predictions, dev_data)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BERT NER BASELINE RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nMacro-averaged Metrics:\")\n",
    "print(f\"  Macro-Precision: {precision:.4f}\")\n",
    "print(f\"  Macro-Recall:    {recall:.4f}\")\n",
    "print(f\"  Macro-F1 Score:  {f1:.4f}\")\n",
    "\n",
    "print(\"\\nMicro-averaged Metrics:\")\n",
    "print(f\"  Micro-Precision: {micro_precision:.4f}\")\n",
    "print(f\"  Micro-Recall:    {micro_recall:.4f}\")\n",
    "print(f\"  Micro-F1 Score:  {micro_f1:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace99515",
   "metadata": {},
   "source": [
    "## Example Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d6bd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example predictions\n",
    "print(\"Example Predictions:\\n\")\n",
    "\n",
    "sample_pmids = list(dev_data.keys())[:5]\n",
    "\n",
    "for pmid in sample_pmids:\n",
    "    article = dev_data[pmid]\n",
    "    pred = predictions[pmid]\n",
    "    \n",
    "    print(f\"Document PMID: {pmid}\")\n",
    "    print(f\"Title: {article['metadata']['title'][:100]}...\")\n",
    "    print(f\"\\nGold entities: {len(article['entities'])}\")\n",
    "    print(f\"Predicted entities: {len(pred['entities'])}\")\n",
    "    \n",
    "    # Show first few predicted entities\n",
    "    print(\"\\nSample predictions:\")\n",
    "    for entity in pred['entities'][:5]:\n",
    "        print(f\"  - '{entity['text_span']}' [{entity['label']}] in {entity['location']}\")\n",
    "    \n",
    "    # Calculate match statistics\n",
    "    gold_set = set()\n",
    "    for entity in article['entities']:\n",
    "        gold_set.add((\n",
    "            entity['start_idx'],\n",
    "            entity['end_idx'],\n",
    "            entity['location'],\n",
    "            entity['text_span'],\n",
    "            entity['label']\n",
    "        ))\n",
    "    \n",
    "    pred_set = set()\n",
    "    for entity in pred['entities']:\n",
    "        pred_set.add((\n",
    "            entity['start_idx'],\n",
    "            entity['end_idx'],\n",
    "            entity['location'],\n",
    "            entity['text_span'],\n",
    "            entity['label']\n",
    "        ))\n",
    "    \n",
    "    correct = len(gold_set & pred_set)\n",
    "    missed = len(gold_set - pred_set)\n",
    "    wrong = len(pred_set - gold_set)\n",
    "    \n",
    "    print(f\"\\n✓ Correct: {correct}\")\n",
    "    print(f\"✗ Missed: {missed}\")\n",
    "    print(f\"✗ Wrong: {wrong}\")\n",
    "    print(\"-\" * 80)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68be8bec",
   "metadata": {},
   "source": [
    "## Analysis: Entity Distribution by Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3fadee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count entities by label in predictions\n",
    "pred_label_counts = Counter()\n",
    "for pmid, pred in predictions.items():\n",
    "    for entity in pred['entities']:\n",
    "        pred_label_counts[entity['label']] += 1\n",
    "\n",
    "# Count entities by label in gold standard\n",
    "gold_label_counts = Counter()\n",
    "for pmid, article in dev_data.items():\n",
    "    for entity in article['entities']:\n",
    "        gold_label_counts[entity['label']] += 1\n",
    "\n",
    "print(\"Entity Distribution by Label:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Label':<25} {'Gold':<10} {'Predicted':<10}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "all_labels = set(gold_label_counts.keys()) | set(pred_label_counts.keys())\n",
    "for label in sorted(all_labels):\n",
    "    print(f\"{label:<25} {gold_label_counts[label]:<10} {pred_label_counts[label]:<10}\")\n",
    "\n",
    "print(\"-\"*60)\n",
    "print(f\"{'TOTAL':<25} {sum(gold_label_counts.values()):<10} {sum(pred_label_counts.values()):<10}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ate-it",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

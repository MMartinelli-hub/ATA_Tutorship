{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d870a3e6",
   "metadata": {},
   "source": [
    "# Vanilla Term Clustering Baseline\n",
    "\n",
    "Simple similarity-based clustering baseline:\n",
    "- Load training data with term-cluster mappings\n",
    "- For each dev term, find the most similar training term\n",
    "- Assign the cluster of the most similar training term\n",
    "- Use simple string similarity metrics (exact match, substring, word overlap)\n",
    "\n",
    "Dataset: EvalITA 2025 ATE-IT (Automatic Term Extraction - Italian Testbed) - Subtask B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5fcddf",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd6f629f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from difflib import SequenceMatcher\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf561e19",
   "metadata": {},
   "source": [
    "## Load Training Data\n",
    "\n",
    "Load the training set and build a mapping of terms to clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3482e48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (713, 2)\n",
      "Number of unique clusters: 299\n",
      "Number of terms: 713\n",
      "\n",
      "Example clusters:\n",
      "  Cluster 0: ['biodegradabili']\n",
      "  Cluster 1: ['cassonetti', 'contenitori stradali', 'cassonetti stradali', 'postazioni stradali']\n",
      "  Cluster 2: ['separare i rifiuti']\n",
      "  Cluster 3: ['deposito', 'conferimento', 'conferimento delle frazioni', 'conferimento dei rifiuti', 'conferimenti', 'operazioni di conferimento']\n",
      "  Cluster 4: ['conferiti', 'conferito', 'conferita']\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../data\"\n",
    "train_path = os.path.join(data_path, \"subtask_b_train.csv\")\n",
    "dev_path = os.path.join(data_path, \"subtask_b_dev.csv\")\n",
    "\n",
    "# Load training data\n",
    "train_df = pd.read_csv(train_path)\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Number of unique clusters: {train_df['cluster'].nunique()}\")\n",
    "print(f\"Number of terms: {len(train_df)}\")\n",
    "\n",
    "# Create term-to-cluster mapping\n",
    "term_to_cluster = dict(zip(train_df['term'].str.lower(), train_df['cluster']))\n",
    "\n",
    "# Create cluster-to-terms mapping for analysis\n",
    "cluster_to_terms = defaultdict(list)\n",
    "for term, cluster in term_to_cluster.items():\n",
    "    cluster_to_terms[cluster].append(term)\n",
    "\n",
    "print(f\"\\nExample clusters:\")\n",
    "for cluster_id in sorted(cluster_to_terms.keys())[:5]:\n",
    "    print(f\"  Cluster {cluster_id}: {cluster_to_terms[cluster_id]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d3799f",
   "metadata": {},
   "source": [
    "## Load Dev Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9af47bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev data shape: (242, 2)\n",
      "Number of unique clusters in dev: 147\n",
      "Number of terms in dev: 242\n",
      "\n",
      "First few dev terms:\n",
      "                             term  cluster\n",
      "0                        deposito        3\n",
      "1                    conferimento        3\n",
      "2                       conferiti        4\n",
      "3                   differenziati        5\n",
      "4       servizio di igiene urbana        7\n",
      "5         olio alimentare esausto       10\n",
      "6                     oli esausti       10\n",
      "7  operatori dell'isola ecologica       11\n",
      "8                       pannolini       12\n",
      "9                       pannoloni       12\n"
     ]
    }
   ],
   "source": [
    "# Load dev data\n",
    "dev_df = pd.read_csv(dev_path)\n",
    "print(f\"Dev data shape: {dev_df.shape}\")\n",
    "print(f\"Number of unique clusters in dev: {dev_df['cluster'].nunique()}\")\n",
    "print(f\"Number of terms in dev: {len(dev_df)}\")\n",
    "\n",
    "print(f\"\\nFirst few dev terms:\")\n",
    "print(dev_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5aafe3",
   "metadata": {},
   "source": [
    "## Similarity Functions\n",
    "\n",
    "Define various similarity metrics for matching terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a014ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_match_similarity(term1, term2):\n",
    "    \"\"\"Returns 1.0 if exact match, 0.0 otherwise.\"\"\"\n",
    "    return 1.0 if term1 == term2 else 0.0\n",
    "\n",
    "def substring_similarity(term1, term2):\n",
    "    \"\"\"Returns 0.9 if one term is substring of the other.\"\"\"\n",
    "    if term1 in term2 or term2 in term1:\n",
    "        return 0.9\n",
    "    return 0.0\n",
    "\n",
    "def word_overlap_similarity(term1, term2):\n",
    "    \"\"\"Calculate Jaccard similarity based on word overlap.\"\"\"\n",
    "    # https://www.sciencedirect.com/topics/computer-science/jaccard-similarity\n",
    "    words1 = set(term1.split())\n",
    "    words2 = set(term2.split())\n",
    "    \n",
    "    if not words1 or not words2:\n",
    "        return 0.0\n",
    "    \n",
    "    intersection = len(words1 & words2)\n",
    "    union = len(words1 | words2)\n",
    "    \n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "def sequence_similarity(term1, term2):\n",
    "    \"\"\"Calculate character-level sequence similarity.\"\"\"\n",
    "    return SequenceMatcher(None, term1, term2).ratio()\n",
    "\n",
    "def combined_similarity(term1, term2, word_overlap_weight=0.6, seq_weight=0.4):\n",
    "    \"\"\"Combine multiple similarity metrics with weights.\"\"\"\n",
    "    exact = exact_match_similarity(term1, term2)\n",
    "    if exact == 1.0:\n",
    "        return 1.0\n",
    "    \n",
    "    substring = substring_similarity(term1, term2)\n",
    "    if substring > 0:\n",
    "        return substring\n",
    "    \n",
    "    word_overlap = word_overlap_similarity(term1, term2)\n",
    "    seq_sim = sequence_similarity(term1, term2)\n",
    "    \n",
    "    # Weighted combination\n",
    "    return word_overlap_weight * word_overlap + seq_weight * seq_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7f787c",
   "metadata": {},
   "source": [
    "## Predict Clusters for Dev Set\n",
    "\n",
    "For each term in the dev set, find the most similar training term and assign its cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3adef3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_cluster(term, term_to_cluster, similarity_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Find the best matching cluster for a term.\n",
    "    Returns the cluster ID and the similarity score.\n",
    "    \"\"\"\n",
    "    term_lower = term.lower()\n",
    "    \n",
    "    # Check for exact match first\n",
    "    if term_lower in term_to_cluster:\n",
    "        return term_to_cluster[term_lower], 1.0\n",
    "    \n",
    "    # Find most similar term\n",
    "    best_similarity = 0.0\n",
    "    best_cluster = None\n",
    "    \n",
    "    for train_term, cluster_id in term_to_cluster.items():\n",
    "        sim = combined_similarity(term_lower, train_term)\n",
    "        if sim > best_similarity:\n",
    "            best_similarity = sim\n",
    "            best_cluster = cluster_id\n",
    "    \n",
    "    # If similarity is below threshold, return -1 (unknown cluster)\n",
    "    if best_similarity < similarity_threshold:\n",
    "        return -1, best_similarity\n",
    "    \n",
    "    return best_cluster, best_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f887f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting clusters for dev set...\n",
      "Term: 'deposito' -> Cluster: 3 (similarity: 1.000)\n",
      "Term: 'conferimento' -> Cluster: 3 (similarity: 1.000)\n",
      "Term: 'conferiti' -> Cluster: 4 (similarity: 1.000)\n",
      "Term: 'differenziati' -> Cluster: 44 (similarity: 0.900)\n",
      "Term: 'servizio di igiene urbana' -> Cluster: 7 (similarity: 1.000)\n",
      "Term: 'olio alimentare esausto' -> Cluster: 10 (similarity: 1.000)\n",
      "Term: 'oli esausti' -> Cluster: 10 (similarity: 1.000)\n",
      "Term: 'operatori dell'isola ecologica' -> Cluster: 11 (similarity: 1.000)\n",
      "Term: 'pannolini' -> Cluster: 12 (similarity: 1.000)\n",
      "Term: 'pannoloni' -> Cluster: 12 (similarity: 0.900)\n",
      "\n",
      "Predictions complete!\n",
      "Terms with exact/high similarity match: 218\n",
      "Terms with medium similarity match: 3\n",
      "Terms with low similarity match: 6\n",
      "Terms with no match (unknown cluster): 15\n",
      "\n",
      "Predictions complete!\n",
      "Terms with exact/high similarity match: 218\n",
      "Terms with medium similarity match: 3\n",
      "Terms with low similarity match: 6\n",
      "Terms with no match (unknown cluster): 15\n"
     ]
    }
   ],
   "source": [
    "# Predict clusters for dev set\n",
    "predictions = []\n",
    "similarity_scores = []\n",
    "\n",
    "print(\"Predicting clusters for dev set...\")\n",
    "for idx, row in dev_df.iterrows():\n",
    "    term = row['term']\n",
    "    cluster_pred, sim_score = find_best_cluster(term, term_to_cluster)\n",
    "    predictions.append(cluster_pred)\n",
    "    similarity_scores.append(sim_score)\n",
    "    \n",
    "    if idx < 10:  # Print first 10 predictions\n",
    "        print(f\"Term: '{term}' -> Cluster: {cluster_pred} (similarity: {sim_score:.3f})\")\n",
    "\n",
    "dev_df['predicted_cluster'] = predictions\n",
    "dev_df['similarity_score'] = similarity_scores\n",
    "\n",
    "print(f\"\\nPredictions complete!\")\n",
    "print(f\"Terms with exact/high similarity match: {sum(1 for s in similarity_scores if s >= 0.9)}\")\n",
    "print(f\"Terms with medium similarity match: {sum(1 for s in similarity_scores if 0.5 <= s < 0.9)}\")\n",
    "print(f\"Terms with low similarity match: {sum(1 for s in similarity_scores if 0.3 <= s < 0.5)}\")\n",
    "print(f\"Terms with no match (unknown cluster): {sum(1 for p in predictions if p == -1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb243a9",
   "metadata": {},
   "source": [
    "## Save Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59502e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to predictions/subtask_b_dev_vanilla_preds.csv\n"
     ]
    }
   ],
   "source": [
    "# Save predictions\n",
    "output_path = \"predictions/subtask_b_dev_vanilla_preds.csv\"\n",
    "#dev_df[['term', 'predicted_cluster', 'similarity_score']].to_csv(output_path, index=False)\n",
    "dev_df[['term', 'predicted_cluster']].to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Predictions saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411a8d78",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "Calculate clustering performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b32e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import codecs\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "def load_data(file_path):\n",
    "  \"\"\"\n",
    "  Loads data from a CSV or JSON file and returns a dictionary\n",
    "  where keys are terms and values are cluster_ids.\n",
    "\n",
    "  Args:\n",
    "    file_path: The path to the input file (CSV or JSON).\n",
    "\n",
    "  Returns:\n",
    "    A dictionary containing the loaded data.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: If the file format is not supported.\n",
    "  \"\"\"\n",
    "  if file_path.endswith('.csv'):\n",
    "    # Load data from CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    data = {term: int(cluster) for term, cluster in df.itertuples(index=False)}\n",
    "  elif file_path.endswith('.json'):\n",
    "    # Load data from JSON file\n",
    "    with codecs.open(file_path, 'r', 'utf-8') as f:\n",
    "      json_data = json.load(f)\n",
    "    # Extract terms from JSON data\n",
    "    data = {item[\"term\"]: item[\"cluster\"] for item in json_data[\"data\"]}\n",
    "  else:\n",
    "    # Raise error for unsupported file formats\n",
    "    raise ValueError(\"Unsupported file format. Only CSV and JSON files are supported.\")\n",
    "  return data\n",
    "\n",
    "class BCubed_calculator:\n",
    "  def __init__(self, gold, pred):\n",
    "    self.gold = gold\n",
    "    self.pred = pred\n",
    "    self.gold_cluster = defaultdict(set)\n",
    "    self.pred_cluster = defaultdict(set)\n",
    "    for item, clus_id in gold.items():\n",
    "        self.gold_cluster[clus_id].add(item)\n",
    "    for item, clus_id in pred.items():\n",
    "      self.pred_cluster[clus_id].add(item)\n",
    "\n",
    "  def bc_precision_item(self, item):\n",
    "    pred_id = self.pred[item]\n",
    "    gold_id = self.gold.get(item, None)\n",
    "    TP = len(self.pred_cluster[pred_id].intersection(self.gold_cluster[gold_id]))\n",
    "    FP = len(self.pred_cluster[pred_id]) - TP\n",
    "    return TP/(FP + TP)\n",
    "\n",
    "  def bc_recall_item(self, item):\n",
    "    pred_id = self.pred.get(item, None)\n",
    "    gold_id = self.gold.get(item)\n",
    "    TP = len(self.pred_cluster[pred_id].intersection(self.gold_cluster[gold_id]))\n",
    "    FN = len(self.gold_cluster[gold_id]) - TP\n",
    "    return TP/(TP + FN)\n",
    "\n",
    "def bcubed_precision(gold, pred):\n",
    "  calc = BCubed_calculator(gold, pred)\n",
    "  return np.average([calc.bc_precision_item(item) for item in calc.pred])\n",
    "\n",
    "def bcubed_recall(gold, pred):\n",
    "  calc = BCubed_calculator(gold, pred)\n",
    "  return np.average([calc.bc_recall_item(item) for item in calc.gold])\n",
    "\n",
    "def bcubed_f1(gold, pred):\n",
    "  return 2 * bcubed_precision(gold, pred) * bcubed_recall(gold, pred) / (bcubed_precision(gold, pred) + bcubed_recall(gold, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37c05125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCubed Precision: 0.6555\n",
      "BCubed Recall: 0.8703\n",
      "BCubed F1: 0.7478\n"
     ]
    }
   ],
   "source": [
    "preds = load_data(\"predictions/subtask_b_dev_vanilla_preds.csv\")\n",
    "gold = load_data(\"../data/subtask_b_dev.csv\")\n",
    "print(f\"BCubed Precision: {bcubed_precision(gold, preds):.4f}\")\n",
    "print(f\"BCubed Recall: {bcubed_recall(gold, preds):.4f}\")\n",
    "print(f\"BCubed F1: {bcubed_f1(gold, preds):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb155275",
   "metadata": {},
   "source": [
    "## Possible improvements\n",
    "- Tune the *unknown cluster* threshold\n",
    "- Tune the *word_overlap_weight* and *seq_weight* weights in combined similarity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ate-it",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "103969a8",
   "metadata": {},
   "source": [
    "# SpaCy Term Clustering\n",
    "\n",
    "This notebook demonstrates two approaches to term clustering using SpaCy:\n",
    "1. **Baseline**: Similarity-based clustering using SpaCy embeddings\n",
    "2. **Advanced**: K-means clustering with SpaCy word vectors\n",
    "\n",
    "Dataset: EvalITA 2025 ATE-IT (Automatic Term Extraction - Italian Testbed) - Subtask B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed309e9a",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "- For complete information about the Italian SpaCy models available: https://spacy.io/models/it\n",
    "- We'll use `it_core_news_md` or `it_core_news_lg` for word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0935a52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download it_core_news_sm\n",
    "#!python -m spacy download it_core_news_md\n",
    "#!python -m spacy download it_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8b03e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Italian model loaded successfully\n",
      "  Model: it_core_news_md\n",
      "  Vector dimensions: (20000, 300)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Tuple\n",
    "import spacy\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, v_measure_score\n",
    "from sklearn.metrics import homogeneity_score, completeness_score\n",
    "from scipy.spatial.distance import cosine\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load Italian model with word vectors\n",
    "try:\n",
    "    nlp = spacy.load('it_core_news_md')  # Medium model with vectors\n",
    "    print(f\"✓ Italian model loaded successfully\")\n",
    "    print(f\"  Model: it_core_news_md\")\n",
    "    print(f\"  Vector dimensions: {nlp.vocab.vectors.shape}\")\n",
    "except:\n",
    "    try:\n",
    "        nlp = spacy.load('it_core_news_lg')  # Try large model\n",
    "        print(f\"✓ Italian model loaded successfully\")\n",
    "        print(f\"  Model: it_core_news_lg\")\n",
    "        print(f\"  Vector dimensions: {nlp.vocab.vectors.shape}\")\n",
    "    except:\n",
    "        print(\"Model not found. Install with: python -m spacy download it_core_news_md\")\n",
    "        print(\"Note: Small model (it_core_news_sm) doesn't have word vectors!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90fa22c",
   "metadata": {},
   "source": [
    "## Load Training and Dev Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea15f5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (713, 2)\n",
      "Number of unique clusters: 299\n",
      "Number of terms: 713\n",
      "\n",
      "Dev data shape: (242, 2)\n",
      "Number of unique clusters in dev: 147\n",
      "Number of terms in dev: 242\n",
      "\n",
      "Example clusters from training:\n",
      "  Cluster 0: ['biodegradabili']\n",
      "  Cluster 1: ['cassonetti', 'contenitori stradali', 'cassonetti stradali', 'postazioni stradali']\n",
      "  Cluster 2: ['separare i rifiuti']\n",
      "  Cluster 3: ['deposito', 'conferimento', 'conferimento delle frazioni', 'conferimento dei rifiuti', 'conferimenti', 'operazioni di conferimento']\n",
      "  Cluster 4: ['conferiti', 'conferito', 'conferita']\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../data\"\n",
    "train_path = os.path.join(data_path, \"subtask_b_train.csv\")\n",
    "dev_path = os.path.join(data_path, \"subtask_b_dev.csv\")\n",
    "\n",
    "# Load training data\n",
    "train_df = pd.read_csv(train_path)\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Number of unique clusters: {train_df['cluster'].nunique()}\")\n",
    "print(f\"Number of terms: {len(train_df)}\")\n",
    "\n",
    "# Load dev data\n",
    "dev_df = pd.read_csv(dev_path)\n",
    "print(f\"\\nDev data shape: {dev_df.shape}\")\n",
    "print(f\"Number of unique clusters in dev: {dev_df['cluster'].nunique()}\")\n",
    "print(f\"Number of terms in dev: {len(dev_df)}\")\n",
    "\n",
    "# Create term-to-cluster mapping\n",
    "term_to_cluster = dict(zip(train_df['term'].str.lower(), train_df['cluster']))\n",
    "\n",
    "# Create cluster-to-terms mapping for analysis\n",
    "cluster_to_terms = defaultdict(list)\n",
    "for term, cluster in term_to_cluster.items():\n",
    "    cluster_to_terms[cluster].append(term)\n",
    "\n",
    "print(f\"\\nExample clusters from training:\")\n",
    "for cluster_id in sorted(cluster_to_terms.keys())[:5]:\n",
    "    print(f\"  Cluster {cluster_id}: {cluster_to_terms[cluster_id]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2150a2f6",
   "metadata": {},
   "source": [
    "## SpaCy Vector Utilities\n",
    "\n",
    "Functions to compute term embeddings using SpaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c426d6b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing similarity computation:\n",
      "  'rifiuti' <-> 'spazzatura': 0.572\n",
      "  'rifiuti' <-> 'carta': 0.198\n",
      "  'spazzatura' <-> 'carta': 0.423\n"
     ]
    }
   ],
   "source": [
    "def get_term_vector(term: str, nlp) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Get average word vector for a term.\n",
    "    For multi-word terms, average the word vectors.\n",
    "    \"\"\"\n",
    "    doc = nlp(term)\n",
    "    \n",
    "    # Get vectors for tokens that have them\n",
    "    vectors = [token.vector for token in doc if token.has_vector]\n",
    "    \n",
    "    if not vectors:\n",
    "        # Return zero vector if no vectors found\n",
    "        return np.zeros(nlp.vocab.vectors_length)\n",
    "    \n",
    "    # Average the vectors\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "\n",
    "def compute_similarity(term1: str, term2: str, nlp) -> float:\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between two terms using SpaCy vectors.\n",
    "    \"\"\"\n",
    "    vec1 = get_term_vector(term1, nlp)\n",
    "    vec2 = get_term_vector(term2, nlp)\n",
    "    \n",
    "    # Check for zero vectors\n",
    "    if np.all(vec1 == 0) or np.all(vec2 == 0):\n",
    "        return 0.0\n",
    "    \n",
    "    # Compute cosine similarity (1 - cosine distance)\n",
    "    return 1.0 - cosine(vec1, vec2)\n",
    "\n",
    "\n",
    "# Test\n",
    "test_terms = ['rifiuti', 'spazzatura', 'carta']\n",
    "print(\"Testing similarity computation:\")\n",
    "for i, t1 in enumerate(test_terms):\n",
    "    for t2 in test_terms[i+1:]:\n",
    "        sim = compute_similarity(t1, t2, nlp)\n",
    "        print(f\"  '{t1}' <-> '{t2}': {sim:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c52385",
   "metadata": {},
   "source": [
    "## Baseline: Similarity-Based Clustering\n",
    "\n",
    "For each dev term, find the most similar training term using SpaCy vectors and assign its cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef572cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing term vectors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 331.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built index with 3 terms\n",
      "\n",
      "✓ Baseline model works\n",
      "  'spazzatura' -> Cluster 43 (similarity: 0.676)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class SpacyBaselineClustering:\n",
    "    \"\"\"Similarity-based clustering using SpaCy word vectors.\"\"\"\n",
    "    \n",
    "    def __init__(self, nlp, similarity_threshold=0.5):\n",
    "        self.nlp = nlp\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.term_vectors = {}\n",
    "        self.term_to_cluster = {}\n",
    "    \n",
    "    def fit(self, terms: List[str], clusters: List[int]):\n",
    "        \"\"\"Build the term-cluster mapping and precompute vectors.\"\"\"\n",
    "        print(\"Computing term vectors...\")\n",
    "        for term, cluster in tqdm(zip(terms, clusters), total=len(terms)):\n",
    "            term_lower = term.lower()\n",
    "            self.term_to_cluster[term_lower] = cluster\n",
    "            self.term_vectors[term_lower] = get_term_vector(term_lower, self.nlp)\n",
    "        \n",
    "        print(f\"Built index with {len(self.term_vectors)} terms\")\n",
    "    \n",
    "    def predict_one(self, term: str) -> Tuple[int, float]:\n",
    "        \"\"\"Predict cluster for a single term.\"\"\"\n",
    "        term_lower = term.lower()\n",
    "        \n",
    "        # Check for exact match\n",
    "        if term_lower in self.term_to_cluster:\n",
    "            return self.term_to_cluster[term_lower], 1.0\n",
    "        \n",
    "        # Compute vector for query term\n",
    "        query_vec = get_term_vector(term_lower, self.nlp)\n",
    "        \n",
    "        # Find most similar term\n",
    "        best_similarity = 0.0\n",
    "        best_cluster = -1\n",
    "        \n",
    "        for train_term, train_vec in self.term_vectors.items():\n",
    "            # Skip if either vector is zero\n",
    "            if np.all(query_vec == 0) or np.all(train_vec == 0):\n",
    "                continue\n",
    "            \n",
    "            sim = 1.0 - cosine(query_vec, train_vec)\n",
    "            if sim > best_similarity:\n",
    "                best_similarity = sim\n",
    "                best_cluster = self.term_to_cluster[train_term]\n",
    "        \n",
    "        # Check threshold\n",
    "        if best_similarity < self.similarity_threshold:\n",
    "            return -1, best_similarity\n",
    "        \n",
    "        return best_cluster, best_similarity\n",
    "    \n",
    "    def predict(self, terms: List[str]) -> Tuple[List[int], List[float]]:\n",
    "        \"\"\"Predict clusters for multiple terms.\"\"\"\n",
    "        predictions = []\n",
    "        similarities = []\n",
    "        \n",
    "        for term in tqdm(terms, desc=\"Predicting clusters\"):\n",
    "            cluster, sim = self.predict_one(term)\n",
    "            predictions.append(cluster)\n",
    "            similarities.append(sim)\n",
    "        \n",
    "        return predictions, similarities\n",
    "\n",
    "\n",
    "# Test\n",
    "test_model = SpacyBaselineClustering(nlp, similarity_threshold=0.5)\n",
    "test_terms_list = ['rifiuti', 'carta', 'plastica']\n",
    "test_clusters_list = [37, 74, 43]\n",
    "test_model.fit(test_terms_list, test_clusters_list)\n",
    "pred_cluster, pred_sim = test_model.predict_one('spazzatura')\n",
    "print(f\"\\n Baseline model works!!\")\n",
    "print(f\"  'spazzatura' -> Cluster {pred_cluster} (similarity: {pred_sim:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e85bea",
   "metadata": {},
   "source": [
    "### Train and Evaluate Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5a55733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing term vectors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 713/713 [00:01<00:00, 378.92it/s]\n",
      "100%|██████████| 713/713 [00:01<00:00, 378.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built index with 713 terms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting clusters: 100%|██████████| 242/242 [00:01<00:00, 239.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction statistics:\n",
      "  Total terms: 242\n",
      "  Terms with exact match: 150\n",
      "  Terms with high similarity (≥0.7): 79\n",
      "  Terms with medium similarity (0.5-0.7): 6\n",
      "  Terms below threshold (unknown): 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize baseline model\n",
    "baseline_model = SpacyBaselineClustering(nlp, similarity_threshold=0.5)\n",
    "\n",
    "# Fit on training data\n",
    "baseline_model.fit(train_df['term'].tolist(), train_df['cluster'].tolist())\n",
    "\n",
    "# Predict on dev set\n",
    "baseline_preds, baseline_sims = baseline_model.predict(dev_df['term'].tolist())\n",
    "\n",
    "# Store results\n",
    "dev_df['baseline_cluster'] = baseline_preds\n",
    "dev_df['baseline_similarity'] = baseline_sims\n",
    "\n",
    "print(f\"\\nPrediction statistics:\")\n",
    "print(f\"  Total terms: {len(baseline_preds)}\")\n",
    "print(f\"  Terms with exact match: {sum(1 for s in baseline_sims if s >= 0.99)}\")\n",
    "print(f\"  Terms with high similarity (≥0.7): {sum(1 for s in baseline_sims if 0.7 <= s < 0.99)}\")\n",
    "print(f\"  Terms with medium similarity (0.5-0.7): {sum(1 for s in baseline_sims if 0.5 <= s < 0.7)}\")\n",
    "print(f\"  Terms below threshold (unknown): {sum(1 for p in baseline_preds if p == -1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643ef1d1",
   "metadata": {},
   "source": [
    "## Save Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30ad005c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline predictions saved to predictions/subtask_b_dev_spacy_baseline_preds.csv\n"
     ]
    }
   ],
   "source": [
    "# Save baseline predictions\n",
    "output_path_baseline = \"predictions/subtask_b_dev_spacy_baseline_preds.csv\"\n",
    "#dev_df[['term', 'baseline_cluster', 'baseline_similarity']].to_csv(output_path_baseline, index=False)\n",
    "dev_df[['term', 'baseline_cluster']].to_csv(output_path_baseline, index=False)\n",
    "print(f\"Baseline predictions saved to {output_path_baseline}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d839bf8",
   "metadata": {},
   "source": [
    "### Evaluate Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09307eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import codecs\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "def load_data(file_path):\n",
    "  \"\"\"\n",
    "  Loads data from a CSV or JSON file and returns a dictionary\n",
    "  where keys are terms and values are cluster_ids.\n",
    "\n",
    "  Args:\n",
    "    file_path: The path to the input file (CSV or JSON).\n",
    "\n",
    "  Returns:\n",
    "    A dictionary containing the loaded data.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: If the file format is not supported.\n",
    "  \"\"\"\n",
    "  if file_path.endswith('.csv'):\n",
    "    # Load data from CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    data = {term: int(cluster) for term, cluster in df.itertuples(index=False)}\n",
    "  elif file_path.endswith('.json'):\n",
    "    # Load data from JSON file\n",
    "    with codecs.open(file_path, 'r', 'utf-8') as f:\n",
    "      json_data = json.load(f)\n",
    "    # Extract terms from JSON data\n",
    "    data = {item[\"term\"]: item[\"cluster\"] for item in json_data[\"data\"]}\n",
    "  else:\n",
    "    # Raise error for unsupported file formats\n",
    "    raise ValueError(\"Unsupported file format. Only CSV and JSON files are supported.\")\n",
    "  return data\n",
    "\n",
    "class BCubed_calculator:\n",
    "  def __init__(self, gold, pred):\n",
    "    self.gold = gold\n",
    "    self.pred = pred\n",
    "    self.gold_cluster = defaultdict(set)\n",
    "    self.pred_cluster = defaultdict(set)\n",
    "    for item, clus_id in gold.items():\n",
    "        self.gold_cluster[clus_id].add(item)\n",
    "    for item, clus_id in pred.items():\n",
    "      self.pred_cluster[clus_id].add(item)\n",
    "\n",
    "  def bc_precision_item(self, item):\n",
    "    pred_id = self.pred[item]\n",
    "    gold_id = self.gold.get(item, None)\n",
    "    TP = len(self.pred_cluster[pred_id].intersection(self.gold_cluster[gold_id]))\n",
    "    FP = len(self.pred_cluster[pred_id]) - TP\n",
    "    return TP/(FP + TP)\n",
    "\n",
    "  def bc_recall_item(self, item):\n",
    "    pred_id = self.pred.get(item, None)\n",
    "    gold_id = self.gold.get(item)\n",
    "    TP = len(self.pred_cluster[pred_id].intersection(self.gold_cluster[gold_id]))\n",
    "    FN = len(self.gold_cluster[gold_id]) - TP\n",
    "    return TP/(TP + FN)\n",
    "\n",
    "def bcubed_precision(gold, pred):\n",
    "  calc = BCubed_calculator(gold, pred)\n",
    "  return np.average([calc.bc_precision_item(item) for item in calc.pred])\n",
    "\n",
    "def bcubed_recall(gold, pred):\n",
    "  calc = BCubed_calculator(gold, pred)\n",
    "  return np.average([calc.bc_recall_item(item) for item in calc.gold])\n",
    "\n",
    "def bcubed_f1(gold, pred):\n",
    "  return 2 * bcubed_precision(gold, pred) * bcubed_recall(gold, pred) / (bcubed_precision(gold, pred) + bcubed_recall(gold, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d30c289f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCubed Precision: 0.7549\n",
      "BCubed Recall: 0.8733\n",
      "BCubed F1: 0.8098\n"
     ]
    }
   ],
   "source": [
    "preds = load_data(\"predictions/subtask_b_dev_spacy_baseline_preds.csv\")\n",
    "gold = load_data(\"../data/subtask_b_dev.csv\")\n",
    "print(f\"BCubed Precision: {bcubed_precision(gold, preds):.4f}\")\n",
    "print(f\"BCubed Recall: {bcubed_recall(gold, preds):.4f}\")\n",
    "print(f\"BCubed F1: {bcubed_f1(gold, preds):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e27eb4",
   "metadata": {},
   "source": [
    "## K-Means Clustering with SpaCy Vectors\n",
    "\n",
    "Train a K-means model on training term vectors, then predict clusters for dev terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "616b3a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing term vectors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 247.06it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid vectors: 3 / 3\n",
      "Training K-means with 3 clusters...\n",
      "Training complete! Built mapping for 3 clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting clusters: 100%|██████████| 1/1 [00:00<00:00, 166.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " K-means model works!!\n",
      "  'spazzatura' -> Cluster 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class SpacyKMeansClustering:\n",
    "    \"\"\"K-means clustering using SpaCy word vectors.\"\"\"\n",
    "    \n",
    "    def __init__(self, nlp, n_clusters=None):\n",
    "        self.nlp = nlp\n",
    "        self.n_clusters = n_clusters\n",
    "        self.kmeans = None\n",
    "        self.cluster_mapping = {}  # Maps learned cluster IDs to gold cluster IDs\n",
    "    \n",
    "    def fit(self, terms: List[str], true_clusters: List[int]):\n",
    "        \"\"\"Train K-means on term vectors.\"\"\"\n",
    "        # Compute vectors for all terms\n",
    "        print(\"Computing term vectors...\")\n",
    "        vectors = []\n",
    "        valid_indices = []\n",
    "        \n",
    "        for i, term in enumerate(tqdm(terms)):\n",
    "            vec = get_term_vector(term.lower(), self.nlp)\n",
    "            if not np.all(vec == 0):  # Keep only terms with valid vectors\n",
    "                vectors.append(vec)\n",
    "                valid_indices.append(i)\n",
    "        \n",
    "        vectors = np.array(vectors, dtype=np.float64)  # Convert to float64 for sklearn\n",
    "        valid_clusters = [true_clusters[i] for i in valid_indices]\n",
    "        \n",
    "        print(f\"Valid vectors: {len(vectors)} / {len(terms)}\")\n",
    "        \n",
    "        # Determine number of clusters\n",
    "        if self.n_clusters is None:\n",
    "            self.n_clusters = len(set(true_clusters))\n",
    "        \n",
    "        print(f\"Training K-means with {self.n_clusters} clusters...\")\n",
    "        self.kmeans = KMeans(n_clusters=self.n_clusters, random_state=42, n_init=10)\n",
    "        learned_clusters = self.kmeans.fit_predict(vectors)\n",
    "        \n",
    "        # Map learned cluster IDs to gold cluster IDs\n",
    "        # For each learned cluster, find the most common gold cluster\n",
    "        self.cluster_mapping = {}\n",
    "        for learned_id in range(self.n_clusters):\n",
    "            mask = learned_clusters == learned_id\n",
    "            if sum(mask) > 0:\n",
    "                gold_clusters_in_learned = [valid_clusters[i] for i, m in enumerate(mask) if m]\n",
    "                # Most common gold cluster in this learned cluster\n",
    "                most_common = max(set(gold_clusters_in_learned), key=gold_clusters_in_learned.count)\n",
    "                self.cluster_mapping[learned_id] = most_common\n",
    "        \n",
    "        print(f\"Training complete! Built mapping for {len(self.cluster_mapping)} clusters\")\n",
    "    \n",
    "    def predict(self, terms: List[str]) -> List[int]:\n",
    "        \"\"\"Predict clusters for new terms.\"\"\"\n",
    "        if self.kmeans is None:\n",
    "            raise RuntimeError(\"Model not trained. Call fit() first.\")\n",
    "        \n",
    "        predictions = []\n",
    "        \n",
    "        for term in tqdm(terms, desc=\"Predicting clusters\"):\n",
    "            vec = get_term_vector(term.lower(), self.nlp)\n",
    "            \n",
    "            if np.all(vec == 0):\n",
    "                predictions.append(-1)  # Unknown\n",
    "            else:\n",
    "                vec = np.array([vec], dtype=np.float64)  # Convert to float64 and reshape\n",
    "                learned_cluster = self.kmeans.predict(vec)[0]\n",
    "                # Map to gold cluster ID\n",
    "                gold_cluster = self.cluster_mapping.get(learned_cluster, -1)\n",
    "                predictions.append(gold_cluster)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "\n",
    "# Test\n",
    "test_kmeans = SpacyKMeansClustering(nlp)\n",
    "test_kmeans.fit(['rifiuti', 'carta', 'plastica'], [37, 74, 43])\n",
    "test_pred = test_kmeans.predict(['spazzatura'])\n",
    "print(f\"\\n K-means model works!!\")\n",
    "print(f\"  'spazzatura' -> Cluster {test_pred[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e90403",
   "metadata": {},
   "source": [
    "### Train and Evaluate K-Means Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d326335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing term vectors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 713/713 [00:01<00:00, 367.00it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid vectors: 702 / 713\n",
      "Training K-means with 299 clusters...\n",
      "Training complete! Built mapping for 299 clusters\n",
      "Training complete! Built mapping for 299 clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting clusters: 100%|██████████| 242/242 [00:00<00:00, 309.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction statistics:\n",
      "  Total terms: 242\n",
      "  Terms with predictions: 239\n",
      "  Terms without predictions (unknown): 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize K-means model\n",
    "kmeans_model = SpacyKMeansClustering(nlp)\n",
    "\n",
    "# Fit on training data\n",
    "kmeans_model.fit(train_df['term'].tolist(), train_df['cluster'].tolist())\n",
    "\n",
    "# Predict on dev set\n",
    "kmeans_preds = kmeans_model.predict(dev_df['term'].tolist())\n",
    "\n",
    "# Store results\n",
    "dev_df['kmeans_cluster'] = kmeans_preds\n",
    "\n",
    "print(f\"\\nPrediction statistics:\")\n",
    "print(f\"  Total terms: {len(kmeans_preds)}\")\n",
    "print(f\"  Terms with predictions: {sum(1 for p in kmeans_preds if p != -1)}\")\n",
    "print(f\"  Terms without predictions (unknown): {sum(1 for p in kmeans_preds if p == -1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfef454",
   "metadata": {},
   "source": [
    "## Save Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "48d7ea7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-means predictions saved to predictions/subtask_b_dev_spacy_kmeans_preds.csv\n"
     ]
    }
   ],
   "source": [
    "# Save K-means predictions\n",
    "output_path_kmeans = \"predictions/subtask_b_dev_spacy_kmeans_preds.csv\"\n",
    "#dev_df[['term', 'kmeans_cluster']].to_csv(output_path_kmeans, index=False)\n",
    "dev_df[['term', 'kmeans_cluster']].to_csv(output_path_kmeans, index=False)\n",
    "print(f\"K-means predictions saved to {output_path_kmeans}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab89ec0",
   "metadata": {},
   "source": [
    "### Evaluate K-means Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4bedd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCubed Precision: 0.5629\n",
      "BCubed Recall: 0.7730\n",
      "BCubed F1: 0.6514\n"
     ]
    }
   ],
   "source": [
    "preds = load_data(\"predictions/subtask_b_dev_spacy_kmeans_preds.csv\")\n",
    "gold = load_data(\"../data/subtask_b_dev.csv\")\n",
    "print(f\"BCubed Precision: {bcubed_precision(gold, preds):.4f}\")\n",
    "print(f\"BCubed Recall: {bcubed_recall(gold, preds):.4f}\")\n",
    "print(f\"BCubed F1: {bcubed_f1(gold, preds):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52b7f05",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7785ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODEL COMPARISON - BCubed Metrics\n",
      "================================================================================\n",
      "                Model  BCubed Precision  BCubed Recall  BCubed F1\n",
      "Baseline (Similarity)          0.754900       0.873318   0.809802\n",
      "              K-Means          0.562915       0.772963   0.651426\n",
      "================================================================================\n",
      "\n",
      "K-Means vs Baseline:\n",
      "  BCubed Precision improvement: -25.4%\n",
      "  BCubed Recall improvement: -11.5%\n",
      "  BCubed F1 improvement: -19.6%\n"
     ]
    }
   ],
   "source": [
    "# Load predictions and gold standard for both models\n",
    "gold = load_data(\"../data/subtask_b_dev.csv\")\n",
    "baseline_preds = load_data(\"predictions/subtask_b_dev_spacy_baseline_preds.csv\")\n",
    "kmeans_preds = load_data(\"predictions/subtask_b_dev_spacy_kmeans_preds.csv\")\n",
    "\n",
    "# Compute BCubed metrics for baseline model\n",
    "baseline_precision = bcubed_precision(gold, baseline_preds)\n",
    "baseline_recall = bcubed_recall(gold, baseline_preds)\n",
    "baseline_f1 = bcubed_f1(gold, baseline_preds)\n",
    "\n",
    "# Compute BCubed metrics for K-means model\n",
    "kmeans_precision = bcubed_precision(gold, kmeans_preds)\n",
    "kmeans_recall = bcubed_recall(gold, kmeans_preds)\n",
    "kmeans_f1 = bcubed_f1(gold, kmeans_preds)\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame([\n",
    "    {\n",
    "        'Model': 'Baseline (Similarity)',\n",
    "        'BCubed Precision': baseline_precision,\n",
    "        'BCubed Recall': baseline_recall,\n",
    "        'BCubed F1': baseline_f1\n",
    "    },\n",
    "    {\n",
    "        'Model': 'K-Means',\n",
    "        'BCubed Precision': kmeans_precision,\n",
    "        'BCubed Recall': kmeans_recall,\n",
    "        'BCubed F1': kmeans_f1\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON - BCubed Metrics\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate improvements\n",
    "precision_improvement = (kmeans_precision - baseline_precision) / baseline_precision * 100 if baseline_precision > 0 else 0\n",
    "recall_improvement = (kmeans_recall - baseline_recall) / baseline_recall * 100 if baseline_recall > 0 else 0\n",
    "f1_improvement = (kmeans_f1 - baseline_f1) / baseline_f1 * 100 if baseline_f1 > 0 else 0\n",
    "\n",
    "print(f\"\\nK-Means vs Baseline:\")\n",
    "print(f\"  BCubed Precision improvement: {precision_improvement:+.1f}%\")\n",
    "print(f\"  BCubed Recall improvement: {recall_improvement:+.1f}%\")\n",
    "print(f\"  BCubed F1 improvement: {f1_improvement:+.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636627a0",
   "metadata": {},
   "source": [
    "## Possible improvements\n",
    "- Tune *similarity_threshold* in baseline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ate-it",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "300afcee",
   "metadata": {},
   "source": [
    "# BERT Token Classification for Italian Term Extraction\n",
    "\n",
    "This notebook demonstrates a BERT-based approach to term extraction:\n",
    "- Uses BIO tagging scheme (Beginning-Inside-Outside)\n",
    "- Fine-tunes Italian BERT model for token classification\n",
    "- Trains on labeled data to recognize term boundaries\n",
    "\n",
    "Dataset: EvalITA 2025 ATE-IT (Automatic Term Extraction - Italian Testbed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257d342c",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacd6f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ims/anaconda3/envs/martinelli_GBIE/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete\n",
      "PyTorch version: 2.7.1+cu118\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForTokenClassification, \n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Setup complete\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8767bd33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['O', 'B-TERM', 'I-TERM']\n",
      "Label to ID: {'O': 0, 'B-TERM': 1, 'I-TERM': 2}\n",
      "\n",
      "Model: dbmdz/bert-base-italian-uncased\n",
      "Output directory: models/bert_token_classification\n"
     ]
    }
   ],
   "source": [
    "# Define label mappings for BIO tagging scheme\n",
    "label_list = ['O', 'B-TERM', 'I-TERM']\n",
    "label2id = {k: v for v, k in enumerate(label_list)}\n",
    "id2label = {v: k for v, k in enumerate(label_list)}\n",
    "\n",
    "print(f\"Labels: {label_list}\")\n",
    "print(f\"Label to ID: {label2id}\")\n",
    "\n",
    "# Model configuration\n",
    "model_name = \"dbmdz/bert-base-italian-uncased\"\n",
    "output_model_dir = \"models/bert_token_classification\"\n",
    "\n",
    "print(f\"\\nModel: {model_name}\")\n",
    "print(f\"Output directory: {output_model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8142a69c",
   "metadata": {},
   "source": [
    "## Data Loading and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfe3bbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data loading functions defined\n"
     ]
    }
   ],
   "source": [
    "def load_jsonl(path: str):\n",
    "    \"\"\"Load a JSON lines file or JSON array file.\"\"\"\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read().strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    try:\n",
    "        data = json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        data = []\n",
    "        for line in text.splitlines():\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "\n",
    "def build_sentence_gold_map(records):\n",
    "    \"\"\"Convert dataset rows into list of sentences with aggregated terms.\"\"\"\n",
    "    out = {}\n",
    "    \n",
    "    if isinstance(records, dict) and 'data' in records:\n",
    "        rows = records['data']\n",
    "    else:\n",
    "        rows = records\n",
    "    \n",
    "    for r in rows:\n",
    "        key = (r.get('document_id'), r.get('paragraph_id'), r.get('sentence_id'))\n",
    "        if key not in out:\n",
    "            out[key] = {\n",
    "                'document_id': r.get('document_id'),\n",
    "                'paragraph_id': r.get('paragraph_id'),\n",
    "                'sentence_id': r.get('sentence_id'),\n",
    "                'sentence_text': r.get('sentence_text', ''),\n",
    "                'terms': []\n",
    "            }\n",
    "        \n",
    "        if isinstance(r.get('term_list'), list):\n",
    "            for t in r.get('term_list'):\n",
    "                if t and t not in out[key]['terms']:\n",
    "                    out[key]['terms'].append(t)\n",
    "        else:\n",
    "            term = r.get('term')\n",
    "            if term and term not in out[key]['terms']:\n",
    "                out[key]['terms'].append(term)\n",
    "    \n",
    "    return list(out.values())\n",
    "\n",
    "\n",
    "print(\"✓ Data loading functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05486208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sentences: 2308\n",
      "Dev sentences: 577\n",
      "\n",
      "Example sentence:\n",
      "  Text: AFFIDAMENTO DEL “SERVIZIO DI SPAZZAMENTO, RACCOLTA, TRASPORTO E SMALTIMENTO/RECUPERO DEI RIFIUTI URBANI ED ASSIMILATI E SERVIZI COMPLEMENTARI DELLA CITTA' DI AGROPOLI” VALEVOLE PER UN QUINQUENNIO\n",
      "  Terms: ['raccolta', 'recupero', 'servizio di raccolta', 'servizio di spazzamento', 'smaltimento', 'trasporto']\n"
     ]
    }
   ],
   "source": [
    "# Load training and dev data\n",
    "train_data = load_jsonl('../data/subtask_a_train.json')\n",
    "dev_data = load_jsonl('../data/subtask_a_dev.json')\n",
    "\n",
    "train_sentences = build_sentence_gold_map(train_data)\n",
    "dev_sentences = build_sentence_gold_map(dev_data)\n",
    "\n",
    "print(f\"Training sentences: {len(train_sentences)}\")\n",
    "print(f\"Dev sentences: {len(dev_sentences)}\")\n",
    "print(f\"\\nExample sentence:\")\n",
    "print(f\"  Text: {train_sentences[6]['sentence_text']}\")\n",
    "print(f\"  Terms: {train_sentences[6]['terms']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90654f36",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "Using the official evaluation metrics from the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2708c291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Evaluation functions defined\n"
     ]
    }
   ],
   "source": [
    "def micro_f1_score(gold_standard, system_output):\n",
    "    \"\"\"\n",
    "    Evaluates performance using Precision, Recall, and F1 score \n",
    "    based on individual term matching (micro-average).\n",
    "    \"\"\"\n",
    "    total_true_positives = 0\n",
    "    total_false_positives = 0\n",
    "    total_false_negatives = 0\n",
    "    \n",
    "    for gold, system in zip(gold_standard, system_output):\n",
    "        gold_set = set(gold)\n",
    "        system_set = set(system)\n",
    "        \n",
    "        true_positives = len(gold_set.intersection(system_set))\n",
    "        false_positives = len(system_set - gold_set)\n",
    "        false_negatives = len(gold_set - system_set)\n",
    "        \n",
    "        total_true_positives += true_positives\n",
    "        total_false_positives += false_positives\n",
    "        total_false_negatives += false_negatives\n",
    "    \n",
    "    precision = total_true_positives / (total_true_positives + total_false_positives) if (total_true_positives + total_false_positives) > 0 else 0\n",
    "    recall = total_true_positives / (total_true_positives + total_false_negatives) if (total_true_positives + total_false_negatives) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return precision, recall, f1, total_true_positives, total_false_positives, total_false_negatives\n",
    "\n",
    "\n",
    "def type_f1_score(gold_standard, system_output):\n",
    "    \"\"\"\n",
    "    Evaluates performance using Type Precision, Type Recall, and Type F1 score\n",
    "    based on the set of unique terms extracted at least once across the entire dataset.\n",
    "    \"\"\"\n",
    "    all_gold_terms = set()\n",
    "    for item_terms in gold_standard:\n",
    "        all_gold_terms.update(item_terms)\n",
    "    \n",
    "    all_system_terms = set()\n",
    "    for item_terms in system_output:\n",
    "        all_system_terms.update(item_terms)\n",
    "    \n",
    "    type_true_positives = len(all_gold_terms.intersection(all_system_terms))\n",
    "    type_false_positives = len(all_system_terms - all_gold_terms)\n",
    "    type_false_negatives = len(all_gold_terms - all_system_terms)\n",
    "    \n",
    "    type_precision = type_true_positives / (type_true_positives + type_false_positives) if (type_true_positives + type_false_positives) > 0 else 0\n",
    "    type_recall = type_true_positives / (type_true_positives + type_false_negatives) if (type_true_positives + type_false_negatives) > 0 else 0\n",
    "    type_f1 = 2 * (type_precision * type_recall) / (type_precision + type_recall) if (type_precision + type_recall) > 0 else 0\n",
    "    \n",
    "    return type_precision, type_recall, type_f1\n",
    "\n",
    "\n",
    "print(\"✓ Evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfac0bc",
   "metadata": {},
   "source": [
    "## Initialize BERT Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bb1b27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing BERT tokenizer and model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dbmdz/bert-base-italian-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Tokenizer loaded: BertTokenizerFast\n",
      "✓ Model loaded with 3 labels\n",
      "  Vocabulary size: 31102\n",
      "\n",
      "Sample tokenization: ['il', 'servizio', 'era', 'eccellente', 'e', 'il', 'cibo', 'delizioso', '.']...\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer and model\n",
    "print(\"Initializing BERT tokenizer and model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=len(label_list), \n",
    "    id2label=id2label, \n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "print(f\"✓ Tokenizer loaded: {tokenizer.__class__.__name__}\")\n",
    "print(f\"✓ Model loaded with {model.num_labels} labels\")\n",
    "print(f\"  Vocabulary size: {tokenizer.vocab_size}\")\n",
    "\n",
    "# Test tokenization\n",
    "sample_text = \"Il servizio era eccellente e il cibo delizioso.\"\n",
    "tokens = tokenizer.tokenize(sample_text)\n",
    "print(f\"\\nSample tokenization: {tokens}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681cd148",
   "metadata": {},
   "source": [
    "## BIO Tag Generation for Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "158f6ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ BIO tag generation function defined\n"
     ]
    }
   ],
   "source": [
    "def create_ner_tags(text, terms, tokenizer, label2id):\n",
    "    \"\"\"\n",
    "    Create NER tags for tokenized text based on the given terms.\n",
    "    Uses BIO scheme: B-TERM for beginning, I-TERM for inside, O for outside.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    ner_tags = ['O'] * len(tokens)\n",
    "    \n",
    "    # Sort terms by length (descending) to handle overlapping terms\n",
    "    sorted_terms = sorted(terms, key=len, reverse=True)\n",
    "    \n",
    "    for term in sorted_terms:\n",
    "        normalized_text = text.lower()\n",
    "        normalized_term = term.lower()\n",
    "        \n",
    "        # Find all occurrences of the term\n",
    "        term_positions = []\n",
    "        start = 0\n",
    "        while True:\n",
    "            pos = normalized_text.find(normalized_term, start)\n",
    "            if pos == -1:\n",
    "                break\n",
    "            term_positions.append((pos, pos + len(normalized_term)))\n",
    "            start = pos + 1\n",
    "        \n",
    "        # Map character positions to token positions\n",
    "        for start_char, end_char in term_positions:\n",
    "            token_start_idx = None\n",
    "            token_end_idx = None\n",
    "            \n",
    "            encoding = tokenizer(text, return_offsets_mapping=True, add_special_tokens=False)\n",
    "            offsets = encoding['offset_mapping']\n",
    "            \n",
    "            for i, (token_start, token_end) in enumerate(offsets):\n",
    "                if token_start < end_char and token_end > start_char:\n",
    "                    if token_start_idx is None:\n",
    "                        token_start_idx = i\n",
    "                    token_end_idx = i\n",
    "            \n",
    "            # Apply BIO tagging\n",
    "            if token_start_idx is not None and token_end_idx is not None:\n",
    "                for i in range(token_start_idx, token_end_idx + 1):\n",
    "                    if i < len(ner_tags) and ner_tags[i] == 'O':\n",
    "                        if i == token_start_idx:\n",
    "                            ner_tags[i] = 'B-TERM'\n",
    "                        else:\n",
    "                            ner_tags[i] = 'I-TERM'\n",
    "    \n",
    "    ner_tag_ids = [label2id[tag] for tag in ner_tags]\n",
    "    return tokens, ner_tag_ids\n",
    "\n",
    "\n",
    "print(\"✓ BIO tag generation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0668cf5c",
   "metadata": {},
   "source": [
    "## Process Training and Dev Data with BIO Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1b6518b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training data...\n",
      "  Processed 0/2308\n",
      "  Processed 1000/2308\n",
      "  Processed 2000/2308\n",
      "✓ Training data processed: 2308 sentences\n",
      "\n",
      "Processing dev data...\n",
      "  Processed 0/577\n",
      "  Processed 200/577\n",
      "  Processed 400/577\n",
      "✓ Dev data processed: 577 sentences\n",
      "\n",
      "Sample train sentence:\n",
      "  Text: AFFIDAMENTO DEL “SERVIZIO DI SPAZZAMENTO, RACCOLTA, TRASPORTO E SMALTIMENTO/RECUPERO DEI RIFIUTI URBANI ED ASSIMILATI E SERVIZI COMPLEMENTARI DELLA CITTA' DI AGROPOLI” VALEVOLE PER UN QUINQUENNIO\n",
      "  Terms: ['raccolta', 'recupero', 'servizio di raccolta', 'servizio di spazzamento', 'smaltimento', 'trasporto']\n",
      "\n",
      "|    | Token         | Tag    |\n",
      "|---:|:--------------|:-------|\n",
      "|  0 | affidamento   | O      |\n",
      "|  1 | del           | O      |\n",
      "|  2 | “             | O      |\n",
      "|  3 | servizio      | B-TERM |\n",
      "|  4 | di            | I-TERM |\n",
      "|  5 | spa           | I-TERM |\n",
      "|  6 | ##zzamento    | I-TERM |\n",
      "|  7 | ,             | O      |\n",
      "|  8 | raccolta      | B-TERM |\n",
      "|  9 | ,             | O      |\n",
      "| 10 | trasporto     | B-TERM |\n",
      "| 11 | e             | O      |\n",
      "| 12 | smaltimento   | B-TERM |\n",
      "| 13 | /             | O      |\n",
      "| 14 | recupero      | B-TERM |\n",
      "| 15 | dei           | O      |\n",
      "| 16 | rifiuti       | O      |\n",
      "| 17 | urbani        | O      |\n",
      "| 18 | ed            | O      |\n",
      "| 19 | assimi        | O      |\n",
      "| 20 | ##lati        | O      |\n",
      "| 21 | e             | O      |\n",
      "| 22 | servizi       | O      |\n",
      "| 23 | complementari | O      |\n",
      "| 24 | della         | O      |\n",
      "| 25 | citta         | O      |\n",
      "| 26 | '             | O      |\n",
      "| 27 | di            | O      |\n",
      "| 28 | agro          | O      |\n",
      "| 29 | ##poli        | O      |\n",
      "| 30 | ”             | O      |\n",
      "| 31 | vale          | O      |\n",
      "| 32 | ##vole        | O      |\n",
      "| 33 | per           | O      |\n",
      "| 34 | un            | O      |\n",
      "| 35 | quinqu        | O      |\n",
      "| 36 | ##ennio       | O      |\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Process training data\n",
    "print(\"Processing training data...\")\n",
    "for i, entry in enumerate(train_sentences):\n",
    "    text = entry['sentence_text']\n",
    "    terms = entry['terms']\n",
    "    \n",
    "    tokens, ner_tags = create_ner_tags(text, terms, tokenizer, label2id)\n",
    "    entry['tokens'] = tokens\n",
    "    entry['ner_tags'] = ner_tags\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print(f\"  Processed {i}/{len(train_sentences)}\")\n",
    "\n",
    "print(f\"✓ Training data processed: {len(train_sentences)} sentences\")\n",
    "\n",
    "# Process dev data\n",
    "print(\"\\nProcessing dev data...\")\n",
    "for i, entry in enumerate(dev_sentences):\n",
    "    text = entry['sentence_text']\n",
    "    terms = entry['terms']\n",
    "    \n",
    "    tokens, ner_tags = create_ner_tags(text, terms, tokenizer, label2id)\n",
    "    entry['tokens'] = tokens\n",
    "    entry['ner_tags'] = ner_tags\n",
    "    \n",
    "    if i % 200 == 0:\n",
    "        print(f\"  Processed {i}/{len(dev_sentences)}\")\n",
    "\n",
    "print(f\"✓ Dev data processed: {len(dev_sentences)} sentences\")\n",
    "\n",
    "print(f\"\\nSample train sentence:\")\n",
    "print(f\"  Text: {train_sentences[6]['sentence_text']}\")\n",
    "print(f\"  Terms: {train_sentences[6]['terms']}\")\n",
    "token_tags = []\n",
    "for token, tag in zip(train_sentences[6]['tokens'], train_sentences[6]['ner_tags']):\n",
    "    token_tags.append((token, id2label[tag]))\n",
    "print(f\"\\n{pd.DataFrame(token_tags, columns=['Token', 'Tag']).to_markdown()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f0740c",
   "metadata": {},
   "source": [
    "## Prepare Dataset for BERT Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98289860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Custom dataset class defined\n"
     ]
    }
   ],
   "source": [
    "class TokenClassificationDataset(Dataset):\n",
    "    \"\"\"Custom dataset for token classification with BERT.\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, tokens, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.tokens = tokens\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        original_labels = self.labels[idx]\n",
    "        \n",
    "        # Tokenize the text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "        \n",
    "        # Initialize labels with -100 (ignored by loss)\n",
    "        labels = torch.full((self.max_length,), -100, dtype=torch.long)\n",
    "        \n",
    "        # Align original labels with tokenized input\n",
    "        for i, token_id in enumerate(input_ids):\n",
    "            if i == 0:  # CLS token\n",
    "                labels[i] = -100\n",
    "            elif token_id == self.tokenizer.sep_token_id:  # SEP token\n",
    "                labels[i] = -100\n",
    "            elif token_id == self.tokenizer.pad_token_id:  # PAD token\n",
    "                labels[i] = -100\n",
    "            else:\n",
    "                original_idx = i - 1\n",
    "                if original_idx < len(original_labels):\n",
    "                    labels[i] = original_labels[original_idx]\n",
    "                else:\n",
    "                    labels[i] = -100\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"✓ Custom dataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d622d3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training datasets...\n",
      "✓ Training dataset: 2308 examples\n",
      "✓ Dev dataset: 577 examples\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "print(\"Creating training datasets...\")\n",
    "\n",
    "train_dataset = TokenClassificationDataset(\n",
    "    texts=[entry['sentence_text'] for entry in train_sentences],\n",
    "    tokens=[entry['tokens'] for entry in train_sentences],\n",
    "    labels=[entry['ner_tags'] for entry in train_sentences],\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "dev_dataset = TokenClassificationDataset(\n",
    "    texts=[entry['sentence_text'] for entry in dev_sentences],\n",
    "    tokens=[entry['tokens'] for entry in dev_sentences],\n",
    "    labels=[entry['ner_tags'] for entry in dev_sentences],\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(f\"✓ Training dataset: {len(train_dataset)} examples\")\n",
    "print(f\"✓ Dev dataset: {len(dev_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75ca5f8",
   "metadata": {},
   "source": [
    "## Configure Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ebe321ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data collator initialized\n"
     ]
    }
   ],
   "source": [
    "# Setup data collator for token classification\n",
    "# Data collator is used to dynamically pad inputs and labels\n",
    "data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "print(\"✓ Data collator initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b250c850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Training configuration ready\n",
      "  Batch size: 16\n",
      "  Epochs: 3\n",
      "  Learning rate: 2e-05\n"
     ]
    }
   ],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_model_dir,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    "    logging_steps=100,\n",
    "    save_total_limit=2,\n",
    "    seed=42,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "print(\"✓ Training configuration ready\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b55df57",
   "metadata": {},
   "source": [
    "## Train BERT Model\n",
    "\n",
    "Note: This cell might take several minutes to run.\n",
    "\n",
    "\n",
    "**Additional configurations to test**\n",
    "- Aggregate training samples per paragraph/document\n",
    "- Change hyperparameters (*learning_rate*, *batch_size*, *num_train_epochs*, *weight_decay*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b6c44051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Trainer...\n",
      "✓ Trainer initialized\n",
      "  Training samples: 2308\n",
      "  Evaluation samples: 577\n"
     ]
    }
   ],
   "source": [
    "# Initialize Trainer\n",
    "print(\"Initializing Trainer...\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "print(\"✓ Trainer initialized\")\n",
    "print(f\"  Training samples: {len(train_dataset)}\")\n",
    "print(f\"  Evaluation samples: {len(dev_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "900dd7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Starting model training...\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "NCCL Error 2: unhandled system error (run with NCCL_DEBUG=INFO for details)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      7\u001b[0m training_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 9\u001b[0m train_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m training_duration \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m training_start_time\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/martinelli_GBIE/lib/python3.10/site-packages/transformers/trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/martinelli_GBIE/lib/python3.10/site-packages/transformers/trainer.py:2388\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2387\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2388\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2391\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2392\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2393\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2394\u001b[0m ):\n\u001b[1;32m   2395\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2396\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda3/envs/martinelli_GBIE/lib/python3.10/site-packages/transformers/trainer.py:3485\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3484\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3485\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3487\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3489\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3490\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3491\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/envs/martinelli_GBIE/lib/python3.10/site-packages/transformers/trainer.py:3532\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3530\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3531\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3532\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3533\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3534\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/martinelli_GBIE/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/martinelli_GBIE/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/martinelli_GBIE/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:193\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 193\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_apply(replicas, inputs, module_kwargs)\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/martinelli_GBIE/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:200\u001b[0m, in \u001b[0;36mDataParallel.replicate\u001b[0;34m(self, module, device_ids)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreplicate\u001b[39m(\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m, module: T, device_ids: Sequence[Union[\u001b[38;5;28mint\u001b[39m, torch\u001b[38;5;241m.\u001b[39mdevice]]\n\u001b[1;32m    199\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[T]:\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreplicate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_grad_enabled\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/martinelli_GBIE/lib/python3.10/site-packages/torch/nn/parallel/replicate.py:126\u001b[0m, in \u001b[0;36mreplicate\u001b[0;34m(network, devices, detach)\u001b[0m\n\u001b[1;32m    124\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(network\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[1;32m    125\u001b[0m param_indices \u001b[38;5;241m=\u001b[39m {param: idx \u001b[38;5;28;01mfor\u001b[39;00m idx, param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(params)}\n\u001b[0;32m--> 126\u001b[0m param_copies \u001b[38;5;241m=\u001b[39m \u001b[43m_broadcast_coalesced_reshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m buffers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(network\u001b[38;5;241m.\u001b[39mbuffers())\n\u001b[1;32m    129\u001b[0m buffers_rg: \u001b[38;5;28mlist\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/martinelli_GBIE/lib/python3.10/site-packages/torch/nn/parallel/replicate.py:95\u001b[0m, in \u001b[0;36m_broadcast_coalesced_reshape\u001b[0;34m(tensors, devices, detach)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;66;03m# Use the autograd function to broadcast if not detach\u001b[39;00m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tensors) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 95\u001b[0m         tensor_copies \u001b[38;5;241m=\u001b[39m \u001b[43mBroadcast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m     97\u001b[0m             tensor_copies[i : i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(tensors)]\n\u001b[1;32m     98\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(tensor_copies), \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[1;32m     99\u001b[0m         ]\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/martinelli_GBIE/lib/python3.10/site-packages/torch/autograd/function.py:575\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/martinelli_GBIE/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:23\u001b[0m, in \u001b[0;36mBroadcast.forward\u001b[0;34m(ctx, target_gpus, *inputs)\u001b[0m\n\u001b[1;32m     21\u001b[0m ctx\u001b[38;5;241m.\u001b[39mnum_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(inputs)\n\u001b[1;32m     22\u001b[0m ctx\u001b[38;5;241m.\u001b[39minput_device \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_device()\n\u001b[0;32m---> 23\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcomm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_coalesced\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_gpus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m non_differentiables \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, input_requires_grad \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(ctx\u001b[38;5;241m.\u001b[39mneeds_input_grad[\u001b[38;5;241m1\u001b[39m:]):\n",
      "File \u001b[0;32m~/anaconda3/envs/martinelli_GBIE/lib/python3.10/site-packages/torch/nn/parallel/comm.py:66\u001b[0m, in \u001b[0;36mbroadcast_coalesced\u001b[0;34m(tensors, devices, buffer_size)\u001b[0m\n\u001b[1;32m     64\u001b[0m devices \u001b[38;5;241m=\u001b[39m [_get_device_index(d) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m devices]\n\u001b[1;32m     65\u001b[0m tensors \u001b[38;5;241m=\u001b[39m [_handle_complex(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tensors]\n\u001b[0;32m---> 66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_broadcast_coalesced\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: NCCL Error 2: unhandled system error (run with NCCL_DEBUG=INFO for details)"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "print(\"=\"*60)\n",
    "print(\"Starting model training...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import time\n",
    "training_start_time = time.time()\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "training_duration = time.time() - training_start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ TRAINING COMPLETED!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training time: {training_duration/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9d8a7e",
   "metadata": {},
   "source": [
    "## Save Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be37024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "print(\"Saving trained model...\")\n",
    "\n",
    "os.makedirs(output_model_dir, exist_ok=True)\n",
    "trainer.save_model(output_model_dir)\n",
    "tokenizer.save_pretrained(output_model_dir)\n",
    "\n",
    "print(f\"✓ Model saved to: {output_model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca318c0",
   "metadata": {},
   "source": [
    "## Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb4cef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model for inference\n",
    "print(\"Loading trained model for inference...\")\n",
    "\n",
    "inference_model = AutoModelForTokenClassification.from_pretrained(output_model_dir)\n",
    "inference_tokenizer = AutoTokenizer.from_pretrained(output_model_dir)\n",
    "inference_model.eval()\n",
    "\n",
    "print(f\"✓ Model loaded from: {output_model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b2a2d4",
   "metadata": {},
   "source": [
    "## Predict on Dev Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2699ea53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_inference(model, tokenizer, text, id2label):\n",
    "    \"\"\"Perform token classification inference on a single text.\"\"\"\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**{k: v for k, v in inputs.items() if k != 'offset_mapping'})\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        predicted_labels = torch.argmax(predictions, dim=-1)\n",
    "    \n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    labels = [id2label[pred.item()] for pred in predicted_labels[0]]\n",
    "    \n",
    "    # Extract terms using BIO scheme\n",
    "    predicted_terms = []\n",
    "    current_term = []\n",
    "    \n",
    "    for token, label in zip(tokens, labels):\n",
    "        if token in ['[CLS]', '[SEP]', '[PAD]']:\n",
    "            continue\n",
    "            \n",
    "        if label == 'B-TERM':\n",
    "            if current_term:\n",
    "                predicted_terms.append(tokenizer.convert_tokens_to_string(current_term))\n",
    "            current_term = [token]\n",
    "        elif label == 'I-TERM' and current_term:\n",
    "            current_term.append(token)\n",
    "        else:\n",
    "            if current_term:\n",
    "                predicted_terms.append(tokenizer.convert_tokens_to_string(current_term))\n",
    "                current_term = []\n",
    "    \n",
    "    if current_term:\n",
    "        predicted_terms.append(tokenizer.convert_tokens_to_string(current_term))\n",
    "    \n",
    "    # Clean predicted terms\n",
    "    predicted_terms = [term.replace(' ##', '').strip() for term in predicted_terms if term.strip()]\n",
    "    \n",
    "    return predicted_terms\n",
    "\n",
    "\n",
    "print(\"✓ Inference function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a9127b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on all dev sentences\n",
    "print(\"Running inference on dev set...\")\n",
    "bert_preds = []\n",
    "\n",
    "for i, sentence in enumerate(dev_sentences):\n",
    "    if i % 200 == 0:\n",
    "        print(f\"  Processing {i}/{len(dev_sentences)}\")\n",
    "    \n",
    "    predicted_terms = perform_inference(\n",
    "        inference_model,\n",
    "        inference_tokenizer,\n",
    "        sentence['sentence_text'],\n",
    "        id2label\n",
    "    )\n",
    "    bert_preds.append(predicted_terms)\n",
    "\n",
    "print(f\"✓ Inference completed: {len(bert_preds)} predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067a1d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare gold standard and predictions for evaluation\n",
    "dev_gold = [s['terms'] for s in dev_sentences]\n",
    "\n",
    "# Evaluate using competition metrics\n",
    "precision, recall, f1, tp, fp, fn = micro_f1_score(dev_gold, bert_preds)\n",
    "type_precision, type_recall, type_f1 = type_f1_score(dev_gold, bert_preds)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BERT TOKEN CLASSIFICATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nMicro-averaged Metrics:\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall:    {recall:.4f}\")\n",
    "print(f\"  F1 Score:  {f1:.4f}\")\n",
    "print(f\"  TP={tp}, FP={fp}, FN={fn}\")\n",
    "\n",
    "print(\"\\nType-level Metrics:\")\n",
    "print(f\"  Type Precision: {type_precision:.4f}\")\n",
    "print(f\"  Type Recall:    {type_recall:.4f}\")\n",
    "print(f\"  Type F1 Score:  {type_f1:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a1c3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions in competition format\n",
    "def save_predictions(predictions, sentences, output_path):\n",
    "    \"\"\"Save predictions in competition format.\"\"\"\n",
    "    output = {'data': []}\n",
    "    for pred, sent in zip(predictions, sentences):\n",
    "        output['data'].append({\n",
    "            'document_id': sent['document_id'],\n",
    "            'paragraph_id': sent['paragraph_id'],\n",
    "            'sentence_id': sent['sentence_id'],\n",
    "            'term_list': pred\n",
    "        })\n",
    "    \n",
    "    os.makedirs(os.path.dirname(output_path) or '.', exist_ok=True)\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"✓ Saved {len(predictions)} predictions to {output_path}\")\n",
    "\n",
    "\n",
    "save_predictions(bert_preds, dev_sentences, 'predictions/subtask_a_dev_bert_token_classification_preds.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6f6fcc",
   "metadata": {},
   "source": [
    "## Example Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8465b3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example predictions\n",
    "print(\"Example Predictions:\\n\")\n",
    "\n",
    "count = 0\n",
    "for i in range(len(dev_sentences)):\n",
    "    if len(dev_gold[i]) > 0 and count < 5:\n",
    "        print(f\"Sentence: {dev_sentences[i]['sentence_text'][:100]}...\")\n",
    "        print(f\"Gold terms: {dev_gold[i][:5]}\")\n",
    "        print(f\"BERT predictions: {bert_preds[i][:5]}\")\n",
    "        \n",
    "        correct = set(dev_gold[i]) & set(bert_preds[i])\n",
    "        missed = set(dev_gold[i]) - set(bert_preds[i])\n",
    "        wrong = set(bert_preds[i]) - set(dev_gold[i])\n",
    "        \n",
    "        print(f\"✓ Correct: {len(correct)}\")\n",
    "        print(f\"✗ Missed: {len(missed)}\")\n",
    "        print(f\"✗ Wrong: {len(wrong)}\")\n",
    "        print(\"-\"*80)\n",
    "        print()\n",
    "        \n",
    "        count += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ate-it",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

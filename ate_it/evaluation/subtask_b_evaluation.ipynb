{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicolaCirillo/ate-it/blob/main/evaluation/subtask_b_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rWN0iJhqUkmI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import codecs\n",
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "def load_data(file_path):\n",
        "  \"\"\"\n",
        "  Loads data from a CSV or JSON file and returns a dictionary\n",
        "  where keys are terms and values are cluster_ids.\n",
        "\n",
        "  Args:\n",
        "    file_path: The path to the input file (CSV or JSON).\n",
        "\n",
        "  Returns:\n",
        "    A dictionary containing the loaded data.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: If the file format is not supported.\n",
        "  \"\"\"\n",
        "  if file_path.endswith('.csv'):\n",
        "    # Load data from CSV file\n",
        "    df = pd.read_csv(file_path)\n",
        "    data = {term: int(cluster) for term, cluster in df.itertuples(index=False)}\n",
        "  elif file_path.endswith('.json'):\n",
        "    # Load data from JSON file\n",
        "    with codecs.open(file_path, 'r', 'utf-8') as f:\n",
        "      json_data = json.load(f)\n",
        "    # Extract terms from JSON data\n",
        "    data = {item[\"term\"]: item[\"cluster\"] for item in json_data[\"data\"]}\n",
        "  else:\n",
        "    # Raise error for unsupported file formats\n",
        "    raise ValueError(\"Unsupported file format. Only CSV and JSON files are supported.\")\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GBLQd1tCfnLL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class BCubed_calculator:\n",
        "  def __init__(self, gold, pred):\n",
        "    self.gold = gold\n",
        "    self.pred = pred\n",
        "    self.gold_cluster = defaultdict(set)\n",
        "    self.pred_cluster = defaultdict(set)\n",
        "    for item, clus_id in gold.items():\n",
        "        self.gold_cluster[clus_id].add(item)\n",
        "    for item, clus_id in pred.items():\n",
        "      self.pred_cluster[clus_id].add(item)\n",
        "\n",
        "  def bc_precision_item(self, item):\n",
        "    pred_id = self.pred[item]\n",
        "    gold_id = self.gold.get(item, None)\n",
        "    TP = len(self.pred_cluster[pred_id].intersection(self.gold_cluster[gold_id]))\n",
        "    FP = len(self.pred_cluster[pred_id]) - TP\n",
        "    return TP/(FP + TP)\n",
        "\n",
        "  def bc_recall_item(self, item):\n",
        "    pred_id = self.pred.get(item, None)\n",
        "    gold_id = self.gold.get(item)\n",
        "    TP = len(self.pred_cluster[pred_id].intersection(self.gold_cluster[gold_id]))\n",
        "    FN = len(self.gold_cluster[gold_id]) - TP\n",
        "    return TP/(TP + FN)\n",
        "\n",
        "def bcubed_precision(gold, pred):\n",
        "  calc = BCubed_calculator(gold, pred)\n",
        "  return np.average([calc.bc_precision_item(item) for item in calc.pred])\n",
        "\n",
        "def bcubed_recall(gold, pred):\n",
        "  calc = BCubed_calculator(gold, pred)\n",
        "  return np.average([calc.bc_recall_item(item) for item in calc.gold])\n",
        "\n",
        "def bcubed_f1(gold, pred):\n",
        "  return 2 * bcubed_precision(gold, pred) * bcubed_recall(gold, pred) / (bcubed_precision(gold, pred) + bcubed_recall(gold, pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5xOrBJx6gRPH"
      },
      "outputs": [],
      "source": [
        "SYSTEM_OUTPUT_PATH = \"C:/Users/marti/Desktop/ATA_Didattica_Integrativa/My_Lectures/01/ate_it/baseline/baseline_b_1.json\"\n",
        "SYSTEM_OUTPUT_PATH = \"C:/Users/marti/Desktop/ATA_Didattica_Integrativa/My_Lectures/01/ate_it/src/predictions/subtask_b_dev_vanilla_preds.csv\"\n",
        "SYSTEM_OUTPUT_PATH = \"C:/Users/marti/Desktop/ATA_Didattica_Integrativa/My_Lectures/01/ate_it/src/predictions/subtask_b_dev_spacy_baseline_preds.csv\"\n",
        "SYSTEM_OUTPUT_PATH = \"C:/Users/marti/Desktop/ATA_Didattica_Integrativa/My_Lectures/01/ate_it/src/predictions/subtask_b_dev_spacy_kmeans_preds.csv\"\n",
        "SYSTEM_OUTPUT_PATH = \"C:/Users/marti/Desktop/ATA_Didattica_Integrativa/My_Lectures/01/ate_it/src/predictions/subtask_b_dev_nltk_baseline_preds.csv\"\n",
        "SYSTEM_OUTPUT_PATH = \"C:/Users/marti/Desktop/ATA_Didattica_Integrativa/My_Lectures/01/ate_it/src/predictions/subtask_b_dev_nltk_tfidf_preds.csv\"\n",
        "SYSTEM_OUTPUT_PATH = \"C:/Users/marti/Desktop/ATA_Didattica_Integrativa/My_Lectures/01/ate_it/src/predictions/subtask_b_dev_bert_baseline_preds.csv\"\n",
        "#SYSTEM_OUTPUT_PATH = \"C:/Users/marti/Desktop/ATA_Didattica_Integrativa/My_Lectures/01/ate_it/src/predictions/subtask_b_dev_bert_kmeans_preds.csv\"\n",
        "\n",
        "GOLD_STANDARD_PATH = \"C:/Users/marti/Desktop/ATA_Didattica_Integrativa/My_Lectures/01/ate_it/data/subtask_b_dev.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fmBbtv4YVdde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BCubed Precision: 0.743\n",
            "BCubed Recall: 0.862\n",
            "BCubed F1 Score: 0.798\n"
          ]
        }
      ],
      "source": [
        "system_output = load_data(SYSTEM_OUTPUT_PATH)\n",
        "gold_standard = load_data(GOLD_STANDARD_PATH)\n",
        "\n",
        "precision = bcubed_precision(gold_standard, system_output)\n",
        "recall = bcubed_recall(gold_standard, system_output)\n",
        "f1 = bcubed_f1(gold_standard, system_output)\n",
        "\n",
        "print(f\"BCubed Precision: {precision:.3f}\")\n",
        "print(f\"BCubed Recall: {recall:.3f}\")\n",
        "print(f\"BCubed F1 Score: {f1:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "XS0sdnZepXBw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating subtask_b_dev_bert_baseline_preds.csv...\n",
            "Evaluating subtask_b_dev_bert_kmeans_preds.csv...\n",
            "Evaluating subtask_b_dev_nltk_baseline_preds.csv...\n",
            "Evaluating subtask_b_dev_nltk_tfidf_preds.csv...\n",
            "Evaluating subtask_b_dev_spacy_baseline_preds.csv...\n",
            "Evaluating subtask_b_dev_spacy_kmeans_preds.csv...\n",
            "Evaluating subtask_b_dev_vanilla_preds.csv...\n",
            "Evaluating baseline_b_1.json...\n",
            "\n",
            "====================================================================================================\n",
            "EVALUATION RESULTS\n",
            "====================================================================================================\n",
            "     file_name  bcubed_precision  bcubed_recall  bcubed_f1\n",
            "    nltk_tfidf             0.800          0.941      0.865\n",
            "spacy_baseline             0.755          0.873      0.810\n",
            " bert_baseline             0.743          0.862      0.798\n",
            "       vanilla             0.656          0.870      0.748\n",
            " nltk_baseline             0.634          0.861      0.730\n",
            "  spacy_kmeans             0.563          0.773      0.651\n",
            "   bert_kmeans             0.539          0.774      0.635\n",
            "  baseline_b_1             0.332          0.950      0.492\n",
            "\n",
            "Results saved to subtask_b_evaluation_results.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "# Path to predictions directory and baseline\n",
        "predictions_dir = \"../src/predictions/\"\n",
        "baseline_file = \"../baseline/baseline_b_1.json\"\n",
        "gold_standard_file = \"../data/subtask_b_dev.json\"\n",
        "\n",
        "# Get all CSV files in the predictions directory\n",
        "prediction_files = glob.glob(os.path.join(predictions_dir, \"*.csv\"))\n",
        "\n",
        "# Add baseline file to the list\n",
        "prediction_files.append(baseline_file)\n",
        "\n",
        "# Load gold standard once\n",
        "gold_standard = load_data(gold_standard_file)\n",
        "\n",
        "# Store results\n",
        "results = []\n",
        "\n",
        "# Evaluate each prediction file\n",
        "for pred_file in prediction_files:\n",
        "    if pred_file.find(\"subtask_b_dev_\") == -1 and pred_file != baseline_file:\n",
        "        continue\n",
        "    file_name = os.path.basename(pred_file)\n",
        "    print(f\"Evaluating {file_name}...\")\n",
        "    \n",
        "    try:\n",
        "        # Load system output\n",
        "        system_output = load_data(pred_file)\n",
        "\n",
        "        # Calculate bcubed metrics\n",
        "        precision, recall, f1 = bcubed_precision(gold_standard, system_output), bcubed_recall(gold_standard, system_output), bcubed_f1(gold_standard, system_output)\n",
        "        \n",
        "        # Store results with cleaned file name\n",
        "        clean_name = file_name.replace(\"subtask_b_dev_\", \"\").replace(\"_preds\", \"\").replace(\".json\", \"\").replace(\".csv\", \"\")\n",
        "        results.append({\n",
        "            'file_name': clean_name,\n",
        "            'bcubed_precision': round(precision, 3),\n",
        "            'bcubed_recall': round(recall, 3),\n",
        "            'bcubed_f1': round(f1, 3)\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"Error evaluating {file_name}: {e}\")\n",
        "\n",
        "# Create DataFrame and sort by bcubed F1 score (descending)\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df = results_df.sort_values('bcubed_f1', ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Display the table\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"EVALUATION RESULTS\")\n",
        "print(\"=\"*100)\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "# Save to CSV\n",
        "output_csv = \"subtask_b_evaluation_results.csv\"\n",
        "results_df.to_csv(output_csv, index=False)\n",
        "print(f\"\\nResults saved to {output_csv}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOr+Ciwknqt8vp/IQu64kGB",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ate-it",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

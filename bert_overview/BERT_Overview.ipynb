{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c5f27fbd",
      "metadata": {
        "id": "c5f27fbd"
      },
      "source": [
        "\n",
        "# BERT Overview\n",
        "\n",
        "### What you'll learn\n",
        "- How tokenization works with BERT (wordpiece, padding strategies, and the **attention mask**).\n",
        "- How to inspect BERT: configuration, architecture, vocabulary, special tokens, and **[unused]** tokens.\n",
        "- How to use BERT for its original tasks: Masked Language Modelling and Next Sentence Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c64c58cc",
      "metadata": {
        "id": "c64c58cc"
      },
      "source": [
        "## 1) Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d6936f7b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6936f7b",
        "outputId": "1e793045-c666-4601-b269-b908e4f52d58"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\marti\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "# If transformers/torch aren't installed in your environment, uncomment and run:\n",
        "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "# !pip install transformers>=4.41.0 datasets==2.19.0\n",
        "\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "import json\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Tuple\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "\n",
        "from transformers import BertTokenizerFast, BertConfig, BertModel, AutoConfig\n",
        "from transformers.optimization import get_linear_schedule_with_warmup\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56f34f45",
      "metadata": {
        "id": "56f34f45"
      },
      "source": [
        "## 2) Tokenizer: effects, padding, attention mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b06f4f58",
      "metadata": {
        "id": "b06f4f58"
      },
      "outputs": [],
      "source": [
        "# Load the tokenizer\n",
        "MODEL_NAME = \"bert-base-uncased\"  \n",
        "tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME, use_fast=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "D5FwkF3nrmOa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5FwkF3nrmOa",
        "outputId": "45725e24-6e9c-4183-9093-e030032c1976"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max length: 512\n",
            "Vocab size: 30522\n",
            "Special tokens: ['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']\n",
            "[UNK] -> 100\n",
            "[SEP] -> 102\n",
            "[PAD] -> 0\n",
            "[CLS] -> 101\n",
            "[MASK] -> 103\n"
          ]
        }
      ],
      "source": [
        "# Inspect vocab & special tokens\n",
        "print(\"Max length:\", tokenizer.model_max_length)\n",
        "print(\"Vocab size:\", tokenizer.vocab_size)\n",
        "print(\"Special tokens:\", tokenizer.all_special_tokens)\n",
        "for token_id in tokenizer.all_special_ids:\n",
        "    print(tokenizer.decode(token_id), \"->\", token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "AW_E7TP3rpUF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AW_E7TP3rpUF",
        "outputId": "99a77650-0af0-42a8-f4e7-cdcee8f3d99b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 10 [unused] tokens: ['[unused0]', '[unused1]', '[unused2]', '[unused3]', '[unused4]', '[unused5]', '[unused6]', '[unused7]', '[unused8]', '[unused9]']\n",
            "Last 10 [unused] tokens: ['[unused984]', '[unused985]', '[unused986]', '[unused987]', '[unused988]', '[unused989]', '[unused990]', '[unused991]', '[unused992]', '[unused993]']\n"
          ]
        }
      ],
      "source": [
        "# Show a few [unused] tokens (BERT ships many)\n",
        "unused = [t for t in tokenizer.get_vocab().keys() if t.startswith(\"[unused\")]\n",
        "unused_digits = [int(t.replace(\"unused\", \"\").replace(\"[\", \"\").replace(\"]\", \"\")) for t in unused]\n",
        "unused_digits = sorted(unused_digits)\n",
        "unused = [f\"[unused{i}]\" for i in unused_digits]\n",
        "print(\"First 10 [unused] tokens:\", unused[:10])\n",
        "print(\"Last 10 [unused] tokens:\", unused[-10:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "X_2KbN7Zrtgp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_2KbN7Zrtgp",
        "outputId": "52dd22ec-0b0d-448c-f170-4ea3bd50dec1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original text: Tokenization and padding are fundamental steps in most NLP pipelines.\n",
            "Tokenized: ['[CLS]', 'token', '##ization', 'and', 'pad', '##ding', 'are', 'fundamental', 'steps', 'in', 'most', 'nl', '##p', 'pipeline', '##s', '.', '[SEP]']\n",
            "Num of tokens: 17\n",
            "\n",
            "\n",
            "input_ids torch.Size([1, 17]) tensor([  101, 19204,  3989,  1998, 11687,  4667,  2024,  8050,  4084,  1999,\n",
            "         2087, 17953,  2361, 13117,  2015,  1012,   102])\n",
            "token_type_ids torch.Size([1, 17]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "attention_mask torch.Size([1, 17]) tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
            "\n",
            "\n",
            "\n",
            "|    | token       |   token_id |   attention_mask_id |\n",
            "|---:|:------------|-----------:|--------------------:|\n",
            "|  0 | [CLS]       |        101 |                   1 |\n",
            "|  1 | token       |      19204 |                   1 |\n",
            "|  2 | ##ization   |       3989 |                   1 |\n",
            "|  3 | and         |       1998 |                   1 |\n",
            "|  4 | pad         |      11687 |                   1 |\n",
            "|  5 | ##ding      |       4667 |                   1 |\n",
            "|  6 | are         |       2024 |                   1 |\n",
            "|  7 | fundamental |       8050 |                   1 |\n",
            "|  8 | steps       |       4084 |                   1 |\n",
            "|  9 | in          |       1999 |                   1 |\n",
            "| 10 | most        |       2087 |                   1 |\n",
            "| 11 | nl          |      17953 |                   1 |\n",
            "| 12 | ##p         |       2361 |                   1 |\n",
            "| 13 | pipeline    |      13117 |                   1 |\n",
            "| 14 | ##s         |       2015 |                   1 |\n",
            "| 15 | .           |       1012 |                   1 |\n",
            "| 16 | [SEP]       |        102 |                   1 |\n"
          ]
        }
      ],
      "source": [
        "#sample = \"I absolutely loved the cinematography, but the acting was so-so.\"\n",
        "sample = \"Tokenization and padding are fundamental steps in most NLP pipelines.\"\n",
        "enc = tokenizer(sample, return_tensors=\"pt\")\n",
        "\n",
        "print(\"Original text:\", sample)\n",
        "tokens = []\n",
        "for token_id in enc[\"input_ids\"][0].tolist():\n",
        "    tokens.append(tokenizer.decode(token_id))\n",
        "print(\"Tokenized:\", tokens)\n",
        "print(\"Num of tokens:\", len(tokens))\n",
        "\n",
        "print(\"\\n\")\n",
        "for k,v in enc.items():\n",
        "    print(k, v.shape, v[0])\n",
        "\n",
        "tokens = []\n",
        "for token_id, attention_mask_id in zip(enc[\"input_ids\"][0], enc[\"attention_mask\"][0]):\n",
        "    tokens.append({\n",
        "        \"token\": tokenizer.decode(token_id),\n",
        "        \"token_id\": token_id,\n",
        "        \"attention_mask_id\": attention_mask_id\n",
        "    })\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "print(pd.DataFrame.from_dict(tokens).to_markdown())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "pcdkE-qSsuiG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcdkE-qSsuiG",
        "outputId": "448a78e2-06fd-4faf-e821-8c0674294dfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoding strings with different lengths without padding gives error!\n",
            "Error: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
            "\n",
            "--- Padding to a fixed `max_length` (e.g., 16) ---\n",
            "Purpose: Ensures all sequences have the exact same length, often used for fixed-size model inputs.\n",
            "Input IDs shape: torch.Size([3, 16])\n",
            "Attention Mask shape: torch.Size([3, 16])\n",
            "First sequence input_ids (padded):\n",
            "|    | token   |   token_id |   attention_mask_id |\n",
            "|---:|:--------|-----------:|--------------------:|\n",
            "|  0 | [CLS]   |        101 |                   1 |\n",
            "|  1 | this    |       2023 |                   1 |\n",
            "|  2 | movie   |       3185 |                   1 |\n",
            "|  3 | was     |       2001 |                   1 |\n",
            "|  4 | amazing |       6429 |                   1 |\n",
            "|  5 | !       |        999 |                   1 |\n",
            "|  6 | [SEP]   |        102 |                   1 |\n",
            "|  7 | [PAD]   |          0 |                   0 |\n",
            "|  8 | [PAD]   |          0 |                   0 |\n",
            "|  9 | [PAD]   |          0 |                   0 |\n",
            "| 10 | [PAD]   |          0 |                   0 |\n",
            "| 11 | [PAD]   |          0 |                   0 |\n",
            "| 12 | [PAD]   |          0 |                   0 |\n",
            "| 13 | [PAD]   |          0 |                   0 |\n",
            "| 14 | [PAD]   |          0 |                   0 |\n",
            "| 15 | [PAD]   |          0 |                   0 |\n",
            "\n",
            "\n",
            "--- Padding to the `longest` sequence in the current batch ---\n",
            "Purpose: Minimizes padding by only matching the longest sequence in the batch, saving computation.\n",
            "Input IDs shape: torch.Size([3, 11])\n",
            "Attention Mask shape: torch.Size([3, 11])\n",
            "|    | token    |   token_id |   attention_mask_id |\n",
            "|---:|:---------|-----------:|--------------------:|\n",
            "|  0 | [CLS]    |        101 |                   1 |\n",
            "|  1 | i        |       1045 |                   1 |\n",
            "|  2 | would    |       2052 |                   1 |\n",
            "|  3 | not      |       2025 |                   1 |\n",
            "|  4 | watch    |       3422 |                   1 |\n",
            "|  5 | it       |       2009 |                   1 |\n",
            "|  6 | again    |       2153 |                   1 |\n",
            "|  7 | ,        |       1010 |                   1 |\n",
            "|  8 | honestly |       9826 |                   1 |\n",
            "|  9 | .        |       1012 |                   1 |\n",
            "| 10 | [SEP]    |        102 |                   1 |\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Padding strategies and attention mask\n",
        "batch = [\n",
        "    \"This movie was amazing!\",\n",
        "    \"Bad.\",\n",
        "    \"I would not watch it again, honestly.\",\n",
        "]\n",
        "\n",
        "\n",
        "try:\n",
        "    enc_nopad = tokenizer(batch, padding=False, truncation=True, return_tensors=\"pt\")\n",
        "except Exception as e:\n",
        "    print(\"Encoding strings with different lengths without padding gives error!\")\n",
        "    print(\"Error:\", e)\n",
        "\n",
        "print(\"\\n--- Padding to a fixed `max_length` (e.g., 16) ---\")\n",
        "print(\"Purpose: Ensures all sequences have the exact same length, often used for fixed-size model inputs.\")\n",
        "enc_pad_left = tokenizer(batch, padding=\"max_length\", max_length=16, truncation=True, return_tensors=\"pt\") # if we don't specify the value for max length, it will take by default the one of the model\n",
        "print(\"Input IDs shape:\", enc_pad_left[\"input_ids\"].shape)\n",
        "print(\"Attention Mask shape:\", enc_pad_left[\"attention_mask\"].shape)\n",
        "print(\"First sequence input_ids (padded):\")\n",
        "tokens = []\n",
        "for token_id, attention_mask_id in zip(enc_pad_left[\"input_ids\"][0], enc_pad_left[\"attention_mask\"][0]):\n",
        "    tokens.append({\n",
        "        \"token\": tokenizer.decode(token_id),\n",
        "        \"token_id\": token_id,\n",
        "        \"attention_mask_id\": attention_mask_id\n",
        "    })\n",
        "print(pd.DataFrame.from_dict(tokens).to_markdown())\n",
        "\n",
        "print(\"\\n\\n--- Padding to the `longest` sequence in the current batch ---\")\n",
        "print(\"Purpose: Minimizes padding by only matching the longest sequence in the batch, saving computation.\")\n",
        "enc_pad_longest = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "print(\"Input IDs shape:\", enc_pad_longest[\"input_ids\"].shape)\n",
        "print(\"Attention Mask shape:\", enc_pad_longest[\"attention_mask\"].shape)\n",
        "tokens = []\n",
        "for token_id, attention_mask_id in zip(enc_pad_longest[\"input_ids\"][2], enc_pad_longest[\"attention_mask\"][2]):\n",
        "    tokens.append({\n",
        "        \"token\": tokenizer.decode(token_id),\n",
        "        \"token_id\": token_id,\n",
        "        \"attention_mask_id\": attention_mask_id\n",
        "    })\n",
        "print(pd.DataFrame.from_dict(tokens).to_markdown())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bff7b025",
      "metadata": {
        "id": "bff7b025"
      },
      "source": [
        "\n",
        "### Why attention masks matter\n",
        "\n",
        "Let's pass two padded sequences through BERT **with** and **without** the attention mask and see how the outputs change.  \n",
        "Padding tokens should **not** influence the representation of real tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f56b5c1a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f56b5c1a",
        "outputId": "1e64567c-2456-44db-b0d3-487dbd1cdaf5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "L2 distance between CLS with/without mask per sequence: [0.0, 2.567479133605957]\n"
          ]
        }
      ],
      "source": [
        "config = BertConfig.from_pretrained(MODEL_NAME, output_hidden_states=True)\n",
        "bert = BertModel.from_pretrained(MODEL_NAME, config=config).to(device)\n",
        "bert.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    padded = tokenizer([\"hello world\", \"hello\"], padding=True, return_tensors=\"pt\").to(device)\n",
        "    #padded = tokenizer([\"hello world\", \"hello\"], padding=\"max_length\", max_length=50, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # With mask\n",
        "    out_with = bert(input_ids=padded[\"input_ids\"], attention_mask=padded[\"attention_mask\"])\n",
        "    # Without mask (pretend padding is real content)\n",
        "    out_without = bert(input_ids=padded[\"input_ids\"], attention_mask=None)\n",
        "\n",
        "# Compare [CLS] embeddings difference\n",
        "cls_with = out_with.last_hidden_state[:,0,:]\n",
        "cls_without = out_without.last_hidden_state[:,0,:]\n",
        "diff = (cls_with - cls_without).pow(2).sum(dim=-1).sqrt().cpu().tolist()\n",
        "print(\"L2 distance between CLS with/without mask per sequence:\", diff)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c884f905",
      "metadata": {
        "id": "c884f905"
      },
      "source": [
        "## 3) BERT architecture: modules, shapes, and parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "44bd51c1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44bd51c1",
        "outputId": "b18bb1ff-2fbb-4a4e-9f41-2a30bb14e0bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total params in bert-base-uncased: 109,482,240 (trainable: 109,482,240)\n",
            "\n",
            "High-level modules:\n",
            " - embeddings : BertEmbeddings\n",
            " - encoder : BertEncoder\n",
            " - pooler : BertPooler\n",
            "\n",
            "Encoder layer stack depth: 12\n",
            "Hidden size: 768\n",
            "Intermediate FF size: 3072\n",
            "Attention heads: 12\n"
          ]
        }
      ],
      "source": [
        "total_params = sum(p.numel() for p in bert.parameters())\n",
        "trainable_params = sum(p.numel() for p in bert.parameters() if p.requires_grad)\n",
        "print(f\"Total params in {MODEL_NAME}: {total_params:,} (trainable: {trainable_params:,})\")\n",
        "\n",
        "print(\"\\nHigh-level modules:\")\n",
        "for name, module in bert.named_children():\n",
        "    print(\" -\", name, \":\", module.__class__.__name__)\n",
        "\n",
        "print(\"\\nEncoder layer stack depth:\", bert.config.num_hidden_layers)\n",
        "print(\"Hidden size:\", bert.config.hidden_size)\n",
        "print(\"Intermediate FF size:\", bert.config.intermediate_size)\n",
        "print(\"Attention heads:\", bert.config.num_attention_heads)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd34fe2e",
      "metadata": {},
      "source": [
        "## 4) BERT's original pre-training tasks\n",
        "\n",
        "BERT was originally pre-trained on two tasks:\n",
        "1. **Masked Language Modeling (MLM)**: Predict masked tokens in a sentence.\n",
        "2. **Next Sentence Prediction (NSP)**: Determine if sentence B follows sentence A.\n",
        "\n",
        "Let's demonstrate both tasks using a pre-trained BERT model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ff5e189c",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[MASK] -> 103\n",
            "Predicted token id: 3000 -> paris\n",
            "Tokens: ['[CLS]', 'the', 'capital', 'of', 'france', 'is', 'paris', '.', '[SEP]']\n",
            "Labels before masking non-[MASK]: tensor([[ 101, 1996, 3007, 1997, 2605, 2003, 3000, 1012,  102]])\n",
            "Labels after masking non-[MASK]: tensor([[-100, -100, -100, -100, -100, -100, 3000, -100, -100]])\n",
            "Loss: 0.88\n"
          ]
        }
      ],
      "source": [
        "# Masked Language Modeling (MLM) Demo\n",
        "# We need BertForMaskedLM for this task\n",
        "from transformers import AutoTokenizer, BertForMaskedLM\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "print(tokenizer.mask_token, \"->\", tokenizer.mask_token_id)\n",
        "inputs = tokenizer(\"The capital of France is [MASK].\", return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Get model predictions (logits) for all tokens in the vocabulary\n",
        "    # logits shape: [batch_size=1, sequence_length, vocab_size=30522]\n",
        "    logits = model(**inputs).logits\n",
        "\n",
        "# Step 1: Find the position of [MASK] in the input sequence\n",
        "mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
        "\n",
        "# Step 2: Get the predicted token for the [MASK] position\n",
        "# logits[0, mask_token_index] extracts predictions at the mask position (shape: [vocab_size])\n",
        "# .argmax(axis=-1) finds the token ID with the highest score (most likely prediction)\n",
        "predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
        "\n",
        "print(\"Predicted token id:\", predicted_token_id.item(), \"->\", tokenizer.decode(predicted_token_id))  \n",
        "\n",
        "# Step 3: Prepare labels for loss calculation\n",
        "# Tokenize the correct sentence to get ground truth token IDs\n",
        "labels = tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\")[\"input_ids\"]\n",
        "\n",
        "# Set all non-[MASK] positions to -100 (tells PyTorch to ignore them in loss calculation)\n",
        "# Only the token at the [MASK] position will be used to compute the loss\n",
        "# Result: [-100, -100, -100, -100, -100, -100, 3000, -100] where 3000 is \"paris\"\n",
        "print(\"Tokens:\", tokenizer.convert_ids_to_tokens(labels[0]))\n",
        "print(\"Labels before masking non-[MASK]:\", labels)\n",
        "labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)\n",
        "print(\"Labels after masking non-[MASK]:\", labels)\n",
        "\n",
        "# Step 4: Calculate loss (how wrong was the prediction?)\n",
        "# The model compares its prediction at [MASK] position with the correct token \"Paris\"\n",
        "# Lower loss = better prediction (0 would mean perfect prediction)\n",
        "outputs = model(**inputs, labels=labels)\n",
        "print(f\"Loss: {round(outputs.loss.item(), 2)}\")  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e3477a7",
      "metadata": {},
      "source": [
        "### Understanding the Loss Value\n",
        "\n",
        "Loss measures **confidence**, not just correctness!\n",
        "\n",
        "- **Loss = -log(probability)** for the correct token\n",
        "- Loss of 0.88 means BERT gave \"paris\" a probability of ~41% (exp(-0.88) ≈ 0.41)\n",
        "- Even though \"paris\" was the top prediction, BERT wasn't 100% confident\n",
        "- BERT must choose from **30,522 possible tokens** in the vocabulary!\n",
        "\n",
        "**Loss interpretation:**\n",
        "- Loss = 0.0 → 100% confident (probability = 1.0) - perfect!\n",
        "- Loss = 0.69 → 50% confident (probability = 0.5)\n",
        "- Loss = 1.0 → 37% confident (probability = 0.37)\n",
        "- Loss = 2.0 → 14% confident (probability = 0.14)\n",
        "\n",
        "Let's see the actual probabilities for the top predictions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "89e89218",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 10 predictions for 'The capital of France is [MASK].':\n",
            "\n",
            "Rank   Token           Probability     Loss if correct     \n",
            "------------------------------------------------------------\n",
            "1      paris           0.4168          0.8752               ← PREDICTED\n",
            "2      lille           0.0714          2.6392              \n",
            "3      lyon            0.0634          2.7584              \n",
            "4      marseille       0.0444          3.1134              \n",
            "5      tours           0.0303          3.4967              \n",
            "6      toulouse        0.0288          3.5489              \n",
            "7      orleans         0.0254          3.6717              \n",
            "8      nantes          0.0228          3.7809              \n",
            "9      brest           0.0226          3.7915              \n",
            "10     bordeaux        0.0212          3.8526              \n",
            "\n",
            "============================================================\n",
            "Notice: Even though 'paris' is #1, it has ~41.7% probability\n",
            "The model spreads probability across other plausible tokens.\n",
            "This is why the loss is 0.88, not 0.0!\n"
          ]
        }
      ],
      "source": [
        "# Let's look at the actual probabilities for top-10 predictions\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Get probabilities (not just logits)\n",
        "probs_at_mask = F.softmax(logits[0, mask_token_index], dim=-1)\n",
        "\n",
        "# Get top 10 predictions with their probabilities\n",
        "top_k = 10\n",
        "top_probs, top_indices = torch.topk(probs_at_mask, top_k, dim=-1)\n",
        "\n",
        "print(f\"Top {top_k} predictions for 'The capital of France is [MASK].':\\n\")\n",
        "print(f\"{'Rank':<6} {'Token':<15} {'Probability':<15} {'Loss if correct':<20}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for rank, (prob, idx) in enumerate(zip(top_probs[0], top_indices[0]), 1):\n",
        "    token = tokenizer.decode([idx])\n",
        "    prob_value = prob.item()\n",
        "    loss_value = -torch.log(prob).item()\n",
        "    \n",
        "    # Highlight the actual prediction (paris)\n",
        "    marker = \" ← PREDICTED\" if rank == 1 else \"\"\n",
        "    print(f\"{rank:<6} {token:<15} {prob_value:<15.4f} {loss_value:<20.4f}{marker}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"Notice: Even though 'paris' is #1, it has ~{top_probs[0][0].item()*100:.1f}% probability\")\n",
        "print(f\"The model spreads probability across other plausible tokens.\")\n",
        "print(f\"This is why the loss is {outputs.loss.item():.2f}, not 0.0!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "354a1f93",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Next Sentence Prediction Demo\n",
            "\n",
            "Starting sentence: 'The weather is beautiful today.'\n",
            "\n",
            "--- True continuation ---\n",
            "Candidate sentence: 'I think I'll go for a walk in the park.'\n",
            "Prediction: IS the next sentence\n",
            "  P(is next)     = 0.9999\n",
            "  P(is NOT next) = 0.0001\n",
            "\n",
            "--- False continuation ---\n",
            "Candidate sentence: 'Deep Learning is a branch of Machine Learning.'\n",
            "Prediction: IS NOT the next sentence\n",
            "  P(is next)     = 0.0000\n",
            "  P(is NOT next) = 1.0000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Next Sentence Prediction (NSP) Demo\n",
        "# We need BertForNextSentencePrediction for this task\n",
        "from transformers import BertForNextSentencePrediction\n",
        "\n",
        "nsp_model = BertForNextSentencePrediction.from_pretrained(MODEL_NAME).to(device)\n",
        "nsp_model.eval()\n",
        "\n",
        "# Test sentences\n",
        "starting_sentence = \"The weather is beautiful today.\"\n",
        "true_next = \"I think I'll go for a walk in the park.\"\n",
        "false_next = \"Deep Learning is a branch of Machine Learning.\"\n",
        "\n",
        "sentence_pairs = [\n",
        "    (starting_sentence, true_next, \"True continuation\"),\n",
        "    (starting_sentence, false_next, \"False continuation\"),\n",
        "]\n",
        "\n",
        "print(\"Next Sentence Prediction Demo\\n\")\n",
        "print(f\"Starting sentence: '{starting_sentence}'\\n\")\n",
        "\n",
        "for sent_a, sent_b, description in sentence_pairs:\n",
        "    # Tokenize the sentence pair\n",
        "    # BERT uses [CLS] sent_a [SEP] sent_b [SEP] format\n",
        "    inputs = tokenizer(sent_a, sent_b, return_tensors=\"pt\").to(device)\n",
        "    \n",
        "    # Get predictions\n",
        "    with torch.no_grad():\n",
        "        outputs = nsp_model(**inputs)\n",
        "        logits = outputs.logits\n",
        "    \n",
        "    # logits shape: [batch_size, 2]\n",
        "    # logits[:, 0] = score for \"is next sentence\"\n",
        "    # logits[:, 1] = score for \"is NOT next sentence\"\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    is_next_prob = probs[0, 0].item()\n",
        "    not_next_prob = probs[0, 1].item()\n",
        "    \n",
        "    prediction = \"IS the next sentence\" if is_next_prob > not_next_prob else \"IS NOT the next sentence\"\n",
        "    \n",
        "    print(f\"--- {description} ---\")\n",
        "    print(f\"Candidate sentence: '{sent_b}'\")\n",
        "    print(f\"Prediction: {prediction}\")\n",
        "    print(f\"  P(is next)     = {is_next_prob:.4f}\")\n",
        "    print(f\"  P(is NOT next) = {not_next_prob:.4f}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98909a26",
      "metadata": {},
      "source": [
        "## 5) References\n",
        "- Many other BertFor** are supported by transformers. You can check the full list at https://huggingface.co/docs/transformers/model_doc/bert\n",
        "- Check out also the \"Auto\" classes, which allows for more flexible pipelines integrating BERT and non-BERT models without changing the code: https://huggingface.co/docs/transformers/model_doc/auto"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4c50701",
      "metadata": {
        "id": "b4c50701"
      },
      "source": [
        "## 6) Exercises \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9575b367",
      "metadata": {},
      "source": [
        "### Exercise 1: Tokenization Analysis\n",
        "\n",
        "**Task:** Tokenize the following sentence and analyze its components:\n",
        "```\n",
        "\"The pre-trained BERT model uses WordPiece tokenization with 30,522 tokens.\"\n",
        "```\n",
        "\n",
        "**Requirements:**\n",
        "1. Tokenize the sentence and print the tokens\n",
        "2. Calculate the total number of tokens (including special tokens)\n",
        "3. Identify which words got split into subwords\n",
        "4. Create a DataFrame showing: token, token_id, and whether it's a subword (starts with ##)\n",
        "\n",
        "**Bonus:** Try with a sentence containing a rare word like \"antidisestablishmentarianism\" and observe the tokenization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "2ed5befc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens: ['[CLS]', 'the', 'pre', '-', 'trained', 'bert', 'model', 'uses', 'word', '##piece', 'token', '##ization', 'with', '30', ',', '52', '##2', 'token', '##s', '.', '[SEP]']\n",
            "\n",
            "Total number of tokens: 21\n",
            "\n",
            "Words split into subwords:\n",
            "  'wordpiece' was split into 'word' + '##piece'\n",
            "  'tokenization' was split into 'token' + '##ization'\n",
            "  '522' was split into '52' + '##2'\n",
            "  'tokens' was split into 'token' + '##s'\n",
            "\n",
            "| token     |   token_id | is_subword   |\n",
            "|:----------|-----------:|:-------------|\n",
            "| [CLS]     |        101 | False        |\n",
            "| the       |       1996 | False        |\n",
            "| pre       |       3653 | False        |\n",
            "| -         |       1011 | False        |\n",
            "| trained   |       4738 | False        |\n",
            "| bert      |      14324 | False        |\n",
            "| model     |       2944 | False        |\n",
            "| uses      |       3594 | False        |\n",
            "| word      |       2773 | False        |\n",
            "| ##piece   |      11198 | True         |\n",
            "| token     |      19204 | False        |\n",
            "| ##ization |       3989 | True         |\n",
            "| with      |       2007 | False        |\n",
            "| 30        |       2382 | False        |\n",
            "| ,         |       1010 | False        |\n",
            "| 52        |       4720 | False        |\n",
            "| ##2       |       2475 | True         |\n",
            "| token     |      19204 | False        |\n",
            "| ##s       |       2015 | True         |\n",
            "| .         |       1012 | False        |\n",
            "| [SEP]     |        102 | False        |\n",
            "\n",
            "--- BONUS: Rare word ---\n",
            "Tokens: ['[CLS]', 'anti', '##dis', '##est', '##ab', '##lish', '##ment', '##arian', '##ism', 'is', 'a', 'very', 'long', 'word', '.', '[SEP]']\n",
            "The word 'antidisestablishmentarianism' was split into 8 subword tokens!\n"
          ]
        }
      ],
      "source": [
        "# Solution to Exercise 1\n",
        "sentence = \"The pre-trained BERT model uses WordPiece tokenization with 30,522 tokens.\"\n",
        "#sentence = \"Antidisestablishmentarianism is a political position that opposes the withdrawal of state support from an established church.\"\n",
        "\n",
        "# 1. Tokenize and print tokens\n",
        "enc = tokenizer(sentence, return_tensors=\"pt\")\n",
        "tokens = [tokenizer.decode([token_id]) for token_id in enc[\"input_ids\"][0]]\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# 2. Calculate total number of tokens\n",
        "print(f\"\\nTotal number of tokens: {len(tokens)}\")\n",
        "\n",
        "# 3. Identify split words\n",
        "print(\"\\nWords split into subwords:\")\n",
        "for i, token in enumerate(tokens):\n",
        "    if token.startswith(\"##\"):\n",
        "        print(f\"  '{tokens[i-1]}{token[2:]}' was split into '{tokens[i-1]}' + '{token}'\")\n",
        "\n",
        "# 4. Create DataFrame\n",
        "import pandas as pd\n",
        "token_data = []\n",
        "for token_id in enc[\"input_ids\"][0]:\n",
        "    token = tokenizer.decode([token_id])\n",
        "    is_subword = token.startswith(\"##\")\n",
        "    token_data.append({\n",
        "        \"token\": token,\n",
        "        \"token_id\": token_id.item(),\n",
        "        \"is_subword\": is_subword\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(token_data)\n",
        "print(\"\\n\" + df.to_markdown(index=False))\n",
        "\n",
        "# Bonus: Rare word tokenization\n",
        "print(\"\\n--- BONUS: Rare word ---\")\n",
        "rare_sentence = \"Antidisestablishmentarianism is a very long word.\"\n",
        "enc_rare = tokenizer(rare_sentence, return_tensors=\"pt\")\n",
        "rare_tokens = [tokenizer.decode([token_id]) for token_id in enc_rare[\"input_ids\"][0]]\n",
        "print(\"Tokens:\", rare_tokens)\n",
        "print(f\"The word 'antidisestablishmentarianism' was split into {len([t for t in rare_tokens if 'anti' in t or '##' in t])} subword tokens!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dc8ca5c",
      "metadata": {},
      "source": [
        "### Exercise 2: Padding Strategy Comparison\n",
        "\n",
        "**Task:** Compare different padding strategies.\n",
        "\n",
        "Given the following batch of sentences:\n",
        "```python\n",
        "sentences = [\n",
        "    \"Great!\",\n",
        "    \"This is a medium length sentence about machine learning.\",\n",
        "    \"Short one.\",\n",
        "    \"This is the longest sentence in our batch and it talks about natural language processing and transformers.\"\n",
        "]\n",
        "```\n",
        "\n",
        "**Requirements:**\n",
        "1. Tokenize with `padding=\"longest\"` and report the shape\n",
        "2. Tokenize with `padding=\"max_length\"` (max_length=64) and report the shape\n",
        "3. Calculate the percentage of padding tokens for each strategy\n",
        "4. Discuss: Which strategy would be better for training? Why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "bf524980",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Padding='longest' ---\n",
            "Shape: torch.Size([4, 20])\n",
            "\n",
            "--- Padding='max_length' (64) ---\n",
            "Shape: torch.Size([4, 64])\n",
            "\n",
            "Padding percentage (longest): 48.75%\n",
            "Padding percentage (max_length=64): 83.98%\n",
            "Efficiency gain: 35.23% less padding with 'longest'\n",
            "\n",
            "--- Discussion ---\n",
            "For training, 'longest' is generally better because:\n",
            "  • Reduces wasted computation on padding tokens\n",
            "  • Each batch is optimally sized for its contents\n",
            "  • Faster training with dynamic batching\n",
            "\n",
            "However, 'max_length' might be preferred when:\n",
            "  • Hardware requires fixed input sizes\n",
            "  • Comparing models fairly across different implementations\n",
            "  • Debugging (consistent shapes are easier to trace)\n"
          ]
        }
      ],
      "source": [
        "# Solution to Exercise 2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "sentences = [\n",
        "    \"Great!\",\n",
        "    \"This is a medium length sentence about machine learning.\",\n",
        "    \"Short one.\",\n",
        "    \"This is the longest sentence in our batch and it talks about natural language processing and transformers.\"\n",
        "]\n",
        "\n",
        "# 1. Padding to longest\n",
        "enc_longest = tokenizer(sentences, padding=\"longest\", truncation=True, return_tensors=\"pt\")\n",
        "print(\"--- Padding='longest' ---\")\n",
        "print(f\"Shape: {enc_longest['input_ids'].shape}\")\n",
        "\n",
        "# 2. Padding to max_length\n",
        "enc_maxlen = tokenizer(sentences, padding=\"max_length\", max_length=64, truncation=True, return_tensors=\"pt\")\n",
        "print(\"\\n--- Padding='max_length' (64) ---\")\n",
        "print(f\"Shape: {enc_maxlen['input_ids'].shape}\")\n",
        "\n",
        "# 3. Calculate percentage of padding\n",
        "def calculate_padding_percentage(attention_mask):\n",
        "    total_tokens = attention_mask.numel()\n",
        "    padding_tokens = (attention_mask == 0).sum().item()\n",
        "    return (padding_tokens / total_tokens) * 100\n",
        "\n",
        "padding_pct_longest = calculate_padding_percentage(enc_longest['attention_mask'])\n",
        "padding_pct_maxlen = calculate_padding_percentage(enc_maxlen['attention_mask'])\n",
        "\n",
        "print(f\"\\nPadding percentage (longest): {padding_pct_longest:.2f}%\")\n",
        "print(f\"Padding percentage (max_length=64): {padding_pct_maxlen:.2f}%\")\n",
        "print(f\"Efficiency gain: {padding_pct_maxlen - padding_pct_longest:.2f}% less padding with 'longest'\")\n",
        "\n",
        "# 4. Discussion\n",
        "print(\"\\n--- Discussion ---\")\n",
        "print(\"For training, 'longest' is generally better because:\")\n",
        "print(\"  • Reduces wasted computation on padding tokens\")\n",
        "print(\"  • Each batch is optimally sized for its contents\")\n",
        "print(\"  • Faster training with dynamic batching\")\n",
        "print(\"\\nHowever, 'max_length' might be preferred when:\")\n",
        "print(\"  • Hardware requires fixed input sizes\")\n",
        "print(\"  • Comparing models fairly across different implementations\")\n",
        "print(\"  • Debugging (consistent shapes are easier to trace)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d5e5b4c",
      "metadata": {},
      "source": [
        "### Exercise 3: Exploring BERT's Architecture\n",
        "\n",
        "**Task:** Investigate BERT's parameter distribution across different components.\n",
        "\n",
        "**Requirements:**\n",
        "1. Calculate the number of parameters in the embedding layer (word embeddings, position embeddings, token type embeddings)\n",
        "2. Calculate the number of parameters in a single encoder layer\n",
        "3. Calculate the number of parameters in the pooler layer\n",
        "4. Verify your calculations sum to the total number of parameters\n",
        "\n",
        "**Hint:** Use `named_parameters()` to iterate through all parameters and their shapes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "0ca2bad0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding layer parameters: 23,837,184\n",
            "  - Word embeddings: 23,440,896\n",
            "  - Position embeddings: 393,216\n",
            "  - Token type embeddings: 1,536\n",
            "  - LayerNorm + others: 1,536\n",
            "\n",
            "Single encoder layer parameters: 7,087,872\n",
            "  - Self-attention: 2,363,904\n",
            "  - Intermediate (FFN): 2,362,368\n",
            "  - Output projection: 2,361,600\n",
            "\n",
            "Pooler layer parameters: 590,592\n",
            "\n",
            "All encoder layers (12 layers) parameters: 85,054,464\n",
            "\n",
            "--- Verification ---\n",
            "Sum of calculated parameters: 109,482,240\n",
            "Actual total parameters: 109,482,240\n",
            "Match: True\n",
            "\n",
            "--- Additional Insights ---\n",
            "Encoder layers contain 77.7% of all parameters\n",
            "Each encoder layer has 7,087,872 parameters\n",
            "Average parameters per layer (including embeddings): 7,820,160\n"
          ]
        }
      ],
      "source": [
        "# Solution to Exercise 3\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Helper function to count parameters\n",
        "def count_params(module):\n",
        "    return sum(p.numel() for p in module.parameters())\n",
        "\n",
        "# 1. Embedding layer parameters\n",
        "embedding_params = count_params(bert.embeddings)\n",
        "print(f\"Embedding layer parameters: {embedding_params:,}\")\n",
        "print(f\"  - Word embeddings: {bert.embeddings.word_embeddings.weight.numel():,}\")\n",
        "print(f\"  - Position embeddings: {bert.embeddings.position_embeddings.weight.numel():,}\")\n",
        "print(f\"  - Token type embeddings: {bert.embeddings.token_type_embeddings.weight.numel():,}\")\n",
        "print(f\"  - LayerNorm + others: {embedding_params - bert.embeddings.word_embeddings.weight.numel() - bert.embeddings.position_embeddings.weight.numel() - bert.embeddings.token_type_embeddings.weight.numel():,}\")\n",
        "\n",
        "# 2. Single encoder layer parameters\n",
        "single_encoder_params = count_params(bert.encoder.layer[0])\n",
        "print(f\"\\nSingle encoder layer parameters: {single_encoder_params:,}\")\n",
        "\n",
        "# Break down a single layer\n",
        "layer0 = bert.encoder.layer[0]\n",
        "attn_params = count_params(layer0.attention)\n",
        "intermediate_params = count_params(layer0.intermediate)\n",
        "output_params = count_params(layer0.output)\n",
        "print(f\"  - Self-attention: {attn_params:,}\")\n",
        "print(f\"  - Intermediate (FFN): {intermediate_params:,}\")\n",
        "print(f\"  - Output projection: {output_params:,}\")\n",
        "\n",
        "# 3. Pooler layer parameters\n",
        "pooler_params = count_params(bert.pooler)\n",
        "print(f\"\\nPooler layer parameters: {pooler_params:,}\")\n",
        "\n",
        "# All encoder layers\n",
        "all_encoder_params = count_params(bert.encoder)\n",
        "print(f\"\\nAll encoder layers (12 layers) parameters: {all_encoder_params:,}\")\n",
        "\n",
        "# 4. Verify calculations\n",
        "param_distribution = {\n",
        "    'Embeddings': embedding_params,\n",
        "    'Encoder Layers (12x)': all_encoder_params,\n",
        "    'Pooler': pooler_params\n",
        "}\n",
        "\n",
        "calculated_total = sum(param_distribution.values())\n",
        "actual_total = count_params(bert)\n",
        "print(f\"\\n--- Verification ---\")\n",
        "print(f\"Sum of calculated parameters: {calculated_total:,}\")\n",
        "print(f\"Actual total parameters: {actual_total:,}\")\n",
        "print(f\"Match: {calculated_total == actual_total}\" if calculated_total == actual_total else f\"Difference: {abs(calculated_total - actual_total):,}\")\n",
        "\n",
        "# Additional insight: parameters per encoder layer\n",
        "print(f\"\\n--- Additional Insights ---\")\n",
        "print(f\"Encoder layers contain {(all_encoder_params/actual_total)*100:.1f}% of all parameters\")\n",
        "print(f\"Each encoder layer has {single_encoder_params:,} parameters\")\n",
        "print(f\"Average parameters per layer (including embeddings): {actual_total // (bert.config.num_hidden_layers + 2):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09906fa5",
      "metadata": {},
      "source": [
        "### Exercise 4: Masked Language Modeling with Multiple Masks\n",
        "\n",
        "**Task:** Implement a function that predicts multiple masked tokens in a sentence and evaluates the predictions.\n",
        "\n",
        "**Requirements:**\n",
        "1. Create a function `predict_masked_tokens(sentence, mask_positions)` that:\n",
        "   - Takes a sentence and a list of word positions to mask (0-indexed, excluding special tokens)\n",
        "   - Replaces those positions with [MASK]\n",
        "   - Returns the top-3 predictions for each masked position with their probabilities\n",
        "2. Test your function with: `\"The quick brown fox jumps over the lazy dog\"` \n",
        "   - Mask positions: 1 (\"quick\"), 4 (\"jumps\"), and 7 (\"lazy\")\n",
        "3. Calculate the average confidence (probability) of the top prediction across all masks\n",
        "4. Discuss: Why might BERT struggle with certain masked words more than others?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "2d8604bf",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original: The quick brown fox jumps over the lazy dog\n",
            "Masked:   The [MASK] brown fox [MASK] over the [MASK] dog\n",
            "\n",
            "======================================================================\n",
            "PREDICTIONS\n",
            "======================================================================\n",
            "\n",
            "Position 1 - Original word: 'quick'\n",
            "--------------------------------------------------\n",
            "  1. little          (confidence: 23.19%)\n",
            "  2. great           (confidence:  7.49%)\n",
            "  3. big             (confidence:  5.30%)\n",
            "  4. new             (confidence:  2.98%)\n",
            "  5. large           (confidence:  2.06%)\n",
            "\n",
            "Position 4 - Original word: 'jumps'\n",
            "--------------------------------------------------\n",
            "  1. takes           (confidence: 74.10%)\n",
            "  2. took            (confidence:  9.22%)\n",
            "  3. wins            (confidence:  7.18%)\n",
            "  4. taking          (confidence:  1.04%)\n",
            "  5. take            (confidence:  1.00%)\n",
            "\n",
            "Position 7 - Original word: 'lazy'\n",
            "--------------------------------------------------\n",
            "  1. white           (confidence:  5.71%)\n",
            "  2. big             (confidence:  5.31%)\n",
            "  3. black           (confidence:  4.23%)\n",
            "  4. red             (confidence:  4.01%)\n",
            "  5. little          (confidence:  3.64%)\n",
            "\n",
            "======================================================================\n",
            "Average confidence of top predictions: 34.33%\n",
            "\n",
            "======================================================================\n",
            "DISCUSSION: Why BERT struggles with certain words\n",
            "======================================================================\n",
            "\n",
            "BERT's prediction confidence varies based on:\n",
            "\n",
            "1. **Context informativeness**: \n",
            "   - \"lazy\" before \"dog\" is more predictable (common phrase)\n",
            "   - \"quick\" could be many adjectives (fast, small, big, etc.)\n",
            "\n",
            "2. **Word frequency in training data**:\n",
            "   - Common words are easier to predict\n",
            "   - Rare words have fewer training examples\n",
            "\n",
            "3. **Part of speech**:\n",
            "   - Content words (nouns, verbs) often harder than function words\n",
            "   - Adjectives can be very context-dependent\n",
            "\n",
            "4. **Semantic ambiguity**:\n",
            "   - Multiple valid options reduce confidence\n",
            "   - \"jumps\" could be runs, leaps, moves, etc.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Solution to Exercise 4\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import BertForMaskedLM\n",
        "\n",
        "mlm_model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "mlm_model.eval()\n",
        "\n",
        "# 1. Create prediction function\n",
        "def predict_masked_tokens(sentence, mask_positions, top_k=3):\n",
        "    \"\"\"\n",
        "    Predict masked tokens in a sentence.\n",
        "    \n",
        "    Args:\n",
        "        sentence: Input sentence (string)\n",
        "        mask_positions: List of word positions to mask (0-indexed, excluding [CLS])\n",
        "        top_k: Number of top predictions to return\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with predictions for each masked position\n",
        "    \"\"\"\n",
        "    # Tokenize to get word positions\n",
        "    words = sentence.split()\n",
        "    \n",
        "    # Replace specified positions with [MASK]\n",
        "    masked_words = words.copy()\n",
        "    original_words = {}\n",
        "    for pos in mask_positions:\n",
        "        original_words[pos] = masked_words[pos]\n",
        "        masked_words[pos] = \"[MASK]\"\n",
        "    \n",
        "    masked_sentence = \" \".join(masked_words)\n",
        "    print(f\"Original: {sentence}\")\n",
        "    print(f\"Masked:   {masked_sentence}\\n\")\n",
        "    \n",
        "    # Tokenize (note: some words might be split into subwords)\n",
        "    inputs = tokenizer(masked_sentence, return_tensors=\"pt\")\n",
        "    \n",
        "    # Get predictions\n",
        "    with torch.no_grad():\n",
        "        outputs = mlm_model(**inputs)\n",
        "        logits = outputs.logits\n",
        "    \n",
        "    # Find [MASK] positions in tokenized input\n",
        "    mask_token_indices = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
        "    \n",
        "    # Get predictions for each mask\n",
        "    results = {}\n",
        "    for i, mask_idx in enumerate(mask_token_indices):\n",
        "        # Get probabilities for this mask position\n",
        "        mask_logits = logits[0, mask_idx]\n",
        "        probs = F.softmax(mask_logits, dim=-1)\n",
        "        \n",
        "        # Get top-k predictions\n",
        "        top_probs, top_indices = torch.topk(probs, top_k)\n",
        "        \n",
        "        predictions = []\n",
        "        for prob, idx in zip(top_probs, top_indices):\n",
        "            token = tokenizer.decode([idx]).strip()\n",
        "            predictions.append({\n",
        "                'token': token,\n",
        "                'probability': prob.item()\n",
        "            })\n",
        "        \n",
        "        word_pos = mask_positions[i]\n",
        "        results[word_pos] = {\n",
        "            'original': original_words[word_pos],\n",
        "            'predictions': predictions\n",
        "        }\n",
        "    \n",
        "    return results\n",
        "\n",
        "# 2. Test the function\n",
        "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
        "mask_positions = [1, 4, 7]  # \"quick\", \"jumps\", \"lazy\"\n",
        "\n",
        "top_k = 5\n",
        "predictions = predict_masked_tokens(sentence, mask_positions, top_k=top_k)\n",
        "\n",
        "# Display results\n",
        "print(\"=\" * 70)\n",
        "print(\"PREDICTIONS\")\n",
        "print(\"=\" * 70)\n",
        "for pos, data in predictions.items():\n",
        "    print(f\"\\nPosition {pos} - Original word: '{data['original']}'\")\n",
        "    print(\"-\" * 50)\n",
        "    for i, pred in enumerate(data['predictions'], 1):\n",
        "        marker = \" ✓\" if pred['token'] == data['original'].lower() else \"\"\n",
        "        print(f\"  {i}. {pred['token']:<15} (confidence: {pred['probability']*100:5.2f}%){marker}\")\n",
        "\n",
        "# 3. Calculate average confidence\n",
        "top_confidences = [data['predictions'][0]['probability'] for data in predictions.values()]\n",
        "avg_confidence = sum(top_confidences) / len(top_confidences)\n",
        "print(f\"\\n{'=' * 70}\")\n",
        "print(f\"Average confidence of top predictions: {avg_confidence*100:.2f}%\")\n",
        "\n",
        "# 4. Discussion\n",
        "print(f\"\\n{'=' * 70}\")\n",
        "print(\"DISCUSSION: Why BERT struggles with certain words\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\"\"\n",
        "BERT's prediction confidence varies based on:\n",
        "\n",
        "1. **Context informativeness**: \n",
        "   - \"lazy\" before \"dog\" is more predictable (common phrase)\n",
        "   - \"quick\" could be many adjectives (fast, small, big, etc.)\n",
        "\n",
        "2. **Word frequency in training data**:\n",
        "   - Common words are easier to predict\n",
        "   - Rare words have fewer training examples\n",
        "\n",
        "3. **Part of speech**:\n",
        "   - Content words (nouns, verbs) often harder than function words\n",
        "   - Adjectives can be very context-dependent\n",
        "\n",
        "4. **Semantic ambiguity**:\n",
        "   - Multiple valid options reduce confidence\n",
        "   - \"jumps\" could be runs, leaps, moves, etc.\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65c2a750",
      "metadata": {},
      "source": [
        "### Exercise 5: Next Sentence Prediction Evaluation\n",
        "\n",
        "**Task:** Create a small dataset to evaluate BERT's Next Sentence Prediction capabilities and analyze its performance.\n",
        "\n",
        "**Requirements:**\n",
        "1. Create 10 sentence pairs: 5 that are actual continuations and 5 that are not\n",
        "   - Use diverse topics (news, stories, technical content, etc.)\n",
        "2. For each pair, get BERT's NSP prediction and confidence scores\n",
        "3. Calculate accuracy, precision, and recall for the predictions\n",
        "4. Identify which pair had the highest confidence (regardless of correctness)\n",
        "5. Create a confusion matrix visualization\n",
        "\n",
        "**Hint:** Use `BertForNextSentencePrediction` and apply softmax to get probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "87466e60",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Solution to Exercise 6\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertForNextSentencePrediction\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "# Load NSP model\n",
        "nsp_model = BertForNextSentencePrediction.from_pretrained(\"bert-base-uncased\").to(device)\n",
        "nsp_model.eval()\n",
        "\n",
        "# 1. Create dataset of sentence pairs\n",
        "sentence_pairs = [\n",
        "    # True continuations (label = 1 for \"is next\")\n",
        "    {\n",
        "        'sent_a': \"The weather forecast predicts heavy rain tomorrow.\",\n",
        "        'sent_b': \"Don't forget to bring an umbrella when you go out.\",\n",
        "        'is_next': True,\n",
        "        'topic': 'Weather'\n",
        "    },\n",
        "    {\n",
        "        'sent_a': \"She studied hard for her final exams all week.\",\n",
        "        'sent_b': \"Her efforts paid off when she received excellent grades.\",\n",
        "        'is_next': True,\n",
        "        'topic': 'Education'\n",
        "    },\n",
        "    {\n",
        "        'sent_a': \"The company announced record profits in Q4.\",\n",
        "        'sent_b': \"Shareholders were pleased with the financial results.\",\n",
        "        'is_next': True,\n",
        "        'topic': 'Business'\n",
        "    },\n",
        "    {\n",
        "        'sent_a': \"Neural networks learn by adjusting their weights.\",\n",
        "        'sent_b': \"This process is called backpropagation and uses gradient descent.\",\n",
        "        'is_next': True,\n",
        "        'topic': 'Technical'\n",
        "    },\n",
        "    {\n",
        "        'sent_a': \"The movie had stunning visual effects and great acting.\",\n",
        "        'sent_b': \"It went on to win several Academy Awards that year.\",\n",
        "        'is_next': True,\n",
        "        'topic': 'Entertainment'\n",
        "    },\n",
        "    \n",
        "    # False continuations (label = 0 for \"is NOT next\")\n",
        "    {\n",
        "        'sent_a': \"Quantum computing uses qubits instead of classical bits.\",\n",
        "        'sent_b': \"Pizza is one of the most popular foods in Italy.\",\n",
        "        'is_next': False,\n",
        "        'topic': 'Random'\n",
        "    },\n",
        "    {\n",
        "        'sent_a': \"The patient showed symptoms of seasonal allergies.\",\n",
        "        'sent_b': \"The stock market reached an all-time high today.\",\n",
        "        'is_next': False,\n",
        "        'topic': 'Random'\n",
        "    },\n",
        "    {\n",
        "        'sent_a': \"Python is a versatile programming language.\",\n",
        "        'sent_b': \"Pythons are the largest snakes in the world.\",\n",
        "        'is_next': False,\n",
        "        'topic': 'Random'\n",
        "    },\n",
        "    {\n",
        "        'sent_a': \"Climate change affects global weather patterns.\",\n",
        "        'sent_b': \"The restaurant serves authentic Japanese cuisine.\",\n",
        "        'is_next': False,\n",
        "        'topic': 'Random'\n",
        "    },\n",
        "    {\n",
        "        'sent_a': \"The team celebrated their championship victory.\",\n",
        "        'sent_b': \"Photosynthesis converts light energy into chemical energy.\",\n",
        "        'is_next': False,\n",
        "        'topic': 'Random'\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "3cca050c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==========================================================================================\n",
            "NEXT SENTENCE PREDICTION RESULTS\n",
            "==========================================================================================\n",
            "\n",
            "Pair 1 (Weather) - ✓ CORRECT\n",
            "  A: The weather forecast predicts heavy rain tomorrow....\n",
            "  B: Don't forget to bring an umbrella when you go out....\n",
            "  True: IS next | Predicted: IS next\n",
            "  Confidence: 100.00% | P(is_next)=1.000, P(not_next)=0.000\n",
            "\n",
            "Pair 2 (Education) - ✓ CORRECT\n",
            "  A: She studied hard for her final exams all week....\n",
            "  B: Her efforts paid off when she received excellent grades....\n",
            "  True: IS next | Predicted: IS next\n",
            "  Confidence: 100.00% | P(is_next)=1.000, P(not_next)=0.000\n",
            "\n",
            "Pair 3 (Business) - ✓ CORRECT\n",
            "  A: The company announced record profits in Q4....\n",
            "  B: Shareholders were pleased with the financial results....\n",
            "  True: IS next | Predicted: IS next\n",
            "  Confidence: 100.00% | P(is_next)=1.000, P(not_next)=0.000\n",
            "\n",
            "Pair 4 (Technical) - ✓ CORRECT\n",
            "  A: Neural networks learn by adjusting their weights....\n",
            "  B: This process is called backpropagation and uses gradient descent....\n",
            "  True: IS next | Predicted: IS next\n",
            "  Confidence: 100.00% | P(is_next)=1.000, P(not_next)=0.000\n",
            "\n",
            "Pair 5 (Entertainment) - ✓ CORRECT\n",
            "  A: The movie had stunning visual effects and great acting....\n",
            "  B: It went on to win several Academy Awards that year....\n",
            "  True: IS next | Predicted: IS next\n",
            "  Confidence: 100.00% | P(is_next)=1.000, P(not_next)=0.000\n",
            "\n",
            "Pair 6 (Random) - ✓ CORRECT\n",
            "  A: Quantum computing uses qubits instead of classical bits....\n",
            "  B: Pizza is one of the most popular foods in Italy....\n",
            "  True: NOT next | Predicted: NOT next\n",
            "  Confidence: 100.00% | P(is_next)=0.000, P(not_next)=1.000\n",
            "\n",
            "Pair 7 (Random) - ✓ CORRECT\n",
            "  A: The patient showed symptoms of seasonal allergies....\n",
            "  B: The stock market reached an all-time high today....\n",
            "  True: NOT next | Predicted: NOT next\n",
            "  Confidence: 98.52% | P(is_next)=0.015, P(not_next)=0.985\n",
            "\n",
            "Pair 8 (Random) - ✗ WRONG\n",
            "  A: Python is a versatile programming language....\n",
            "  B: Pythons are the largest snakes in the world....\n",
            "  True: NOT next | Predicted: IS next\n",
            "  Confidence: 99.99% | P(is_next)=1.000, P(not_next)=0.000\n",
            "\n",
            "Pair 9 (Random) - ✓ CORRECT\n",
            "  A: Climate change affects global weather patterns....\n",
            "  B: The restaurant serves authentic Japanese cuisine....\n",
            "  True: NOT next | Predicted: NOT next\n",
            "  Confidence: 100.00% | P(is_next)=0.000, P(not_next)=1.000\n",
            "\n",
            "Pair 10 (Random) - ✓ CORRECT\n",
            "  A: The team celebrated their championship victory....\n",
            "  B: Photosynthesis converts light energy into chemical energy....\n",
            "  True: NOT next | Predicted: NOT next\n",
            "  Confidence: 100.00% | P(is_next)=0.000, P(not_next)=1.000\n",
            "\n",
            "==========================================================================================\n",
            "PERFORMANCE METRICS\n",
            "==========================================================================================\n",
            "Accuracy:  90.00% (9/10 correct)\n",
            "Precision: 83.33% (of predicted 'IS next', how many were correct)\n",
            "Recall:    100.00% (of true 'IS next', how many were found)\n",
            "F1-Score:  90.91%\n",
            "\n",
            "==========================================================================================\n",
            "HIGHEST CONFIDENCE PREDICTION\n",
            "==========================================================================================\n",
            "Pair 9 with 100.00% confidence\n",
            "  Prediction: NOT next\n",
            "  Actually: NOT next\n",
            "  Status: Correct ✓\n"
          ]
        }
      ],
      "source": [
        "# 2. Get predictions for each pair\n",
        "print(\"=\" * 90)\n",
        "print(\"NEXT SENTENCE PREDICTION RESULTS\")\n",
        "print(\"=\" * 90)\n",
        "\n",
        "results = []\n",
        "for i, pair in enumerate(sentence_pairs):\n",
        "    # Tokenize the pair\n",
        "    inputs = tokenizer(pair['sent_a'], pair['sent_b'], return_tensors=\"pt\").to(device)\n",
        "    \n",
        "    # Get prediction\n",
        "    with torch.no_grad():\n",
        "        outputs = nsp_model(**inputs)\n",
        "        logits = outputs.logits\n",
        "    \n",
        "    # Get probabilities\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    is_next_prob = probs[0, 0].item()  # Probability that sent_b IS next\n",
        "    not_next_prob = probs[0, 1].item()  # Probability that sent_b is NOT next\n",
        "    \n",
        "    # Make prediction\n",
        "    predicted_is_next = is_next_prob > not_next_prob\n",
        "    correct = predicted_is_next == pair['is_next']\n",
        "    confidence = max(is_next_prob, not_next_prob)\n",
        "    \n",
        "    results.append({\n",
        "        'pair_id': i + 1,\n",
        "        'sent_a': pair['sent_a'],\n",
        "        'sent_b': pair['sent_b'],\n",
        "        'true_label': pair['is_next'],\n",
        "        'predicted_label': predicted_is_next,\n",
        "        'is_next_prob': is_next_prob,\n",
        "        'not_next_prob': not_next_prob,\n",
        "        'confidence': confidence,\n",
        "        'correct': correct,\n",
        "        'topic': pair['topic']\n",
        "    })\n",
        "    \n",
        "    # Display\n",
        "    status = \"✓ CORRECT\" if correct else \"✗ WRONG\"\n",
        "    print(f\"\\nPair {i+1} ({pair['topic']}) - {status}\")\n",
        "    print(f\"  A: {pair['sent_a'][:65]}...\")\n",
        "    print(f\"  B: {pair['sent_b'][:65]}...\")\n",
        "    print(f\"  True: {'IS next' if pair['is_next'] else 'NOT next'} | \"\n",
        "          f\"Predicted: {'IS next' if predicted_is_next else 'NOT next'}\")\n",
        "    print(f\"  Confidence: {confidence*100:.2f}% | P(is_next)={is_next_prob:.3f}, P(not_next)={not_next_prob:.3f}\")\n",
        "\n",
        "# 3. Calculate metrics\n",
        "y_true = [1 if r['true_label'] else 0 for r in results]\n",
        "y_pred = [1 if r['predicted_label'] else 0 for r in results]\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred, zero_division=0)\n",
        "recall = recall_score(y_true, y_pred, zero_division=0)\n",
        "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "print(\"\\n\" + \"=\" * 90)\n",
        "print(\"PERFORMANCE METRICS\")\n",
        "print(\"=\" * 90)\n",
        "print(f\"Accuracy:  {accuracy*100:.2f}% ({sum([r['correct'] for r in results])}/{len(results)} correct)\")\n",
        "print(f\"Precision: {precision*100:.2f}% (of predicted 'IS next', how many were correct)\")\n",
        "print(f\"Recall:    {recall*100:.2f}% (of true 'IS next', how many were found)\")\n",
        "print(f\"F1-Score:  {f1*100:.2f}%\")\n",
        "\n",
        "# 4. Identify highest confidence pair\n",
        "highest_conf_result = max(results, key=lambda x: x['confidence'])\n",
        "print(\"\\n\" + \"=\" * 90)\n",
        "print(\"HIGHEST CONFIDENCE PREDICTION\")\n",
        "print(\"=\" * 90)\n",
        "print(f\"Pair {highest_conf_result['pair_id']} with {highest_conf_result['confidence']*100:.2f}% confidence\")\n",
        "print(f\"  Prediction: {'IS next' if highest_conf_result['predicted_label'] else 'NOT next'}\")\n",
        "print(f\"  Actually: {'IS next' if highest_conf_result['true_label'] else 'NOT next'}\")\n",
        "print(f\"  Status: {'Correct ✓' if highest_conf_result['correct'] else 'Wrong ✗'}\")\n",
        "\n",
        "# Confidence distribution\n",
        "true_positives = [r for r in results if r['true_label'] and r['predicted_label']]\n",
        "true_negatives = [r for r in results if not r['true_label'] and not r['predicted_label']]\n",
        "false_positives = [r for r in results if not r['true_label'] and r['predicted_label']]\n",
        "false_negatives = [r for r in results if r['true_label'] and not r['predicted_label']]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ATE_COURSE",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
